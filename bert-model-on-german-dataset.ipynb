{"cells":[{"cell_type":"code","metadata":{"cell_id":"00000-5ef145c4-fdb3-49b0-8fef-c25471ea8dea"},"source":"# Start writing code here...","execution_count":150,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-6be88c99-a6a7-41da-a236-db767e8b38bd"},"source":"!pip install nltk\n!pip install bert-tensorflow\n!pip install transformers\n!pip install seaborn\n!pip install -U sentence-transformers\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n","execution_count":151,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk) (4.48.2)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: bert-tensorflow in /opt/venv/lib/python3.7/site-packages (1.0.4)\nRequirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from bert-tensorflow) (1.15.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: transformers in /opt/venv/lib/python3.7/site-packages (3.0.2)\nRequirement already satisfied: numpy in /opt/venv/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: filelock in /opt/venv/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: sacremoses in /opt/venv/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: tqdm>=4.27 in /opt/venv/lib/python3.7/site-packages (from transformers) (4.48.2)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/venv/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from transformers) (2.24.0)\nRequirement already satisfied: packaging in /opt/venv/lib/python3.7/site-packages (from transformers) (20.4)\nRequirement already satisfied: tokenizers==0.8.1.rc1 in /opt/venv/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.7/site-packages (from transformers) (2020.7.14)\nRequirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: seaborn in /opt/venv/lib/python3.7/site-packages (0.10.1)\nRequirement already satisfied: matplotlib>=2.1.2 in /opt/venv/lib/python3.7/site-packages (from seaborn) (3.2.2)\nRequirement already satisfied: scipy>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.4.1)\nRequirement already satisfied: numpy>=1.13.3 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.18.5)\nRequirement already satisfied: pandas>=0.22.0 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\nRequirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already up-to-date: sentence-transformers in /opt/venv/lib/python3.7/site-packages (0.3.3)\nRequirement already satisfied, skipping upgrade: scipy in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.4.1)\nRequirement already satisfied, skipping upgrade: nltk in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (3.5)\nRequirement already satisfied, skipping upgrade: numpy in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.18.5)\nRequirement already satisfied, skipping upgrade: tqdm in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (4.48.2)\nRequirement already satisfied, skipping upgrade: scikit-learn in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (0.23.1)\nRequirement already satisfied, skipping upgrade: transformers>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (3.0.2)\nRequirement already satisfied, skipping upgrade: torch>=1.2.0 in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.5.0+cu101)\nRequirement already satisfied, skipping upgrade: regex in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (2020.7.14)\nRequirement already satisfied, skipping upgrade: click in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (7.1.2)\nRequirement already satisfied, skipping upgrade: joblib in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (0.16.0)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/venv/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\nRequirement already satisfied, skipping upgrade: filelock in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (3.0.12)\nRequirement already satisfied, skipping upgrade: sacremoses in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.0.43)\nRequirement already satisfied, skipping upgrade: packaging in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (20.4)\nRequirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.1.91)\nRequirement already satisfied, skipping upgrade: requests in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (2.24.0)\nRequirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc1 in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.8.1rc1)\nRequirement already satisfied, skipping upgrade: future in /opt/venv/lib/python3.7/site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\nRequirement already satisfied, skipping upgrade: six in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers) (1.15.0)\nRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers) (2.4.7)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (1.25.9)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2.10)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2020.6.20)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"execute_result","execution_count":151,"data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-f16f20eb-f58b-4be2-bcd8-142ec0b02ce9"},"source":"# Insert code here.\nimport pandas as pd\nimport numpy as np\nimport random\nimport re\nimport time\nimport datetime\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\nimport torch\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nsent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')\n","execution_count":152,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-3532c383-2872-45bd-8034-48c22aab0a67"},"source":"if torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":153,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-db889112-cd7e-4c59-90a7-a796d5a1e4e3"},"source":"torch.cuda.empty_cache()\n!nvidia-smi","execution_count":154,"outputs":[{"name":"stdout","text":"Fri Aug 21 08:18:53 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   77C    P0    31W /  70W |  11362MiB / 15079MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-ac9afc8d-aa53-4b5f-b8e1-faaff6b127df"},"source":"DATASET_PATH = '/home/jovyan/work/Hate-Speech-Detection/hasoc_2020_de_train.csv'","execution_count":155,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-bbfa31f2-aba1-421e-a4e9-f5401eab7a01"},"source":"df = pd.read_csv(DATASET_PATH)\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","execution_count":156,"outputs":[{"name":"stdout","text":"Number of training sentences: 2,452\n\n","output_type":"stream"},{"output_type":"execute_result","execution_count":156,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":5,"columns":[{"name":"tweet_id","dtype":"int64","stats":{"unique_count":10,"nan_count":0,"min":1124639967291039700,"max":1134189608986710000,"histogram":[{"bin_start":1124639967291039700,"bin_end":1125594931460606700,"count":2},{"bin_start":1125594931460606700,"bin_end":1126549895630173800,"count":0},{"bin_start":1126549895630173800,"bin_end":1127504859799740800,"count":0},{"bin_start":1127504859799740800,"bin_end":1128459823969307900,"count":0},{"bin_start":1128459823969307900,"bin_end":1129414788138874900,"count":3},{"bin_start":1129414788138874900,"bin_end":1130369752308441900,"count":0},{"bin_start":1130369752308441900,"bin_end":1131324716478009000,"count":1},{"bin_start":1131324716478009000,"bin_end":1132279680647575900,"count":1},{"bin_start":1132279680647575900,"bin_end":1133234644817143000,"count":1},{"bin_start":1133234644817143000,"bin_end":1134189608986710000,"count":2}]}},{"name":"text","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt.","count":1},{"name":"RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…","count":1},{"name":"8 others","count":8}]}},{"name":"task1","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"HOF","count":8},{"name":"NOT","count":2}]}},{"name":"task2","dtype":"object","stats":{"unique_count":4,"nan_count":0,"categories":[{"name":"NONE","count":3},{"name":"PRFN","count":3},{"name":"2 others","count":4}]}},{"name":"ID","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"hasoc_2020_de_2475","count":1},{"name":"hasoc_2020_de_177","count":1},{"name":"8 others","count":8}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"tweet_id":1134189608986710000,"text":"wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt.","task1":"HOF","task2":"NONE","ID":"hasoc_2020_de_2475","_deepnote_index_column":94},{"tweet_id":1131102202762805200,"text":"RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…","task1":"HOF","task2":"HATE","ID":"hasoc_2020_de_177","_deepnote_index_column":2418},{"tweet_id":1133612703447179300,"text":"Warum zünden Frauen ihren Arsch an? \n\nWeil sie es können.","task1":"HOF","task2":"PRFN","ID":"hasoc_2020_de_1966","_deepnote_index_column":345},{"tweet_id":1124639967291039700,"text":"@schwartz_ht @internetbazi @KulierJarolina Ist Dein bester Freund auch so 1 Hurensohn wie der Bazi?","task1":"HOF","task2":"OFFN","ID":"hasoc_2020_de_1245","_deepnote_index_column":121},{"tweet_id":1129250614708297700,"text":"ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU OPFA!","task1":"HOF","task2":"OFFN","ID":"hasoc_2020_de_1513","_deepnote_index_column":2129},{"tweet_id":1124698624632545300,"text":"\"Ach scheiß FridaysforFuture! Die sollen erstmal lernen gehen! Alles ungebildete Schulschwänzer da. Alle dumm\"\n... *schnauft genervt*","task1":"HOF","task2":"HATE","ID":"hasoc_2020_de_3139","_deepnote_index_column":1510},{"tweet_id":1131670132529487900,"text":"@champagnenihal er stürzt ab mit diese drogensachen keske yanimda olsa adam ederdim","task1":"NOT","task2":"NONE","ID":"hasoc_2020_de_1491","_deepnote_index_column":1147},{"tweet_id":1128982363785048000,"text":"heimlich ficken deutsche blondine fickt frauen virgin https://t.co/kjS22CZwrr","task1":"HOF","task2":"PRFN","ID":"hasoc_2020_de_275","_deepnote_index_column":565},{"tweet_id":1132685820991823900,"text":"Vergleicht Gauland gerade wirklich die Volkskammer der DDR mit dem EU-Parlament? Urgh... #ZDF #Wahl2019","task1":"NOT","task2":"NONE","ID":"hasoc_2020_de_2565","_deepnote_index_column":2128},{"tweet_id":1129299763566981100,"text":"@CCrunk9 Was erwartet der hurensohn was passiert","task1":"HOF","task2":"PRFN","ID":"hasoc_2020_de_2906","_deepnote_index_column":382}],"rows_bottom":null},"text/plain":"                 tweet_id                                               text  \\\n94    1134189608986710021  wo sitzt dieser schlaumeier, der für alle idio...   \n2418  1131102202762805248  RT @helllud123: @ZDFheute @sven_giegold @Joerg...   \n345   1133612703447179264  Warum zünden Frauen ihren Arsch an? \\n\\nWeil s...   \n121   1124639967291039745  @schwartz_ht @internetbazi @KulierJarolina Ist...   \n2129  1129250614708297728  ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU...   \n1510  1124698624632545281  \"Ach scheiß FridaysforFuture! Die sollen erstm...   \n1147  1131670132529487872  @champagnenihal er stürzt ab mit diese drogens...   \n565   1128982363785048066  heimlich ficken deutsche blondine fickt frauen...   \n2128  1132685820991823872  Vergleicht Gauland gerade wirklich die Volkska...   \n382   1129299763566981120   @CCrunk9 Was erwartet der hurensohn was passiert   \n\n     task1 task2                  ID  \n94     HOF  NONE  hasoc_2020_de_2475  \n2418   HOF  HATE   hasoc_2020_de_177  \n345    HOF  PRFN  hasoc_2020_de_1966  \n121    HOF  OFFN  hasoc_2020_de_1245  \n2129   HOF  OFFN  hasoc_2020_de_1513  \n1510   HOF  HATE  hasoc_2020_de_3139  \n1147   NOT  NONE  hasoc_2020_de_1491  \n565    HOF  PRFN   hasoc_2020_de_275  \n2128   NOT  NONE  hasoc_2020_de_2565  \n382    HOF  PRFN  hasoc_2020_de_2906  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n      <th>task1</th>\n      <th>task2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>94</th>\n      <td>1134189608986710021</td>\n      <td>wo sitzt dieser schlaumeier, der für alle idio...</td>\n      <td>HOF</td>\n      <td>NONE</td>\n      <td>hasoc_2020_de_2475</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>1131102202762805248</td>\n      <td>RT @helllud123: @ZDFheute @sven_giegold @Joerg...</td>\n      <td>HOF</td>\n      <td>HATE</td>\n      <td>hasoc_2020_de_177</td>\n    </tr>\n    <tr>\n      <th>345</th>\n      <td>1133612703447179264</td>\n      <td>Warum zünden Frauen ihren Arsch an? \\n\\nWeil s...</td>\n      <td>HOF</td>\n      <td>PRFN</td>\n      <td>hasoc_2020_de_1966</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>1124639967291039745</td>\n      <td>@schwartz_ht @internetbazi @KulierJarolina Ist...</td>\n      <td>HOF</td>\n      <td>OFFN</td>\n      <td>hasoc_2020_de_1245</td>\n    </tr>\n    <tr>\n      <th>2129</th>\n      <td>1129250614708297728</td>\n      <td>ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU...</td>\n      <td>HOF</td>\n      <td>OFFN</td>\n      <td>hasoc_2020_de_1513</td>\n    </tr>\n    <tr>\n      <th>1510</th>\n      <td>1124698624632545281</td>\n      <td>\"Ach scheiß FridaysforFuture! Die sollen erstm...</td>\n      <td>HOF</td>\n      <td>HATE</td>\n      <td>hasoc_2020_de_3139</td>\n    </tr>\n    <tr>\n      <th>1147</th>\n      <td>1131670132529487872</td>\n      <td>@champagnenihal er stürzt ab mit diese drogens...</td>\n      <td>NOT</td>\n      <td>NONE</td>\n      <td>hasoc_2020_de_1491</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>1128982363785048066</td>\n      <td>heimlich ficken deutsche blondine fickt frauen...</td>\n      <td>HOF</td>\n      <td>PRFN</td>\n      <td>hasoc_2020_de_275</td>\n    </tr>\n    <tr>\n      <th>2128</th>\n      <td>1132685820991823872</td>\n      <td>Vergleicht Gauland gerade wirklich die Volkska...</td>\n      <td>NOT</td>\n      <td>NONE</td>\n      <td>hasoc_2020_de_2565</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>1129299763566981120</td>\n      <td>@CCrunk9 Was erwartet der hurensohn was passiert</td>\n      <td>HOF</td>\n      <td>PRFN</td>\n      <td>hasoc_2020_de_2906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-06c38179-c7e5-4fbe-b82c-e4e68acc16c5"},"source":"LE = LabelEncoder()\ndf['task1'] = LE.fit_transform(df['task1'])\ndf['task2'] = LE.fit_transform(df['task2'])\ndf.head()","execution_count":157,"outputs":[{"output_type":"execute_result","execution_count":157,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":5,"columns":[{"name":"tweet_id","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":1123576753199484900,"max":1133388798925189100,"histogram":[{"bin_start":1123576753199484900,"bin_end":1124557957772055300,"count":1},{"bin_start":1124557957772055300,"bin_end":1125539162344625800,"count":0},{"bin_start":1125539162344625800,"bin_end":1126520366917196200,"count":0},{"bin_start":1126520366917196200,"bin_end":1127501571489766700,"count":1},{"bin_start":1127501571489766700,"bin_end":1128482776062337000,"count":0},{"bin_start":1128482776062337000,"bin_end":1129463980634907400,"count":1},{"bin_start":1129463980634907400,"bin_end":1130445185207477900,"count":0},{"bin_start":1130445185207477900,"bin_end":1131426389780048300,"count":1},{"bin_start":1131426389780048300,"bin_end":1132407594352618800,"count":0},{"bin_start":1132407594352618800,"bin_end":1133388798925189100,"count":1}]}},{"name":"text","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd","count":1},{"name":"Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL","count":1},{"name":"3 others","count":3}]}},{"name":"task1","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":4}]}},{"name":"task2","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":1,"max":1,"histogram":[{"bin_start":0.5,"bin_end":0.6,"count":0},{"bin_start":0.6,"bin_end":0.7,"count":0},{"bin_start":0.7,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.1,"count":5},{"bin_start":1.1,"bin_end":1.2000000000000002,"count":0},{"bin_start":1.2000000000000002,"bin_end":1.3,"count":0},{"bin_start":1.3,"bin_end":1.4,"count":0},{"bin_start":1.4,"bin_end":1.5,"count":0}]}},{"name":"ID","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"hasoc_2020_de_2684","count":1},{"name":"hasoc_2020_de_2440","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"tweet_id":1133388798925189100,"text":"Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd","task1":1,"task2":1,"ID":"hasoc_2020_de_2684","_deepnote_index_column":0},{"tweet_id":1131117000279961600,"text":"Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL","task1":0,"task2":1,"ID":"hasoc_2020_de_2440","_deepnote_index_column":1},{"tweet_id":1127134592517980200,"text":"RT @NDRinfo: Die deutsche Klimaaktivistin Luisa Neubauer wirft Kanzlerin Merkel wegen ihrer fehlenden Unterstützung für den europäischen Kl…","task1":1,"task2":1,"ID":"hasoc_2020_de_1042","_deepnote_index_column":2},{"tweet_id":1128897106171842600,"text":"@ruhrbahn jeden Morgen eine neue „Fahrzeugstörung“, ihr seid einfach nur zum Kotzen","task1":1,"task2":1,"ID":"hasoc_2020_de_774","_deepnote_index_column":3},{"tweet_id":1123576753199484900,"text":"@Junge_Freiheit Die Inkas hatten sich schon dämlich angestellt, bei den spanischen Eindringlingen, aber der Deutsche toppt dann doch alles","task1":1,"task2":1,"ID":"hasoc_2020_de_559","_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"              tweet_id                                               text  \\\n0  1133388798925189122  Deutsche rothaarige porno reife deutsche fraue...   \n1  1131117000279961600  Lehrstück auch, wie in der linken Jammerfemini...   \n2  1127134592517980161  RT @NDRinfo: Die deutsche Klimaaktivistin Luis...   \n3  1128897106171842560  @ruhrbahn jeden Morgen eine neue „Fahrzeugstör...   \n4  1123576753199484928  @Junge_Freiheit Die Inkas hatten sich schon dä...   \n\n   task1  task2                  ID  \n0      1      1  hasoc_2020_de_2684  \n1      0      1  hasoc_2020_de_2440  \n2      1      1  hasoc_2020_de_1042  \n3      1      1   hasoc_2020_de_774  \n4      1      1   hasoc_2020_de_559  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n      <th>task1</th>\n      <th>task2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1133388798925189122</td>\n      <td>Deutsche rothaarige porno reife deutsche fraue...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_2684</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1131117000279961600</td>\n      <td>Lehrstück auch, wie in der linken Jammerfemini...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>hasoc_2020_de_2440</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1127134592517980161</td>\n      <td>RT @NDRinfo: Die deutsche Klimaaktivistin Luis...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_1042</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1128897106171842560</td>\n      <td>@ruhrbahn jeden Morgen eine neue „Fahrzeugstör...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_774</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1123576753199484928</td>\n      <td>@Junge_Freiheit Die Inkas hatten sich schon dä...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_559</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-1cd64ffd-062b-483d-ae4c-1c1e60b931d6"},"source":"df","execution_count":158,"outputs":[{"output_type":"execute_result","execution_count":158,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":2452,"column_count":5,"columns":[{"name":"tweet_id","dtype":"int64","stats":{"unique_count":2452,"nan_count":0,"min":1123475540441169900,"max":1134698944267804700,"histogram":[{"bin_start":1123475540441169900,"bin_end":1124597880823833300,"count":234},{"bin_start":1124597880823833300,"bin_end":1125720221206496900,"count":236},{"bin_start":1125720221206496900,"bin_end":1126842561589160300,"count":194},{"bin_start":1126842561589160300,"bin_end":1127964901971823900,"count":222},{"bin_start":1127964901971823900,"bin_end":1129087242354487300,"count":234},{"bin_start":1129087242354487300,"bin_end":1130209582737150700,"count":258},{"bin_start":1130209582737150700,"bin_end":1131331923119814300,"count":191},{"bin_start":1131331923119814300,"bin_end":1132454263502477700,"count":255},{"bin_start":1132454263502477700,"bin_end":1133576603885141200,"count":349},{"bin_start":1133576603885141200,"bin_end":1134698944267804700,"count":279}]}},{"name":"text","dtype":"object","stats":{"unique_count":2452,"nan_count":0,"categories":[{"name":"Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd","count":1},{"name":"Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL","count":1},{"name":"2450 others","count":2450}]}},{"name":"task1","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":601},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1851}]}},{"name":"task2","dtype":"int64","stats":{"unique_count":4,"nan_count":0,"min":0,"max":3,"histogram":[{"bin_start":0,"bin_end":0.3,"count":102},{"bin_start":0.3,"bin_end":0.6,"count":0},{"bin_start":0.6,"bin_end":0.8999999999999999,"count":0},{"bin_start":0.8999999999999999,"bin_end":1.2,"count":1860},{"bin_start":1.2,"bin_end":1.5,"count":0},{"bin_start":1.5,"bin_end":1.7999999999999998,"count":0},{"bin_start":1.7999999999999998,"bin_end":2.1,"count":126},{"bin_start":2.1,"bin_end":2.4,"count":0},{"bin_start":2.4,"bin_end":2.6999999999999997,"count":0},{"bin_start":2.6999999999999997,"bin_end":3,"count":364}]}},{"name":"ID","dtype":"object","stats":{"unique_count":2452,"nan_count":0,"categories":[{"name":"hasoc_2020_de_2684","count":1},{"name":"hasoc_2020_de_2440","count":1},{"name":"2450 others","count":2450}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"tweet_id":1133388798925189100,"text":"Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd","task1":1,"task2":1,"ID":"hasoc_2020_de_2684","_deepnote_index_column":0},{"tweet_id":1131117000279961600,"text":"Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL","task1":0,"task2":1,"ID":"hasoc_2020_de_2440","_deepnote_index_column":1},{"tweet_id":1127134592517980200,"text":"RT @NDRinfo: Die deutsche Klimaaktivistin Luisa Neubauer wirft Kanzlerin Merkel wegen ihrer fehlenden Unterstützung für den europäischen Kl…","task1":1,"task2":1,"ID":"hasoc_2020_de_1042","_deepnote_index_column":2},{"tweet_id":1128897106171842600,"text":"@ruhrbahn jeden Morgen eine neue „Fahrzeugstörung“, ihr seid einfach nur zum Kotzen","task1":1,"task2":1,"ID":"hasoc_2020_de_774","_deepnote_index_column":3},{"tweet_id":1123576753199484900,"text":"@Junge_Freiheit Die Inkas hatten sich schon dämlich angestellt, bei den spanischen Eindringlingen, aber der Deutsche toppt dann doch alles","task1":1,"task2":1,"ID":"hasoc_2020_de_559","_deepnote_index_column":4},{"tweet_id":1128743783393312800,"text":"RT @technosteron: leute die 'boar' schreiben lassen sich bestimmt auch von ihren analphabetischen Vater in arsch ficken","task1":0,"task2":1,"ID":"hasoc_2020_de_1969","_deepnote_index_column":5},{"tweet_id":1134139256375525400,"text":"ich habe mir gerade diese dorne aus meinem arsch gezogen https://t.co/x40mpKgBZN","task1":1,"task2":3,"ID":"hasoc_2020_de_2926","_deepnote_index_column":6},{"tweet_id":1132759271618293800,"text":"RT @Schrammi44: Er ist einfach der GOAT @Bouncaay https://t.co/bb9b5SWaKi","task1":1,"task2":1,"ID":"hasoc_2020_de_2994","_deepnote_index_column":7},{"tweet_id":1131107693123526700,"text":"KFM Deutsche Mittelstand AG: Deutscher Mittelstandsanleihen FONDS zeichnet neue Hörmann-Anleihe mit Mindest-Kupon 4… https://t.co/5HeRedf9cx","task1":1,"task2":1,"ID":"hasoc_2020_de_1574","_deepnote_index_column":8},{"tweet_id":1133093570229997600,"text":"@mentalshards_ ich will jetzt nicht offenden oder so, ich will einfach nur, das die scheiße weniger wird","task1":1,"task2":3,"ID":"hasoc_2020_de_1480","_deepnote_index_column":9},{"tweet_id":1126898700683501600,"text":"RT @SteinbachErika: Einfach skandalös! https://t.co/dC5NXFbviQ","task1":1,"task2":1,"ID":"hasoc_2020_de_3224","_deepnote_index_column":10},{"tweet_id":1128386596452085800,"text":"\"Das geht nicht.\"\n\"Doch. Zugreifen!\"\n\nAch halt doch die Fresse. Scheiß Werbung.","task1":1,"task2":3,"ID":"hasoc_2020_de_410","_deepnote_index_column":11},{"tweet_id":1124322999551840300,"text":"@holzraspler @Apple @Botometer Ja Touchbar da kotzen die meisten, unnötig wie sau\nXcode: seit 2003 aber egal… https://t.co/BQnklEeDMw","task1":1,"task2":1,"ID":"hasoc_2020_de_3441","_deepnote_index_column":12},{"tweet_id":1123489897548001300,"text":"XXL weights pack v1.1.1.0 FS19 https://t.co/zTpqTSGujE","task1":1,"task2":1,"ID":"hasoc_2020_de_1685","_deepnote_index_column":13},{"tweet_id":1128294539872165900,"text":"@Kizer_Media @Zero_Aquila @tagesschau (1) https://t.co/oiwSZhwsPt\n(2) https://t.co/l12aHaxDIy\n(3) Agora ist etwas z… https://t.co/kHsKBubRsk","task1":1,"task2":1,"ID":"hasoc_2020_de_2215","_deepnote_index_column":14},{"tweet_id":1130919100451106800,"text":"deutsche lesben free deutsches ehepaar fickt porno deutscher porno ffentlichkeit free porno muschilecken deutsch https://t.co/GRHmv0F1J9","task1":0,"task2":3,"ID":"hasoc_2020_de_3404","_deepnote_index_column":15},{"tweet_id":1129884743137398800,"text":"RT @johannesgrunert: Armselig, peinlich und dumm 👎\n#ESC2019 #Iceland https://t.co/sWesovTO9q","task1":0,"task2":2,"ID":"hasoc_2020_de_1996","_deepnote_index_column":16},{"tweet_id":1127236438624473100,"text":"@FlorianFOLI 1. Waren es 2\n2. Musst du nicht, aber hat halt eh keinen Effekt\n3. Ist der Sieg immernoch eher lucky als verdient","task1":1,"task2":1,"ID":"hasoc_2020_de_3198","_deepnote_index_column":17},{"tweet_id":1132021174773461000,"text":"@joahlen Da ist nichts mehr zu retten. Hirn schon gar nicht. Wo nichts ist, kann nichts werden.","task1":1,"task2":1,"ID":"hasoc_2020_de_1803","_deepnote_index_column":18},{"tweet_id":1124358286218649600,"text":"süggest movies plz ty :&gt;","task1":1,"task2":1,"ID":"hasoc_2020_de_2391","_deepnote_index_column":19},{"tweet_id":1132029215279394800,"text":"@EinJanik @MHP2002 Ja ok du arsch","task1":0,"task2":3,"ID":"hasoc_2020_de_1228","_deepnote_index_column":20},{"tweet_id":1131180745324728300,"text":"@KONRAD1000 @LVZ Wenn man Ihnen eine Körperverletzung antut, wird man hoffentlich genauso handeln und Täter-Opfer R… https://t.co/cq2bdfZ3ly","task1":0,"task2":1,"ID":"hasoc_2020_de_2683","_deepnote_index_column":21},{"tweet_id":1128638657353650200,"text":"RT @maxotte_says: \"#Merkel muss weg!\" darf bei #Kundgebug in #Wuppertal nicht gerufen werden. Willkommen in der dunklen Zeit! #Repression #…","task1":1,"task2":1,"ID":"hasoc_2020_de_1618","_deepnote_index_column":22},{"tweet_id":1125703697332351000,"text":"@heuteshow Und linke Journalisten wollen die humanitäre Aufnahme von Flüchtlingen, halt nur nicht in ihrer Wohngege… https://t.co/qr9WNCNZtP","task1":1,"task2":1,"ID":"hasoc_2020_de_2427","_deepnote_index_column":23},{"tweet_id":1131536384542629900,"text":"@zeitonline was haben die beiden Kopalken nur im Kopf?","task1":1,"task2":1,"ID":"hasoc_2020_de_1804","_deepnote_index_column":24},{"tweet_id":1132713729852870700,"text":"@tagesschau Die Hasselfeld, Wahlkampfhelferin der Grünen. https://t.co/FLQKHv0jFb","task1":1,"task2":1,"ID":"hasoc_2020_de_1970","_deepnote_index_column":25},{"tweet_id":1131482508711874600,"text":"Gescher -  Christdemokrat möchte, dass Gänse aus dem Naturschutzgebiet Berkeltal verschwinden. Dreck.… https://t.co/b8060gV3hK","task1":1,"task2":1,"ID":"hasoc_2020_de_776","_deepnote_index_column":26},{"tweet_id":1133037848913961000,"text":"Desnudas xxx gratis, deutsche schwgerin porno! Italiani porno gratis, canal chat gratis. https://t.co/wJWFnVuhVe","task1":0,"task2":1,"ID":"hasoc_2020_de_2755","_deepnote_index_column":27},{"tweet_id":1127483176929116200,"text":"@SatireUNfreiesD So ist es, der Islam ist keine Religion des Friedens! https://t.co/NSyjYNJ477","task1":0,"task2":0,"ID":"hasoc_2020_de_3043","_deepnote_index_column":28},{"tweet_id":1127626345285136400,"text":"@hubertus_zulu @nilzee011 @LinoTobi @dieserRoman Wie dumm kann man eigentlich sein?","task1":1,"task2":1,"ID":"hasoc_2020_de_2820","_deepnote_index_column":29},{"tweet_id":1130853572814393300,"text":"porno estar gratis unterwrfige deutsche frauen porno mutter fisten sexy mdchen in hotpants https://t.co/FvAKmqCJCu","task1":0,"task2":3,"ID":"hasoc_2020_de_759","_deepnote_index_column":30},{"tweet_id":1123649289463451600,"text":"@LianePoost @welt Ich beziehe mich nicht nur auf die DDR. Sozialistische System waren schon immer unproduktiv und w… https://t.co/lrQeZvtXF6","task1":1,"task2":1,"ID":"hasoc_2020_de_2431","_deepnote_index_column":31},{"tweet_id":1133836125641105400,"text":"@brunipamder237 @Sonnenbebrillt3 @Altonese86 Wir halt einfach immer","task1":1,"task2":1,"ID":"hasoc_2020_de_3466","_deepnote_index_column":32},{"tweet_id":1125537229575807000,"text":"Ohne scheiss, das is doch einfach unfair","task1":1,"task2":1,"ID":"hasoc_2020_de_1268","_deepnote_index_column":33},{"tweet_id":1131886101415759900,"text":"RT @yungatakan: niemand:\n\nvertrau mir niemand\n\nnichtmal merkel:\n\ndeutsche kinder von afd wählern: https://t.co/EHO9ip8u6z","task1":1,"task2":1,"ID":"hasoc_2020_de_520","_deepnote_index_column":34},{"tweet_id":1125064237922046000,"text":"RT @Deacon__Blues: Da @TwitterDE anscheinend ein Haufen Idioten sind, die momentan auf Zuruf von afd-Bots Leute wie @RAStadler oder @Sawsan…","task1":1,"task2":1,"ID":"hasoc_2020_de_2861","_deepnote_index_column":35},{"tweet_id":1123704385861824500,"text":"Ist es nicht Extrem Dumm Zeit für Geld zu verkaufen ? Arbeitskraft/Stundenlohn/ Zeit. Ist es nicht erbärmlich Wisse… https://t.co/rymiIWWH7O","task1":1,"task2":1,"ID":"hasoc_2020_de_3475","_deepnote_index_column":36},{"tweet_id":1129724864649150500,"text":"#NWBRB58\n\n+++ Ausfall +++ Osnabrück Hbf (ab 13:26) -&gt; Delmenhorst (an 15:24)  am 18.05.2019  Grund: Fahrzeugstörung… https://t.co/0yqkl1fESN","task1":1,"task2":1,"ID":"hasoc_2020_de_1920","_deepnote_index_column":37},{"tweet_id":1131183823948013600,"text":"@JoergRMayer @lichtmesz Wirst schon noch deinen Versorgungsposten bekommen. So wie jeder hartnäckige Karrierist. Ab… https://t.co/UsQERWQxXm","task1":1,"task2":1,"ID":"hasoc_2020_de_474","_deepnote_index_column":38},{"tweet_id":1127239156508311600,"text":"@Hartes_Geld #Nazis waren schon immer #Linke, halt miese #Sozialisten - wie die #Antifanten der #kriminellen #Antifa.","task1":0,"task2":0,"ID":"hasoc_2020_de_548","_deepnote_index_column":39},{"tweet_id":1128056961906413600,"text":"RT @451_grad: #Benziner dreckiger als #Diesel?\nFür @Die_Gruenen schon, aber stimmt das auch? 🤷‍\n📍 Noch mehr dazu: https://t.co/NSfD2FOCqd 📍…","task1":1,"task2":1,"ID":"hasoc_2020_de_3163","_deepnote_index_column":40},{"tweet_id":1128589927934042100,"text":"@xRxss_ Ich hab halt überall Wireless Charging Pads, wär sau dumm. 😅","task1":1,"task2":1,"ID":"hasoc_2020_de_2813","_deepnote_index_column":41},{"tweet_id":1133741065901609000,"text":"RT @Katerfrieden: Ich bin dafür dass die Grünen das Geschoß ertragen. https://t.co/E8azul8WmQ","task1":1,"task2":1,"ID":"hasoc_2020_de_1035","_deepnote_index_column":42},{"tweet_id":1130112447690530800,"text":"RT @braindead_xy: @tagesschau Deutsche Regierung stürzt ausländische Regierung.\n\nUnd feiert das!\n\nIhr Deutschen (87%) kotzt mich so an! Ihr…","task1":1,"task2":0,"ID":"hasoc_2020_de_2368","_deepnote_index_column":43},{"tweet_id":1134362607249449000,"text":"@johannesw82 @bellefired @VollzeitJonas 🤦‍♂️wie kann man nur so dumm sein 😂🤣","task1":1,"task2":1,"ID":"hasoc_2020_de_3140","_deepnote_index_column":44},{"tweet_id":1128223819708162000,"text":"*hier nach belieben auch Politik(er) einfügen.","task1":1,"task2":1,"ID":"hasoc_2020_de_2663","_deepnote_index_column":45},{"tweet_id":1129130045224685600,"text":"Schlagabtausch  im ZDF..... regierungsgesteuerte SCHEISSE mit dementsprechendem Publikum vor Ort!!!!!!!!!!","task1":0,"task2":2,"ID":"hasoc_2020_de_1266","_deepnote_index_column":46},{"tweet_id":1123557279037644800,"text":"ich verleih nie wieder mein scheiß crusher","task1":1,"task2":3,"ID":"hasoc_2020_de_874","_deepnote_index_column":47},{"tweet_id":1131663094458003500,"text":"RT @neomagazin: Deutsche Carpool Karaoke #DoTheyKnowItsEurope https://t.co/JENTkGq11j","task1":1,"task2":1,"ID":"hasoc_2020_de_1344","_deepnote_index_column":48},{"tweet_id":1131180690777751600,"text":"WELT: Horst Seehofer kündigt kriminellen Clans den Kampf an WELT: Horst Seehofer kündigt kriminellen Clans den Kamp… https://t.co/RT1oMHEGvq","task1":1,"task2":1,"ID":"hasoc_2020_de_1282","_deepnote_index_column":49},{"tweet_id":1125641143490895900,"text":"RT @Das_KGB: Zum kotzen https://t.co/LGsgiE4Jjb","task1":1,"task2":1,"ID":"hasoc_2020_de_2590","_deepnote_index_column":50},{"tweet_id":1131966011295584300,"text":"RT @Hihi97948034: Mero ist ein Hurensohn","task1":0,"task2":3,"ID":"hasoc_2020_de_2658","_deepnote_index_column":51},{"tweet_id":1133326932911808500,"text":"Wenn du schon seit knapp 10 Monaten mit einem Kollegen zusammen arbeitest, nie ein Wort Deutsch mit ihm sprachst, w… https://t.co/94SZrnqotc","task1":1,"task2":1,"ID":"hasoc_2020_de_1062","_deepnote_index_column":52},{"tweet_id":1128699319576612900,"text":"@domi_twr Einfach kein assi sein der im kreisel nicht blinkt. Und auf der autobahn wäre blinken auch vorteilhaft","task1":1,"task2":1,"ID":"hasoc_2020_de_2822","_deepnote_index_column":53},{"tweet_id":1125431663163531300,"text":"hab glaube echt gerade den größten hurensohn busfahrer deutschlands","task1":0,"task2":2,"ID":"hasoc_2020_de_1305","_deepnote_index_column":54},{"tweet_id":1125778708260896800,"text":"RT @tagesschau: EVP-Spitzenkandidat Weber will Europa einen https://t.co/ooAUUBLZTj #Europawahl2019 #Weber #Wahlarena","task1":1,"task2":1,"ID":"hasoc_2020_de_1254","_deepnote_index_column":55},{"tweet_id":1124427395782643700,"text":"Warum haben linke Politiker eigentlich kaum ein richtiges Berufsleben?","task1":1,"task2":1,"ID":"hasoc_2020_de_94","_deepnote_index_column":56},{"tweet_id":1131875879913742300,"text":"Aaaain gabriel ❤️😍","task1":1,"task2":1,"ID":"hasoc_2020_de_2946","_deepnote_index_column":57},{"tweet_id":1129105403709743100,"text":"hört sich an wie Hintergrundmusik für  Mobilfunkwerbung #escorf","task1":1,"task2":1,"ID":"hasoc_2020_de_1384","_deepnote_index_column":58},{"tweet_id":1126030790100435000,"text":"@Der_OG_Julius ist der echt mit der zusammen","task1":1,"task2":1,"ID":"hasoc_2020_de_944","_deepnote_index_column":59},{"tweet_id":1132576718722617300,"text":"RT @TopTomCH: Aufruf zum ficken!  Da Er @MichaelHaenel3 seit über 3 Monaten keinen echten Schwanz mehr im Arsch hatte wird eine geiler Top…","task1":0,"task2":3,"ID":"hasoc_2020_de_336","_deepnote_index_column":60},{"tweet_id":1130882173794496500,"text":"Damit wir vor #Scheisse bewahrt werden: #FusionBleibt !! https://t.co/0mLIf78Gms","task1":1,"task2":3,"ID":"hasoc_2020_de_2289","_deepnote_index_column":61},{"tweet_id":1132919389169291300,"text":"@go73729092 @Ralf_Stegner Machen doch alle privaten Medien so!","task1":1,"task2":1,"ID":"hasoc_2020_de_1916","_deepnote_index_column":62},{"tweet_id":1134044712552357900,"text":"@xmerve021 ja ist halt echt","task1":1,"task2":1,"ID":"hasoc_2020_de_923","_deepnote_index_column":63},{"tweet_id":1125980915635773400,"text":"RT @Jesus_folgen: @HaraldBecker80 Als Gegenbeispiel gibt es ein Video, in dem gezeigt wird, wie die DEUTSCHE BUNDESKANZLERIN NAMENS MERKEL…","task1":1,"task2":1,"ID":"hasoc_2020_de_260","_deepnote_index_column":64},{"tweet_id":1126848356435865600,"text":"900.000 deutsche „Flüchtlinge“ im EU-Ausland https://t.co/8RsEOvFFY0","task1":1,"task2":1,"ID":"hasoc_2020_de_1943","_deepnote_index_column":65},{"tweet_id":1125744075880136700,"text":"ALERTE Ruche4 Poids=10.2kg -&gt;https://t.co/WTpfLC1Afv  … Infos-&gt;https://t.co/nzwsT5HGPr (14:47:56) https://t.co/gjZnuXZoSy","task1":1,"task2":1,"ID":"hasoc_2020_de_154","_deepnote_index_column":66},{"tweet_id":1132456476415541200,"text":"@GG_SSBM was soll dieser scheiß man","task1":1,"task2":1,"ID":"hasoc_2020_de_3088","_deepnote_index_column":67},{"tweet_id":1131482424809054200,"text":"RT @BB12_DE: @krippmarie @KunkelMuhme Die linken Schlägertrupps sind wieder in Deutschland unterwegs. 🙄\n\n#unserwir https://t.co/mhkvN554er","task1":1,"task2":1,"ID":"hasoc_2020_de_217","_deepnote_index_column":68},{"tweet_id":1124999989590147100,"text":"RT @Atomreisfleisch: @jacky_lope Und man hört diese ganzen Elektro-Dinger nicht. Ich stelle mich schon heute auf das Geheule ein, wenn Leut…","task1":1,"task2":1,"ID":"hasoc_2020_de_2137","_deepnote_index_column":69},{"tweet_id":1133839673988718600,"text":"* rachsüchtige Frauen. Es gibt hasserfüllte Frauen,  Frauen die betrügen und lügen. \n\nHört auf eure Mysogynie hinte… https://t.co/k2mvmkkVsC","task1":1,"task2":1,"ID":"hasoc_2020_de_1965","_deepnote_index_column":70},{"tweet_id":1123511443687727100,"text":"RT @AchimSpiegel: \"Frau Merkel, es bewegt mich, dass Sie noch immer nicht vor Gericht stehen und tatsächlich noch Kanzlerin sind.\" https://…","task1":0,"task2":1,"ID":"hasoc_2020_de_167","_deepnote_index_column":71},{"tweet_id":1132931582032076800,"text":"die arroganz der leute aus den westlichen bundesländern ist so ekelhaft und abstoßend, ich könnt kotzen \nwie kann m… https://t.co/uXdFuejOZB","task1":0,"task2":1,"ID":"hasoc_2020_de_1886","_deepnote_index_column":72},{"tweet_id":1124444323997716500,"text":"@Pokehita97 Hey sorry das ich so overreacted habe!! (kenn das deutsche wort für overreact nicht sry😅) CAN U PLS FOR… https://t.co/nLPmQIhVCW","task1":1,"task2":1,"ID":"hasoc_2020_de_1962","_deepnote_index_column":73},{"tweet_id":1125147385791942700,"text":"RT @burger_ein: Radikale Moslems und nationalistische Türken Die verschwiegenenen Wähler der Grünen https://t.co/u7Jj38Ldc1","task1":0,"task2":0,"ID":"hasoc_2020_de_2217","_deepnote_index_column":74},{"tweet_id":1134550809855844400,"text":"@Obi_Jan_Kenobii @walkerjole Die haben sich mit der Zeit schon selbst entsorgt.","task1":1,"task2":1,"ID":"hasoc_2020_de_762","_deepnote_index_column":75},{"tweet_id":1124388900464676900,"text":"Siege &gt; CoD","task1":1,"task2":1,"ID":"hasoc_2020_de_227","_deepnote_index_column":76},{"tweet_id":1133480201197948900,"text":"RT @RolandTichy: Keiner redet vom Wetter, nur wir: Die Grünen sitzen jetzt ihrerseits in der Klimafalle: Radiale Maßnahmen sind hübsche For…","task1":1,"task2":1,"ID":"hasoc_2020_de_59","_deepnote_index_column":77},{"tweet_id":1130111608838139900,"text":"@MartinaHolst @rudolfuz Politiker kriegen bestimmt wieder Extrarechte","task1":1,"task2":1,"ID":"hasoc_2020_de_2573","_deepnote_index_column":78},{"tweet_id":1124744715868549100,"text":"Die Bigotterie von Grünen ist kaum aushaltbar. FCK NAZIS auf den Plakaten aber neben ihnen Stehen und Flyer verteil… https://t.co/D2SoJv7zU0","task1":1,"task2":1,"ID":"hasoc_2020_de_980","_deepnote_index_column":79},{"tweet_id":1130096303801872400,"text":"@STARGAMESTV69 @verankernLUL @Papaplatte @tim0cy arsch https://t.co/ezeiGmpJAb","task1":1,"task2":1,"ID":"hasoc_2020_de_792","_deepnote_index_column":80},{"tweet_id":1131161308911398900,"text":"EINFACH NUR AUS ZEITVERTREIB","task1":1,"task2":1,"ID":"hasoc_2020_de_1069","_deepnote_index_column":81},{"tweet_id":1133436311979868200,"text":"@Tlaytweet hilfe diese frau was befindet sich in ihrem kopf an der stelle, wo normale menschen ein hirn haben?","task1":0,"task2":2,"ID":"hasoc_2020_de_2052","_deepnote_index_column":82},{"tweet_id":1124546006505357300,"text":"RT @PeterZobel4: @ZDFheute \"Keine Zusammenarbeit mit rechten Parteien\". Also gehört Merkel einer linken Partei an? Oder was soll das heißen?","task1":1,"task2":1,"ID":"hasoc_2020_de_3433","_deepnote_index_column":83},{"tweet_id":1129294617134784500,"text":"Gutschein \"-50% REBELITA Fleecepulover für Frauen!\" https://t.co/EC0VX6LWYj","task1":1,"task2":1,"ID":"hasoc_2020_de_1435","_deepnote_index_column":84},{"tweet_id":1129510569256587300,"text":"#Hofheim Hobbyhure-sucht-Taschengeld: Anna zarte St*te - Zarte, deutsche St*te  will d... https://t.co/z5s5n4K9fl https://t.co/FJqJxBuyIS","task1":0,"task2":3,"ID":"hasoc_2020_de_2513","_deepnote_index_column":85},{"tweet_id":1132621144824139800,"text":"„Die Wähler sind halt zu dumm, sodass wir ihnen unsere Positionen nicht vermitteln konnten. Diese Verachtung für si… https://t.co/rMolA6HBKi","task1":0,"task2":0,"ID":"hasoc_2020_de_862","_deepnote_index_column":86},{"tweet_id":1125732050818994200,"text":"benutze dein Gehirn! \nNiemand hat mich bezahlt :(\n#EducaAunFifi","task1":1,"task2":1,"ID":"hasoc_2020_de_682","_deepnote_index_column":87},{"tweet_id":1124636523788406800,"text":"RT @Ulrik_von: @spdberlin @gabischoff @RaedSalehBerlin Das neue Logo ist auch schon fertig. https://t.co/BZeIk9yBzf","task1":1,"task2":1,"ID":"hasoc_2020_de_2005","_deepnote_index_column":88},{"tweet_id":1132616921155743700,"text":"@RainerWinklerDE Lieben Heteros, jetz habter richtig Scheiße am Arsch, etzala fliegt ihr raus.","task1":1,"task2":1,"ID":"hasoc_2020_de_1763","_deepnote_index_column":89},{"tweet_id":1134560620349706200,"text":"@Piratenpartei Was soll dieses Gequatsche von Nazis? Wer gegen ILLEGALE Migration und linken Gesinnungsterror ist,… https://t.co/FRLCTDCZoE","task1":1,"task2":1,"ID":"hasoc_2020_de_3104","_deepnote_index_column":90},{"tweet_id":1134399169010065400,"text":"@nicerlucas Scheiß alki","task1":0,"task2":3,"ID":"hasoc_2020_de_2162","_deepnote_index_column":91},{"tweet_id":1128376953747247100,"text":"@DanyVause Deutsche Kultur https://t.co/nUdQfuCLsc","task1":1,"task2":1,"ID":"hasoc_2020_de_2690","_deepnote_index_column":92},{"tweet_id":1128709427824136200,"text":"@maynstream Bin halt aber schon tip-top motivator https://t.co/AvRaWLUEMI","task1":1,"task2":1,"ID":"hasoc_2020_de_380","_deepnote_index_column":93},{"tweet_id":1134189608986710000,"text":"wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt.","task1":0,"task2":1,"ID":"hasoc_2020_de_2475","_deepnote_index_column":94},{"tweet_id":1128390300026789900,"text":"ich hasse meine eltern ohne spass diese dummen hurensohne ich bin fucking 14 und die heulen rum wenn ich EINE fuxki… https://t.co/ARddcXxbDn","task1":0,"task2":3,"ID":"hasoc_2020_de_1320","_deepnote_index_column":95},{"tweet_id":1127530950055874600,"text":"scheiß bauchschmerzen","task1":1,"task2":1,"ID":"hasoc_2020_de_2707","_deepnote_index_column":96},{"tweet_id":1132663154943701000,"text":"@DrThomasBruns @VonSchwer Wer tritt denn heute von den dt. Politikern noch zurück?\nDemnach hätte Merkel schon vor J… https://t.co/rRzVlfdkMK","task1":1,"task2":1,"ID":"hasoc_2020_de_2890","_deepnote_index_column":97},{"tweet_id":1132690636027641900,"text":"RT @ZDFheute: Wer wählte die Grünen bei der #Europawahl2019? https://t.co/bDmYsTVuTf","task1":1,"task2":1,"ID":"hasoc_2020_de_2065","_deepnote_index_column":98},{"tweet_id":1125638278768681000,"text":"@tukyabitch lass linken","task1":1,"task2":1,"ID":"hasoc_2020_de_870","_deepnote_index_column":99}],"rows_bottom":[{"tweet_id":1129829114053947400,"text":"@MRSL0NELYY JEDES MAL EINFACH","task1":1,"task2":1,"ID":"hasoc_2020_de_361","_deepnote_index_column":2352},{"tweet_id":1129651678226128900,"text":"RT @PrimeVideoDE: Bäm, Ninja Style! Fleabag ist zurück.\n#Fleabag #Fleabag2 @fleabag #PhoebeWallerBridge #PrimeVideo https://t.co/0X2Mk5ikxa","task1":1,"task2":1,"ID":"hasoc_2020_de_38","_deepnote_index_column":2353},{"tweet_id":1132704041048387600,"text":"@Magic_Cauldron @Kathari37057708 Früher hat man sich auch mit \"heil hitler\" begrüßt....","task1":1,"task2":1,"ID":"hasoc_2020_de_1510","_deepnote_index_column":2354},{"tweet_id":1133666210187546600,"text":"RT @32etr02: #Deutsche #Doppelmoral und deutsche #Hetze gegen #Muslime, vor allem gegen das #Kopftuch.\n\nWas fressen diese komischen #heuchl…","task1":1,"task2":3,"ID":"hasoc_2020_de_1405","_deepnote_index_column":2355},{"tweet_id":1130916013413937200,"text":"RT @FurzkissenN: Allah ist ein hurensohn","task1":0,"task2":0,"ID":"hasoc_2020_de_2400","_deepnote_index_column":2356},{"tweet_id":1125466853365645300,"text":"@VetovsVictory Löblich.\n\nIch kann mir aber nicht noch mehr Scheiße in den Kopf packen.","task1":1,"task2":1,"ID":"hasoc_2020_de_2392","_deepnote_index_column":2357},{"tweet_id":1125746495993544700,"text":"Kleine, dicke Frauen in weißen Latzhosen haben sich auch nicht diskriminiert gefühlt.\n\nUnd die waren nicht nur sauber, sondern rein!","task1":1,"task2":1,"ID":"hasoc_2020_de_378","_deepnote_index_column":2358},{"tweet_id":1134400817371516900,"text":"@lildidgeridoo1 @OlafWedekind Einfach nur noch cringe","task1":1,"task2":1,"ID":"hasoc_2020_de_2923","_deepnote_index_column":2359},{"tweet_id":1128252580046487600,"text":"RT @SteinbachErika: Hier geht es vor allem um die Pro-Pädophilenpolitik der Grünen. Bis zum heutigen Tage haben die Grünen die Opfer nicht…","task1":0,"task2":0,"ID":"hasoc_2020_de_3498","_deepnote_index_column":2360},{"tweet_id":1131310244431310800,"text":"RT @rbarris: @ReutersPolitics The Deutsche Abides","task1":1,"task2":1,"ID":"hasoc_2020_de_3468","_deepnote_index_column":2361},{"tweet_id":1132389732468645900,"text":"lsterne frauen free porn nutten chatarrero gratis dicke frauen masturbieren https://t.co/9gGfVrRYFx","task1":0,"task2":3,"ID":"hasoc_2020_de_1656","_deepnote_index_column":2362},{"tweet_id":1133406440138448900,"text":"RT @_Zwirn: Leute, die ihre IBAN ohne Leerzeichen angeben, sollen in der Hölle schmoren und mit dem nackten Arsch auf des Teufels Dreizack…","task1":0,"task2":3,"ID":"hasoc_2020_de_1367","_deepnote_index_column":2363},{"tweet_id":1129884160112382000,"text":"RT @MagnumPhotos: Susan Meiselas wins the 2019 Deutsche Börse prize, Sohrab Hura collaborates with fashion designer Kiko Kostadinov, plus m…","task1":1,"task2":1,"ID":"hasoc_2020_de_1395","_deepnote_index_column":2364},{"tweet_id":1123586014205943800,"text":"Das ist mir zu dumm","task1":1,"task2":1,"ID":"hasoc_2020_de_3390","_deepnote_index_column":2365},{"tweet_id":1129737514665893900,"text":"RT @Manuel_k_b: Zeigt mir einen doktortitelklauenden, beim Lügen erwischten, eidbrechenden Politiker in Deutschland, der sich hinstellt und…","task1":1,"task2":1,"ID":"hasoc_2020_de_1841","_deepnote_index_column":2366},{"tweet_id":1126118258149593100,"text":"RT @schmid_claudio: Der «beliebteste» Politiker Deutschlands vereidigt gerade ein uraltes Postulat der GRÜNEN: KINDER;  liegt derzeit in ga…","task1":1,"task2":1,"ID":"hasoc_2020_de_1129","_deepnote_index_column":2367},{"tweet_id":1133665727809032200,"text":"RT @haniawiatrek: Die Grünen und Linken sind gegen Tierquälerei unterstützen aber Halal! Moslems dürfen in Deutschland Tiere quälen!","task1":1,"task2":1,"ID":"hasoc_2020_de_1303","_deepnote_index_column":2368},{"tweet_id":1124599399982751700,"text":"@Hartmantelgesc1 @MiriamOzen @Anemalon19 @tagesschau Die merken schon lange nichts mehr! Die werden rot wenn sie au… https://t.co/Gxa5649VGW","task1":1,"task2":1,"ID":"hasoc_2020_de_3311","_deepnote_index_column":2369},{"tweet_id":1132609212016734200,"text":"einfach unhatebar https://t.co/ALHeePS8Xs","task1":1,"task2":1,"ID":"hasoc_2020_de_2202","_deepnote_index_column":2370},{"tweet_id":1125413187237634000,"text":"FPÖ-Funktionärin tritt nach Hetz-Postings zurück: Nachdem erst Ende April der Braunauer FPÖ-Vizebürgermeister Chris… https://t.co/jAG8uyxRmB","task1":1,"task2":1,"ID":"hasoc_2020_de_2808","_deepnote_index_column":2371},{"tweet_id":1128114411296759800,"text":"@tagesschau Dorr Ami holt mal wieder seine ganze Gestörtheit mit Waffen raus, und die Welt, besonders Dschland badets wieder aus!","task1":1,"task2":1,"ID":"hasoc_2020_de_623","_deepnote_index_column":2372},{"tweet_id":1130021255145545700,"text":"Freiheizstatue: nuevocanario13 _teutonia YouTube Der Buntepräsident erzählt wieder mal dummen Scheiss. Darauf kann… https://t.co/6TFKu2liRB","task1":0,"task2":1,"ID":"hasoc_2020_de_1089","_deepnote_index_column":2373},{"tweet_id":1123642977027592200,"text":"@PolizeiSachsen Nazis in der \"Versammlugsbehörde\". Mal aufräumen da.","task1":1,"task2":1,"ID":"hasoc_2020_de_2787","_deepnote_index_column":2374},{"tweet_id":1127162094590283800,"text":"@alex1990glubb Und du? Was willst du jetzt? Deinen Privilegierten Arsch tätscheln lassen? Zieh Leine!","task1":0,"task2":3,"ID":"hasoc_2020_de_3489","_deepnote_index_column":2375},{"tweet_id":1131674293249683500,"text":"@asltf Der Andere da haut eine Falschinfo nach der Anderen raus und widerspricht sich selbst. Ob man das schon Dunn… https://t.co/vNoGffC85V","task1":1,"task2":1,"ID":"hasoc_2020_de_1525","_deepnote_index_column":2376},{"tweet_id":1134463786432323600,"text":"RT @akita_rocky: #Ich denke mal das der Islam da hinter steckt, denkt an den,, Syrer,, der einen Juden in Berlin mit einem Gürtel verprügel…","task1":1,"task2":1,"ID":"hasoc_2020_de_1719","_deepnote_index_column":2377},{"tweet_id":1126824016893767700,"text":"RT @_donalphonso: \"Der deutsche Gossenkomiker bezeichnet uns Österreicher alle als debil, ich wähle ganz schnell die Grünen, damit er mich…","task1":1,"task2":2,"ID":"hasoc_2020_de_3372","_deepnote_index_column":2378},{"tweet_id":1133777438914355200,"text":"RT @pinkcrazypony: Eine #Islamisierung findet NICHT statt ? Fakten im #ZDF ? \n\n#Islamisierung ist schon weitaus fortgeschrittener als WIR u…","task1":1,"task2":1,"ID":"hasoc_2020_de_35","_deepnote_index_column":2379},{"tweet_id":1131224613529280500,"text":"porno deutsche arbeits kollegin mutter und sohn deutsch bester deutscher anal porno privatsex kostenlos https://t.co/bT2WXRClhM","task1":0,"task2":3,"ID":"hasoc_2020_de_1363","_deepnote_index_column":2380},{"tweet_id":1123921973778763800,"text":"RT @VonErlenbach: @NiemaMovassat @BolzAndrea NATÜRLICH! WIR DEUTSCHE sind doch ohnehin alle Nazis, nicht wahr? \"Ist der Ruf erst ruiniert,…","task1":1,"task2":1,"ID":"hasoc_2020_de_3038","_deepnote_index_column":2381},{"tweet_id":1130808807028924400,"text":"RT @DianeKrass: Gegenwärtig befinden sich die Grünen auf einem regelrechten Höhenflug. \nWenn Sie also der Meinung sind, dass der Islam zu D…","task1":1,"task2":1,"ID":"hasoc_2020_de_899","_deepnote_index_column":2382},{"tweet_id":1130060136318459900,"text":"RT @DSarkast: Fassen wir mal grob zusammen: Du liebst dein Land?- Nazi.\n\nDu gehst nicht konform mit linken Gedankengut?- Nazi.\n\nDu übst Kri…","task1":1,"task2":1,"ID":"hasoc_2020_de_590","_deepnote_index_column":2383},{"tweet_id":1125672789485150200,"text":"RT @ichbinkoelnerin: Schööööön\nSeit einigen Tagen schon zeichnet sich der positive Trend für Marine #LePen ab, nun nimmt die 50-j Politiker…","task1":1,"task2":1,"ID":"hasoc_2020_de_816","_deepnote_index_column":2384},{"tweet_id":1132717248873934800,"text":"RT @SuseSchmitt: Zum Wahl-Ergebnis folgendes:\n\nJault mir noch einmal einer über Merkel ... CDU ... SPD ... oder Grüne die Ohren voll, gibt…","task1":1,"task2":1,"ID":"hasoc_2020_de_1555","_deepnote_index_column":2385},{"tweet_id":1131655276279619600,"text":"@bas_ton Das ist jetzt einfach nur gemein.","task1":1,"task2":1,"ID":"hasoc_2020_de_1160","_deepnote_index_column":2386},{"tweet_id":1123993524410757100,"text":"Grüne klagen gegen Bayerns Grenzpolizei https://t.co/ORCsjS9MD0","task1":1,"task2":1,"ID":"hasoc_2020_de_3410","_deepnote_index_column":2387},{"tweet_id":1124644954335264800,"text":"Schlagabtausch zwischen Schulz und Gauland ► ,,Das ist nicht mein Niveau https://t.co/LApAeP49v5","task1":1,"task2":1,"ID":"hasoc_2020_de_918","_deepnote_index_column":2388},{"tweet_id":1125498482612158500,"text":"RT @wolfgs2: Islam: Das vergessene Massaker der Türken an den Aleviten - WELT https://t.co/aLFAnx4BMn","task1":1,"task2":1,"ID":"hasoc_2020_de_1417","_deepnote_index_column":2389},{"tweet_id":1134530475865837600,"text":"@Quaelgott @jan_wiebe @tonline_news @DSchreckenberg Die linken Medien können nicht anders als überall Nazis zu sehe… https://t.co/g4Qax2yxPR","task1":0,"task2":1,"ID":"hasoc_2020_de_1198","_deepnote_index_column":2390},{"tweet_id":1124990648883601400,"text":"RT @KayeMenner: Boogie Underground @alhanda &gt; https://t.co/kWDZHID7NS","task1":1,"task2":1,"ID":"hasoc_2020_de_1918","_deepnote_index_column":2391},{"tweet_id":1132247184844492800,"text":"Hey Kuzeng\n\n       😜\n   👊/||\\_ \n    _/¯    ¯\\_\n\nWollen wir zusammen Türken sein? \n\n👋 \n    \\ 😳\n        || \\_\n    _/¯  ¯\\_","task1":1,"task2":1,"ID":"hasoc_2020_de_1518","_deepnote_index_column":2392},{"tweet_id":1127494484772651000,"text":"Hört bitte auf mir das Video in die Zeitlinie zu spülen in dem Felix von primitiven Moslems gesteinigt wird","task1":0,"task2":0,"ID":"hasoc_2020_de_231","_deepnote_index_column":2393},{"tweet_id":1130750543922548700,"text":"Dein Yokuzuna-Sumo ficke ich mit 'nem Pushkick\nWas los, du Hurensohn? Komm wieder, wenn du Luft kriegst","task1":0,"task2":3,"ID":"hasoc_2020_de_2831","_deepnote_index_column":2394},{"tweet_id":1127551594424348700,"text":"@KialoHQ lieber nicht das Chaos ist doch schon groß genug dann macht diese EU was sie eh schon macht dank Merkel u.… https://t.co/v3GrtaHSMC","task1":1,"task2":1,"ID":"hasoc_2020_de_1733","_deepnote_index_column":2395},{"tweet_id":1132514328467320800,"text":"@anandahasbi Kangen gabriel","task1":1,"task2":1,"ID":"hasoc_2020_de_3346","_deepnote_index_column":2396},{"tweet_id":1127302482105913300,"text":"Seo ist einfach oder? #internetmarketing https://t.co/CZVtKoCceo","task1":1,"task2":1,"ID":"hasoc_2020_de_1842","_deepnote_index_column":2397},{"tweet_id":1124583923013627900,"text":"@subversivaktiv @erkanallesffm Geh zurück ins Loch.  DUMMES Individuum.","task1":0,"task2":2,"ID":"hasoc_2020_de_240","_deepnote_index_column":2398},{"tweet_id":1130038246258483200,"text":"@cem_oezdemir Grüne sind russophobe Faschisten","task1":1,"task2":0,"ID":"hasoc_2020_de_2373","_deepnote_index_column":2399},{"tweet_id":1134395448637173800,"text":"RT @alfunterwegs70: Sie belügen euch ganz öffentlich und der dumme Deutsche will es einfach nicht kapieren https://t.co/TeaHVQqZZ2","task1":0,"task2":1,"ID":"hasoc_2020_de_1409","_deepnote_index_column":2400},{"tweet_id":1128772627626111000,"text":"Video: Martin Sellner: Auf einen Cafe mit der Lügenpresse (18:15) https://t.co/DktJPZxfXK","task1":0,"task2":1,"ID":"hasoc_2020_de_525","_deepnote_index_column":2401},{"tweet_id":1130840578860748800,"text":"RT @deutsch365: ‼️‼️ Donald Trump Das deutsche Volk wird Angela Merkel stürzen‼️‼️ https://t.co/TY3f7wimF5","task1":1,"task2":1,"ID":"hasoc_2020_de_2046","_deepnote_index_column":2402},{"tweet_id":1131234465974497300,"text":"Können diese “Panik-Attacken“ nicht einfach verschwinden ?!😕","task1":1,"task2":1,"ID":"hasoc_2020_de_385","_deepnote_index_column":2403},{"tweet_id":1128212927096459300,"text":"@edwin_IBOE @Martin_Sellner Rot grün sind nicht selten farben die clowns verwenden.","task1":1,"task2":1,"ID":"hasoc_2020_de_489","_deepnote_index_column":2404},{"tweet_id":1130860015265427500,"text":"RT @suanahilt: Diese #Grünen sind echt anstrengend😁 https://t.co/sHyUqKqzqe","task1":1,"task2":1,"ID":"hasoc_2020_de_1327","_deepnote_index_column":2405},{"tweet_id":1123614787143962600,"text":"@SandraLustig13 Mannomann, dumme, verwöhnte, gepamperte Kinder im Furor der moralisch-ethischen Überhöhung. Sie wer… https://t.co/l7JcrRgEzt","task1":0,"task2":1,"ID":"hasoc_2020_de_3096","_deepnote_index_column":2406},{"tweet_id":1131454021003354100,"text":"@kernkraftzwerg Ich hätte auch eher was anderes vernascht, als den Kopf. 😅","task1":1,"task2":1,"ID":"hasoc_2020_de_1213","_deepnote_index_column":2407},{"tweet_id":1130849340790992900,"text":"RT @Hallo_Max: Sich in Herzenssachen einfach aus dem Staube machen, ist wie nicht gezahlte Zeche - und zeigt Charakterschwäche.","task1":1,"task2":1,"ID":"hasoc_2020_de_2339","_deepnote_index_column":2408},{"tweet_id":1124122994173988900,"text":"RT @helllud123: Unsere alternativlose Gottkanzlerin Merkel sammelt gerade wieder überall illegale Einwanderer ein, die der deutsche Maloche…","task1":1,"task2":1,"ID":"hasoc_2020_de_604","_deepnote_index_column":2409},{"tweet_id":1127915819386789900,"text":"Gesucht wird Michael und Tobias. Sorry steckt euch eure pc in den Arsch. Diese N@zis mal wieder.... Versuchter Tots… https://t.co/3pDfrheKrb","task1":1,"task2":1,"ID":"hasoc_2020_de_1407","_deepnote_index_column":2410},{"tweet_id":1124730123876479000,"text":"Mir fällt auf wie oft ich einfach shcin Konfermation anstatt Konfirmation geschrieben hab wie dumm bin ich eigentlich","task1":1,"task2":1,"ID":"hasoc_2020_de_2534","_deepnote_index_column":2411},{"tweet_id":1128887257962876900,"text":"@JoergSchindler Das haut der Linke einfach mal so raus.","task1":1,"task2":1,"ID":"hasoc_2020_de_2832","_deepnote_index_column":2412},{"tweet_id":1125059523532791800,"text":"RT @RfD_BRD: Frau #Bundeskanzlerin Dr. Angela #Merkel nehmen Sie bitte den Satz zurueck: \"Der #Islam gehoert zu #Deutschland.“ https://t.co…","task1":1,"task2":1,"ID":"hasoc_2020_de_1629","_deepnote_index_column":2413},{"tweet_id":1129789985392144400,"text":"@FCBayern Wo sind die KOVACOUT Leute ? Heuchler.","task1":0,"task2":1,"ID":"hasoc_2020_de_2621","_deepnote_index_column":2414},{"tweet_id":1128355369879838700,"text":"Nazis haben ihn getötet!","task1":1,"task2":1,"ID":"hasoc_2020_de_3063","_deepnote_index_column":2415},{"tweet_id":1125436117518626800,"text":"@tagesschau Dankeschön❤️🌙😽","task1":1,"task2":1,"ID":"hasoc_2020_de_2238","_deepnote_index_column":2416},{"tweet_id":1128560047708155900,"text":"@Tisoow So gefällst du mir schon viel besser :)","task1":1,"task2":1,"ID":"hasoc_2020_de_3288","_deepnote_index_column":2417},{"tweet_id":1131102202762805200,"text":"RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…","task1":0,"task2":0,"ID":"hasoc_2020_de_177","_deepnote_index_column":2418},{"tweet_id":1130092658972610600,"text":"@ZDFheute Warum wird eigentlich die Fresse von dem 🍯-Pot verpixelt?","task1":1,"task2":1,"ID":"hasoc_2020_de_985","_deepnote_index_column":2419},{"tweet_id":1129718078282117100,"text":"@ZeitW @welt Weil auch jeder ein hoher Politiker ist?! Wie dumm bist du eigentlich?","task1":0,"task2":2,"ID":"hasoc_2020_de_2963","_deepnote_index_column":2420},{"tweet_id":1128607783094509600,"text":"@natascher_ Tönung = kurzhaltende Haarfarbe einfach?","task1":1,"task2":1,"ID":"hasoc_2020_de_3188","_deepnote_index_column":2421},{"tweet_id":1134068150318878700,"text":"@wodojiban @princhillmaru9 😍🥰😘😜Der Schwanz ist in deiner Pussy der Arsch ist weiter hinten und enger Babe😜😘🥰😍","task1":0,"task2":2,"ID":"hasoc_2020_de_1328","_deepnote_index_column":2422},{"tweet_id":1129279303722688500,"text":"RT @fraeulein_tessa: Das sind nur 50 von noch viel mehr Frauen, die gesellschaftliche Debatten mit voranbringen – wir zeichnen sie heute Ab…","task1":1,"task2":1,"ID":"hasoc_2020_de_1711","_deepnote_index_column":2423},{"tweet_id":1124957329328357400,"text":"ich breche meine fingernägel an deinem rücken ab","task1":0,"task2":2,"ID":"hasoc_2020_de_2995","_deepnote_index_column":2424},{"tweet_id":1123625692325871600,"text":"@WaisySZN @Tommy_MUFC11 @JuicyComan @clearmindset @BredAsARed wo bin ich du hurensohn","task1":0,"task2":2,"ID":"hasoc_2020_de_1371","_deepnote_index_column":2425},{"tweet_id":1124930687084187600,"text":"RT @den_tyske: ... gibt es wirklich noch Idioten die denken, dass sich der Islam bzw. Muslime domestizieren lassen, geschweige denn integri…","task1":0,"task2":0,"ID":"hasoc_2020_de_551","_deepnote_index_column":2426},{"tweet_id":1133329038464966700,"text":"RT @DDcyclist: Hört endlich auf mit diesen Scheiss Aufklebern! https://t.co/v8hsZpMWG4","task1":1,"task2":1,"ID":"hasoc_2020_de_1758","_deepnote_index_column":2427},{"tweet_id":1125137055254765600,"text":"RT @abc321xyblabla: Mal eine dumme Frage: Wodurch reduziert sich eigentlich das CO2 wenn ich eine Steuer oder ein Zertifikat erwerbe?\n\nZerp…","task1":1,"task2":1,"ID":"hasoc_2020_de_1941","_deepnote_index_column":2428},{"tweet_id":1132583849039487000,"text":"@MatthiasHoehn @dieLinke Wie schon so oft und auch jetzt wieder:\n#DieGrünen: Der schwarze Wolf im grünen Schafspelz… https://t.co/OVNI2MAFCI","task1":1,"task2":1,"ID":"hasoc_2020_de_1440","_deepnote_index_column":2429},{"tweet_id":1132361475438391300,"text":"deutsche mollige schwanzlutscher pornos asmr deutsch porno deutsche gangbangpornos deutsch gespochen porno https://t.co/6cPb7YMSPY","task1":0,"task2":3,"ID":"hasoc_2020_de_52","_deepnote_index_column":2430},{"tweet_id":1132210790893871100,"text":"@PaddyVomDorf Stimmt. Top! Habe mich auch schon ertappt wie ich auf einem Flug nach Capetown nicht ein Mal auf dem Klo war.","task1":1,"task2":1,"ID":"hasoc_2020_de_2896","_deepnote_index_column":2431},{"tweet_id":1124077259491631100,"text":"Kommentar zu Österreichs Steuersenkung: Merkels Steuerversagen https://t.co/0SDm7u8D2I","task1":1,"task2":1,"ID":"hasoc_2020_de_3417","_deepnote_index_column":2432},{"tweet_id":1133140445796732900,"text":"@ZDFheute Die GroKo flirtet mit der Antifa und subventioniert sie. #Lügenpresse","task1":1,"task2":1,"ID":"hasoc_2020_de_2130","_deepnote_index_column":2433},{"tweet_id":1125779517753131000,"text":"RT @julia_verlinden: #Schneckentempo BuReg bei #Klimaschutz:\nbis 2030 will sie nur 13%Punkte besser werden -&gt; erst 2086 (!) wäre Wärmeverso…","task1":1,"task2":1,"ID":"hasoc_2020_de_3262","_deepnote_index_column":2434},{"tweet_id":1134453300693217300,"text":"RT @annakatrein: #Merkel wie sie hätte sein können: eine Weltpolikerin, die aus ihrer Biographie eine mutige, progressive Agenda ableitet #…","task1":1,"task2":1,"ID":"hasoc_2020_de_1475","_deepnote_index_column":2435},{"tweet_id":1130814821656670200,"text":"@Psemy_FUT Krasser Scheiß 🙈🙈🙄","task1":1,"task2":3,"ID":"hasoc_2020_de_2161","_deepnote_index_column":2436},{"tweet_id":1132873633511166000,"text":"RT @sueszling: Ich hasse Deutsche https://t.co/mM6l8mFOWY","task1":0,"task2":0,"ID":"hasoc_2020_de_2729","_deepnote_index_column":2437},{"tweet_id":1127122433234866200,"text":"Das ärgert mich echt, naja dann wird halt wieder improvisiert 😂","task1":1,"task2":1,"ID":"hasoc_2020_de_2201","_deepnote_index_column":2438},{"tweet_id":1124590256408420400,"text":"Und diese fucking nagelneuen weißen, glänzenden Kacheln. Dagegen sehen die scheiß lackierten gelblich aus. Ich wüns… https://t.co/MIwueRZZ2u","task1":1,"task2":1,"ID":"hasoc_2020_de_333","_deepnote_index_column":2439},{"tweet_id":1124582043944398800,"text":"Islam heißt übersetzt, zurück ins Mittelalter","task1":1,"task2":1,"ID":"hasoc_2020_de_3398","_deepnote_index_column":2440},{"tweet_id":1130853782546346000,"text":"RT @schulle99a: Da ist Strache ein Engel 😇  gegen Merkel , von der Laien u. Schwesig! Aber diese Mafia von Altparteien manipulieren alles u…","task1":1,"task2":1,"ID":"hasoc_2020_de_2642","_deepnote_index_column":2441},{"tweet_id":1132999185828110300,"text":"Die Zerstörung der Grünen. https://t.co/bN5BgqdoRe","task1":1,"task2":1,"ID":"hasoc_2020_de_1023","_deepnote_index_column":2442},{"tweet_id":1125801533675696100,"text":"@BeaurisZ Is ja auch fürn Arsch","task1":1,"task2":3,"ID":"hasoc_2020_de_819","_deepnote_index_column":2443},{"tweet_id":1128254433920454700,"text":"RT @WennVolk: Jeder deutsche Bürger muss sich über die Rede der Kanzlerin schämen.\nMerkel sprach über Gastarbeiter und Flüchtlinge die dass…","task1":0,"task2":1,"ID":"hasoc_2020_de_246","_deepnote_index_column":2444},{"tweet_id":1123604343326937100,"text":"RT @Seb_Baehr: #pl0105 Lärm gegen Nazis in der Kaiserstrasse. Die Kirche läutet ihre Glocken, aus den Fenstern dröhnt Grönemeyer, auf dem B…","task1":1,"task2":1,"ID":"hasoc_2020_de_699","_deepnote_index_column":2445},{"tweet_id":1123878575285973000,"text":"@sachsenkrieger4 Das ist mal ein pack as ist dann natürlich schlecht wenn du so ne Sachen in einem untrade pack ziehst","task1":1,"task2":1,"ID":"hasoc_2020_de_2458","_deepnote_index_column":2446},{"tweet_id":1129340066604699600,"text":"in conclusion, ich bin dumm","task1":1,"task2":1,"ID":"hasoc_2020_de_1840","_deepnote_index_column":2447},{"tweet_id":1133032786397409300,"text":"RT @chrisjuko: Mit viel Geld an die Macht \n#Soros #Grünen\n\nhttps://t.co/aSDPJx8rhG","task1":1,"task2":1,"ID":"hasoc_2020_de_345","_deepnote_index_column":2448},{"tweet_id":1125397127251804200,"text":"@FritzAlter1 @MiriamOzen Ich habe den Eindruck, daß sich eine Menge Leute dazu äußern, die nie selbst ein Abitur ab… https://t.co/O906ni7npY","task1":1,"task2":1,"ID":"hasoc_2020_de_700","_deepnote_index_column":2449},{"tweet_id":1128318292228223000,"text":"RT @LucaBBM: Wieso hab ich immer Stress mit so hurnsöhnen die dann ihre Freunde holen weil sie allein den schanz nicht aus dem arsch bekomm…","task1":0,"task2":3,"ID":"hasoc_2020_de_2167","_deepnote_index_column":2450},{"tweet_id":1124962538632958000,"text":"RT @tschemal: Deutsche Depressionen https://t.co/80XKn94V2z","task1":1,"task2":1,"ID":"hasoc_2020_de_225","_deepnote_index_column":2451}]},"text/plain":"                 tweet_id                                               text  \\\n0     1133388798925189122  Deutsche rothaarige porno reife deutsche fraue...   \n1     1131117000279961600  Lehrstück auch, wie in der linken Jammerfemini...   \n2     1127134592517980161  RT @NDRinfo: Die deutsche Klimaaktivistin Luis...   \n3     1128897106171842560  @ruhrbahn jeden Morgen eine neue „Fahrzeugstör...   \n4     1123576753199484928  @Junge_Freiheit Die Inkas hatten sich schon dä...   \n...                   ...                                                ...   \n2447  1129340066604699649                        in conclusion, ich bin dumm   \n2448  1133032786397409280  RT @chrisjuko: Mit viel Geld an die Macht \\n#S...   \n2449  1125397127251804162  @FritzAlter1 @MiriamOzen Ich habe den Eindruck...   \n2450  1128318292228222982  RT @LucaBBM: Wieso hab ich immer Stress mit so...   \n2451  1124962538632957952  RT @tschemal: Deutsche Depressionen https://t....   \n\n      task1  task2                  ID  \n0         1      1  hasoc_2020_de_2684  \n1         0      1  hasoc_2020_de_2440  \n2         1      1  hasoc_2020_de_1042  \n3         1      1   hasoc_2020_de_774  \n4         1      1   hasoc_2020_de_559  \n...     ...    ...                 ...  \n2447      1      1  hasoc_2020_de_1840  \n2448      1      1   hasoc_2020_de_345  \n2449      1      1   hasoc_2020_de_700  \n2450      0      3  hasoc_2020_de_2167  \n2451      1      1   hasoc_2020_de_225  \n\n[2452 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n      <th>task1</th>\n      <th>task2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1133388798925189122</td>\n      <td>Deutsche rothaarige porno reife deutsche fraue...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_2684</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1131117000279961600</td>\n      <td>Lehrstück auch, wie in der linken Jammerfemini...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>hasoc_2020_de_2440</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1127134592517980161</td>\n      <td>RT @NDRinfo: Die deutsche Klimaaktivistin Luis...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_1042</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1128897106171842560</td>\n      <td>@ruhrbahn jeden Morgen eine neue „Fahrzeugstör...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_774</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1123576753199484928</td>\n      <td>@Junge_Freiheit Die Inkas hatten sich schon dä...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_559</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2447</th>\n      <td>1129340066604699649</td>\n      <td>in conclusion, ich bin dumm</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_1840</td>\n    </tr>\n    <tr>\n      <th>2448</th>\n      <td>1133032786397409280</td>\n      <td>RT @chrisjuko: Mit viel Geld an die Macht \\n#S...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_345</td>\n    </tr>\n    <tr>\n      <th>2449</th>\n      <td>1125397127251804162</td>\n      <td>@FritzAlter1 @MiriamOzen Ich habe den Eindruck...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_700</td>\n    </tr>\n    <tr>\n      <th>2450</th>\n      <td>1128318292228222982</td>\n      <td>RT @LucaBBM: Wieso hab ich immer Stress mit so...</td>\n      <td>0</td>\n      <td>3</td>\n      <td>hasoc_2020_de_2167</td>\n    </tr>\n    <tr>\n      <th>2451</th>\n      <td>1124962538632957952</td>\n      <td>RT @tschemal: Deutsche Depressionen https://t....</td>\n      <td>1</td>\n      <td>1</td>\n      <td>hasoc_2020_de_225</td>\n    </tr>\n  </tbody>\n</table>\n<p>2452 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-316001c7-147b-4324-b306-1fb4554c0c25"},"source":"def count_words(text):\n    return len(text.split())","execution_count":159,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-5896edee-3448-491b-92d3-c8f9e249d786"},"source":"train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['text'], df['task1'], test_size=0.2, stratify=df['task1'])","execution_count":160,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-2400c9ff-2747-4df5-a672-d519928ef0f3"},"source":"df.text.apply(count_words).max()","execution_count":161,"outputs":[{"output_type":"execute_result","execution_count":161,"data":{"text/plain":"31"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-bff158fb-59b1-414d-a57c-72b559198e50"},"source":"MAX_LENGTH = 74\nposts = train_x.values\ncategories = train_y.values","execution_count":162,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00013-7cdb24ac-0c1f-4b44-b0da-70bc6499f2bb"},"source":"input_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in posts:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(categories)","execution_count":163,"outputs":[{"name":"stderr","text":"WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-660813de-c66b-4a5f-b451-d2a2bed12c11"},"source":"print('Original: ', posts[0])\nprint('Token IDs:', input_ids[0])","execution_count":164,"outputs":[{"name":"stdout","text":"Original:  @PillePa47127736 @rezomusik Ich wollte nur zeigen, dass meine Tweets einen Scheiß zur Umweltbelastung beiträgt. Und… https://t.co/NXyPAdCIBE\nToken IDs: tensor([  102, 16249,  4322,   231,  1142,  6437,  3423,  3955,  4047, 16249,\n          248,  5267,  4110,   260,  2091,   435,  3274,   806,   347,  1388,\n        21042,  3488,   366, 11294,   327,  2192, 15544,  4718, 30942,   552,\n          143,   101,  6222,   847,  1061,  1061,   160,   552,   928,  1061,\n          132, 12508,  1142, 10177,  1502, 30937,   103,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-fdf17462-fa47-42a5-a326-d5fe2eb0fa22"},"source":"# maxn = 0\n# for i in input_ids:\n#     if maxn < len(i[0]):\n#         maxn = len(i[0])\n# print(maxn)","execution_count":165,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-4ffeb47f-115b-47de-ae86-60e24bbb9f73"},"source":"dataset = TensorDataset(input_ids, attention_masks, labels)\ntrain_size = int(0.875 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))\n","execution_count":166,"outputs":[{"name":"stdout","text":"1,715 training samples\n  246 validation samples\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-35bc439d-d9d7-475f-a81a-4cbf21235a04"},"source":"# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n","execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-930b9086-8194-4ae0-b1f8-9acea0c32938"},"source":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-german-dbmdz-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 4, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()","execution_count":168,"outputs":[{"name":"stderr","text":"WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-german-dbmdz-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nWARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-dbmdz-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":168,"data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(31102, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-7977d83b-29e8-43f5-bbd6-0852f9d1343f"},"source":"params = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","execution_count":169,"outputs":[{"name":"stdout","text":"The BERT model has 201 different named parameters.\n\n==== Embedding Layer ====\n\nbert.embeddings.word_embeddings.weight                  (31102, 768)\nbert.embeddings.position_embeddings.weight                (512, 768)\nbert.embeddings.token_type_embeddings.weight                (2, 768)\nbert.embeddings.LayerNorm.weight                              (768,)\nbert.embeddings.LayerNorm.bias                                (768,)\n\n==== First Transformer ====\n\nbert.encoder.layer.0.attention.self.query.weight          (768, 768)\nbert.encoder.layer.0.attention.self.query.bias                (768,)\nbert.encoder.layer.0.attention.self.key.weight            (768, 768)\nbert.encoder.layer.0.attention.self.key.bias                  (768,)\nbert.encoder.layer.0.attention.self.value.weight          (768, 768)\nbert.encoder.layer.0.attention.self.value.bias                (768,)\nbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\nbert.encoder.layer.0.attention.output.dense.bias              (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\nbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\nbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\nbert.encoder.layer.0.output.dense.weight                 (768, 3072)\nbert.encoder.layer.0.output.dense.bias                        (768,)\nbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\nbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n\n==== Output Layer ====\n\nbert.pooler.dense.weight                                  (768, 768)\nbert.pooler.dense.bias                                        (768,)\nclassifier.weight                                           (4, 768)\nclassifier.bias                                                 (4,)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00019-c7d00cc1-4be1-4afc-8838-768a2e99e91e"},"source":"optimizer = AdamW(model.parameters(),\n                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n            )","execution_count":170,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-7a071a93-c375-4743-9e61-2a669cad31ea"},"source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 2\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n","execution_count":171,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-ec9bf438-ff81-4099-a8d1-1f009425becd"},"source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","execution_count":172,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-f935f9e6-c356-4bc4-8890-88825a6856fe"},"source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n","execution_count":173,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00023-4dd4b295-39f1-49ae-8020-77abe2230103"},"source":"seed_val = 42\ntorch.cuda.empty_cache()\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        # It returns different numbers of parameters depending on what arguments\n        # arge given and what flags are set. For our useage here, it returns\n        # the loss (because we provided labels) and the \"logits\"--the model\n        # outputs prior to activation.\n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","execution_count":174,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 2 ========\nTraining...\n  Batch    40  of    108.    Elapsed: 0:00:10.\n  Batch    80  of    108.    Elapsed: 0:00:20.\n\n  Average training loss: 0.56\n  Training epcoh took: 0:00:26\n\nRunning Validation...\n  Accuracy: 0.86\n  Validation Loss: 0.38\n  Validation took: 0:00:01\n\n======== Epoch 2 / 2 ========\nTraining...\n  Batch    40  of    108.    Elapsed: 0:00:10.\n  Batch    80  of    108.    Elapsed: 0:00:19.\n\n  Average training loss: 0.37\n  Training epcoh took: 0:00:26\n\nRunning Validation...\n  Accuracy: 0.85\n  Validation Loss: 0.35\n  Validation took: 0:00:01\n\nTraining complete!\nTotal training took 0:00:54 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00024-4b845de9-8564-48cc-8e11-5386c6985626"},"source":"import pandas as pd\n\n# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# Display the table.\ndf_stats","execution_count":175,"outputs":[{"output_type":"execute_result","execution_count":175,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":2,"column_count":5,"columns":[{"name":"Training Loss","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":0.3676942778682267,"max":0.5619506409598721,"histogram":[{"bin_start":0.3676942778682267,"bin_end":0.3871199141773913,"count":1},{"bin_start":0.3871199141773913,"bin_end":0.40654555048655583,"count":0},{"bin_start":0.40654555048655583,"bin_end":0.4259711867957203,"count":0},{"bin_start":0.4259711867957203,"bin_end":0.4453968231048849,"count":0},{"bin_start":0.4453968231048849,"bin_end":0.4648224594140494,"count":0},{"bin_start":0.4648224594140494,"bin_end":0.484248095723214,"count":0},{"bin_start":0.484248095723214,"bin_end":0.5036737320323785,"count":0},{"bin_start":0.5036737320323785,"bin_end":0.523099368341543,"count":0},{"bin_start":0.523099368341543,"bin_end":0.5425250046507076,"count":0},{"bin_start":0.5425250046507076,"bin_end":0.5619506409598721,"count":1}]}},{"name":"Valid. Loss","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":0.3498148387297988,"max":0.3754712790250778,"histogram":[{"bin_start":0.3498148387297988,"bin_end":0.3523804827593267,"count":1},{"bin_start":0.3523804827593267,"bin_end":0.3549461267888546,"count":0},{"bin_start":0.3549461267888546,"bin_end":0.3575117708183825,"count":0},{"bin_start":0.3575117708183825,"bin_end":0.3600774148479104,"count":0},{"bin_start":0.3600774148479104,"bin_end":0.3626430588774383,"count":0},{"bin_start":0.3626430588774383,"bin_end":0.3652087029069662,"count":0},{"bin_start":0.3652087029069662,"bin_end":0.36777434693649413,"count":0},{"bin_start":0.36777434693649413,"bin_end":0.370339990966022,"count":0},{"bin_start":0.370339990966022,"bin_end":0.3729056349955499,"count":0},{"bin_start":0.3729056349955499,"bin_end":0.3754712790250778,"count":1}]}},{"name":"Valid. Accur.","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":0.8528645833333334,"max":0.86328125,"histogram":[{"bin_start":0.8528645833333334,"bin_end":0.8539062500000001,"count":1},{"bin_start":0.8539062500000001,"bin_end":0.8549479166666667,"count":0},{"bin_start":0.8549479166666667,"bin_end":0.8559895833333333,"count":0},{"bin_start":0.8559895833333333,"bin_end":0.85703125,"count":0},{"bin_start":0.85703125,"bin_end":0.8580729166666667,"count":0},{"bin_start":0.8580729166666667,"bin_end":0.8591145833333333,"count":0},{"bin_start":0.8591145833333333,"bin_end":0.86015625,"count":0},{"bin_start":0.86015625,"bin_end":0.8611979166666667,"count":0},{"bin_start":0.8611979166666667,"bin_end":0.8622395833333334,"count":0},{"bin_start":0.8622395833333334,"bin_end":0.86328125,"count":1}]}},{"name":"Training Time","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"0:00:26","count":2}]}},{"name":"Validation Time","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"0:00:01","count":2}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"Training Loss":0.5619506409598721,"Valid. Loss":0.3754712790250778,"Valid. Accur.":0.86328125,"Training Time":"0:00:26","Validation Time":"0:00:01","_deepnote_index_column":1},{"Training Loss":0.3676942778682267,"Valid. Loss":0.3498148387297988,"Valid. Accur.":0.8528645833333334,"Training Time":"0:00:26","Validation Time":"0:00:01","_deepnote_index_column":2}],"rows_bottom":null},"text/plain":"       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\nepoch                                                                         \n1               0.56         0.38           0.86       0:00:26         0:00:01\n2               0.37         0.35           0.85       0:00:26         0:00:01","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.56</td>\n      <td>0.38</td>\n      <td>0.86</td>\n      <td>0:00:26</td>\n      <td>0:00:01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.37</td>\n      <td>0.35</td>\n      <td>0.85</td>\n      <td>0:00:26</td>\n      <td>0:00:01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00025-7900fc16-8cd9-4342-853c-22ead051492b"},"source":"sns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","execution_count":176,"outputs":[{"data":{"text/plain":"<Figure size 864x432 with 1 Axes>","image/png":"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\n"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00026-944e6d3b-b8a7-4272-a2b2-4d95a0a336eb"},"source":"posts = valid_x.values\ncategories = valid_y.values","execution_count":177,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00027-7f6a5e98-208e-4b83-a225-ec22610a8e4a"},"source":"input_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in posts:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(categories)\n\n# Set the batch size.  \nbatch_size = 16\n\n# Create the DataLoader.\nprediction_data = TensorDataset(input_ids, attention_masks, labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","execution_count":178,"outputs":[{"name":"stderr","text":"WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nWARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00028-5b7a2ad2-a7c2-4cee-a943-04bed8dcd314"},"source":"print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)\n\nprint('    DONE.')\n","execution_count":179,"outputs":[{"name":"stdout","text":"Predicting labels for 491 test sentences...\n    DONE.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00029-94e88b41-8557-4532-8232-26bcd3e186ba"},"source":"print(predictions[0],true_labels[0])","execution_count":180,"outputs":[{"name":"stdout","text":"[[ 0.68428683  3.8481889  -2.1858387  -2.5279148 ]\n [-0.02049743  3.8279877  -1.8866813  -2.2648325 ]\n [ 0.8243229   3.568537   -2.3134735  -2.5458517 ]\n [ 1.0863403   3.6213236  -2.5824502  -2.6082053 ]\n [ 0.38756293  3.6930938  -2.2260582  -2.460192  ]\n [ 0.41200435  3.9861095  -2.0716238  -2.5371358 ]\n [ 1.1985929   3.6286051  -2.5712252  -2.686405  ]\n [ 0.00659738  3.3986619  -1.7234045  -1.8784758 ]\n [ 0.91401494  3.7148178  -2.5729551  -2.6785269 ]\n [ 1.8955438   2.863001   -2.6840312  -2.831638  ]\n [ 1.2554092   3.492081   -2.6119995  -2.8383734 ]\n [ 0.5254765   3.437034   -2.2476583  -2.5845327 ]\n [ 2.8742137   0.14882568 -2.1940696  -2.1060605 ]\n [ 1.1510572   3.450995   -2.6009371  -2.6221318 ]\n [ 0.65465593  4.0587125  -2.4744384  -2.5904906 ]\n [ 1.5799985   2.9082248  -2.680606   -2.7477477 ]] [1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00030-b8da3ca0-957f-4cde-8168-b16b980fe581"},"source":"from sklearn.metrics import matthews_corrcoef\n\nmatthews_set = []\npredicts = []\naccurate = 0\ntotal_len = 0\n# Evaluate each test batch using Matthew's correlation coefficient\nprint('Calculating Matthews Corr. Coef. for each batch...')\n\n# For each input batch...\nfor i in range(len(true_labels)):\n    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n    # and one column for \"1\"). Pick the label with the highest value and turn this\n    # in to a list of 0s and 1s.\n    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n    predicts.append(pred_labels_i)\n    # Calculate and store the coef for this batch.  \n    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n\n    matthews_set.append(matthews)\n    for j in range(len(true_labels[i])):\n        if true_labels[i][j] == pred_labels_i[j]:\n            accurate+=1\n        total_len+=1\nprint(\"Accuracy:\",accurate/total_len)","execution_count":181,"outputs":[{"name":"stdout","text":"Calculating Matthews Corr. Coef. for each batch...\nAccuracy: 0.8635437881873728\n/opt/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00031-c62c4660-e5a0-495a-8013-f6c2c434d123"},"source":"ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n\nplt.title('MCC Score per Batch')\nplt.ylabel('MCC Score (-1 to +1)')\nplt.xlabel('Batch #')\n\nplt.show()\n","execution_count":182,"outputs":[{"data":{"text/plain":"<Figure size 864x432 with 1 Axes>","image/png":"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\n"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00032-125872d9-8943-42c7-9271-98714294aa87"},"source":"flat_predictions = np.concatenate(predictions, axis=0)\n\n# For each sample, pick the label (0 or 1) with the higher score.\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = np.concatenate(true_labels, axis=0)\n\n# Calculate the MCC\nmcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n\nprint('Total MCC: %.3f' % mcc)","execution_count":183,"outputs":[{"name":"stdout","text":"Total MCC: 0.605\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00033-b3d8fb5e-eec8-4bcf-9f93-b9f621f0a81e"},"source":"accurate = 0\nfor (i,j) in zip(flat_predictions, flat_true_labels):\n    if i==j:\n        accurate += 1\naccurate/len(flat_predictions)","execution_count":184,"outputs":[{"output_type":"execute_result","execution_count":184,"data":{"text/plain":"0.8635437881873728"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00034-d3f29f2b-13c3-44b0-bad3-f7d0274cdc71"},"source":"from sklearn.metrics import f1_score\nf1_score(flat_true_labels, flat_predictions, average='macro')","execution_count":185,"outputs":[{"output_type":"execute_result","execution_count":185,"data":{"text/plain":"0.7949384447561165"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00035-5868d782-1680-4050-88f1-b3e9aec0aed4"},"source":"# MODEL_PATH = '/home/jovyan/work/Hate-Speech-Detection/model_epoch_'+str(epochs)","execution_count":186,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00036-264dc950-4723-40d7-8386-436312bd464d"},"source":"# torch.save(model, MODEL_PATH)","execution_count":187,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00037-f835665e-3af1-4102-ae39-edc715143132"},"source":"# model = torch.load(MODEL_PATH)","execution_count":188,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00038-1ad95704-c388-42e4-bd9f-79a89fc79492"},"source":"","execution_count":189,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"5595bdb8-5007-4b26-a196-adb914f8a1d3","deepnote_execution_queue":[]}}