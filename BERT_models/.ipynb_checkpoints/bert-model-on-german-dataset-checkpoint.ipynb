{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "cell_id": "00000-5ef145c4-fdb3-49b0-8fef-c25471ea8dea"
   },
   "outputs": [],
   "source": [
    "# Start writing code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "cell_id": "00001-6be88c99-a6a7-41da-a236-db767e8b38bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk) (4.48.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: bert-tensorflow in /opt/venv/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from bert-tensorflow) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/venv/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/venv/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/venv/lib/python3.7/site-packages (from transformers) (4.48.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/venv/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /opt/venv/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: seaborn in /opt/venv/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /opt/venv/lib/python3.7/site-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: sentence-transformers in /opt/venv/lib/python3.7/site-packages (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /opt/venv/lib/python3.7/site-packages (from sentence-transformers) (1.5.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: regex in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (2020.7.14)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /opt/venv/lib/python3.7/site-packages (from nltk->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/venv/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.1.91)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc1 in /opt/venv/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.8.1rc1)\n",
      "Requirement already satisfied, skipping upgrade: future in /opt/venv/lib/python3.7/site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/venv/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2020.6.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install bert-tensorflow\n",
    "!pip install transformers\n",
    "!pip install seaborn\n",
    "!pip install -U sentence-transformers\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "cell_id": "00002-f16f20eb-f58b-4be2-bcd8-142ec0b02ce9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "cell_id": "00003-3532c383-2872-45bd-8034-48c22aab0a67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "cell_id": "00004-db889112-cd7e-4c59-90a7-a796d5a1e4e3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 08:18:53 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   77C    P0    31W /  70W |  11362MiB / 15079MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "cell_id": "00005-ac9afc8d-aa53-4b5f-b8e1-faaff6b127df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/jovyan/work/Hate-Speech-Detection/hasoc_2020_de_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "cell_id": "00006-bbfa31f2-aba1-421e-a4e9-f5401eab7a01",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 2,452\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "column_count": 5,
       "columns": [
        {
         "dtype": "int64",
         "name": "tweet_id",
         "stats": {
          "histogram": [
           {
            "bin_end": 1125594931460606700,
            "bin_start": 1124639967291039700,
            "count": 2
           },
           {
            "bin_end": 1126549895630173800,
            "bin_start": 1125594931460606700,
            "count": 0
           },
           {
            "bin_end": 1127504859799740800,
            "bin_start": 1126549895630173800,
            "count": 0
           },
           {
            "bin_end": 1128459823969307900,
            "bin_start": 1127504859799740800,
            "count": 0
           },
           {
            "bin_end": 1129414788138874900,
            "bin_start": 1128459823969307900,
            "count": 3
           },
           {
            "bin_end": 1130369752308441900,
            "bin_start": 1129414788138874900,
            "count": 0
           },
           {
            "bin_end": 1131324716478009000,
            "bin_start": 1130369752308441900,
            "count": 1
           },
           {
            "bin_end": 1132279680647575900,
            "bin_start": 1131324716478009000,
            "count": 1
           },
           {
            "bin_end": 1133234644817143000,
            "bin_start": 1132279680647575900,
            "count": 1
           },
           {
            "bin_end": 1134189608986710000,
            "bin_start": 1133234644817143000,
            "count": 2
           }
          ],
          "max": 1134189608986710000,
          "min": 1124639967291039700,
          "nan_count": 0,
          "unique_count": 10
         }
        },
        {
         "dtype": "object",
         "name": "text",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt."
           },
           {
            "count": 1,
            "name": "RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…"
           },
           {
            "count": 8,
            "name": "8 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 10
         }
        },
        {
         "dtype": "object",
         "name": "task1",
         "stats": {
          "categories": [
           {
            "count": 8,
            "name": "HOF"
           },
           {
            "count": 2,
            "name": "NOT"
           }
          ],
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "object",
         "name": "task2",
         "stats": {
          "categories": [
           {
            "count": 3,
            "name": "NONE"
           },
           {
            "count": 3,
            "name": "PRFN"
           },
           {
            "count": 4,
            "name": "2 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 4
         }
        },
        {
         "dtype": "object",
         "name": "ID",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "hasoc_2020_de_2475"
           },
           {
            "count": 1,
            "name": "hasoc_2020_de_177"
           },
           {
            "count": 8,
            "name": "8 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 10
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 10,
       "rows_bottom": null,
       "rows_top": [
        {
         "ID": "hasoc_2020_de_2475",
         "_deepnote_index_column": 94,
         "task1": "HOF",
         "task2": "NONE",
         "text": "wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt.",
         "tweet_id": 1134189608986710000
        },
        {
         "ID": "hasoc_2020_de_177",
         "_deepnote_index_column": 2418,
         "task1": "HOF",
         "task2": "HATE",
         "text": "RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…",
         "tweet_id": 1131102202762805200
        },
        {
         "ID": "hasoc_2020_de_1966",
         "_deepnote_index_column": 345,
         "task1": "HOF",
         "task2": "PRFN",
         "text": "Warum zünden Frauen ihren Arsch an? \n\nWeil sie es können.",
         "tweet_id": 1133612703447179300
        },
        {
         "ID": "hasoc_2020_de_1245",
         "_deepnote_index_column": 121,
         "task1": "HOF",
         "task2": "OFFN",
         "text": "@schwartz_ht @internetbazi @KulierJarolina Ist Dein bester Freund auch so 1 Hurensohn wie der Bazi?",
         "tweet_id": 1124639967291039700
        },
        {
         "ID": "hasoc_2020_de_1513",
         "_deepnote_index_column": 2129,
         "task1": "HOF",
         "task2": "OFFN",
         "text": "ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU OPFA!",
         "tweet_id": 1129250614708297700
        },
        {
         "ID": "hasoc_2020_de_3139",
         "_deepnote_index_column": 1510,
         "task1": "HOF",
         "task2": "HATE",
         "text": "\"Ach scheiß FridaysforFuture! Die sollen erstmal lernen gehen! Alles ungebildete Schulschwänzer da. Alle dumm\"\n... *schnauft genervt*",
         "tweet_id": 1124698624632545300
        },
        {
         "ID": "hasoc_2020_de_1491",
         "_deepnote_index_column": 1147,
         "task1": "NOT",
         "task2": "NONE",
         "text": "@champagnenihal er stürzt ab mit diese drogensachen keske yanimda olsa adam ederdim",
         "tweet_id": 1131670132529487900
        },
        {
         "ID": "hasoc_2020_de_275",
         "_deepnote_index_column": 565,
         "task1": "HOF",
         "task2": "PRFN",
         "text": "heimlich ficken deutsche blondine fickt frauen virgin https://t.co/kjS22CZwrr",
         "tweet_id": 1128982363785048000
        },
        {
         "ID": "hasoc_2020_de_2565",
         "_deepnote_index_column": 2128,
         "task1": "NOT",
         "task2": "NONE",
         "text": "Vergleicht Gauland gerade wirklich die Volkskammer der DDR mit dem EU-Parlament? Urgh... #ZDF #Wahl2019",
         "tweet_id": 1132685820991823900
        },
        {
         "ID": "hasoc_2020_de_2906",
         "_deepnote_index_column": 382,
         "task1": "HOF",
         "task2": "PRFN",
         "text": "@CCrunk9 Was erwartet der hurensohn was passiert",
         "tweet_id": 1129299763566981100
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1134189608986710021</td>\n",
       "      <td>wo sitzt dieser schlaumeier, der für alle idio...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_de_2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>1131102202762805248</td>\n",
       "      <td>RT @helllud123: @ZDFheute @sven_giegold @Joerg...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_de_177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1133612703447179264</td>\n",
       "      <td>Warum zünden Frauen ihren Arsch an? \\n\\nWeil s...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_de_1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1124639967291039745</td>\n",
       "      <td>@schwartz_ht @internetbazi @KulierJarolina Ist...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_de_1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>1129250614708297728</td>\n",
       "      <td>ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_de_1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1124698624632545281</td>\n",
       "      <td>\"Ach scheiß FridaysforFuture! Die sollen erstm...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_de_3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1131670132529487872</td>\n",
       "      <td>@champagnenihal er stürzt ab mit diese drogens...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_de_1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1128982363785048066</td>\n",
       "      <td>heimlich ficken deutsche blondine fickt frauen...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_de_275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1132685820991823872</td>\n",
       "      <td>Vergleicht Gauland gerade wirklich die Volkska...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_de_2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1129299763566981120</td>\n",
       "      <td>@CCrunk9 Was erwartet der hurensohn was passiert</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_de_2906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "94    1134189608986710021  wo sitzt dieser schlaumeier, der für alle idio...   \n",
       "2418  1131102202762805248  RT @helllud123: @ZDFheute @sven_giegold @Joerg...   \n",
       "345   1133612703447179264  Warum zünden Frauen ihren Arsch an? \\n\\nWeil s...   \n",
       "121   1124639967291039745  @schwartz_ht @internetbazi @KulierJarolina Ist...   \n",
       "2129  1129250614708297728  ICH FICK DISCH GLEISCH LAN, HALT DIE FRESSE DU...   \n",
       "1510  1124698624632545281  \"Ach scheiß FridaysforFuture! Die sollen erstm...   \n",
       "1147  1131670132529487872  @champagnenihal er stürzt ab mit diese drogens...   \n",
       "565   1128982363785048066  heimlich ficken deutsche blondine fickt frauen...   \n",
       "2128  1132685820991823872  Vergleicht Gauland gerade wirklich die Volkska...   \n",
       "382   1129299763566981120   @CCrunk9 Was erwartet der hurensohn was passiert   \n",
       "\n",
       "     task1 task2                  ID  \n",
       "94     HOF  NONE  hasoc_2020_de_2475  \n",
       "2418   HOF  HATE   hasoc_2020_de_177  \n",
       "345    HOF  PRFN  hasoc_2020_de_1966  \n",
       "121    HOF  OFFN  hasoc_2020_de_1245  \n",
       "2129   HOF  OFFN  hasoc_2020_de_1513  \n",
       "1510   HOF  HATE  hasoc_2020_de_3139  \n",
       "1147   NOT  NONE  hasoc_2020_de_1491  \n",
       "565    HOF  PRFN   hasoc_2020_de_275  \n",
       "2128   NOT  NONE  hasoc_2020_de_2565  \n",
       "382    HOF  PRFN  hasoc_2020_de_2906  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "cell_id": "00007-06c38179-c7e5-4fbe-b82c-e4e68acc16c5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "column_count": 5,
       "columns": [
        {
         "dtype": "int64",
         "name": "tweet_id",
         "stats": {
          "histogram": [
           {
            "bin_end": 1124557957772055300,
            "bin_start": 1123576753199484900,
            "count": 1
           },
           {
            "bin_end": 1125539162344625800,
            "bin_start": 1124557957772055300,
            "count": 0
           },
           {
            "bin_end": 1126520366917196200,
            "bin_start": 1125539162344625800,
            "count": 0
           },
           {
            "bin_end": 1127501571489766700,
            "bin_start": 1126520366917196200,
            "count": 1
           },
           {
            "bin_end": 1128482776062337000,
            "bin_start": 1127501571489766700,
            "count": 0
           },
           {
            "bin_end": 1129463980634907400,
            "bin_start": 1128482776062337000,
            "count": 1
           },
           {
            "bin_end": 1130445185207477900,
            "bin_start": 1129463980634907400,
            "count": 0
           },
           {
            "bin_end": 1131426389780048300,
            "bin_start": 1130445185207477900,
            "count": 1
           },
           {
            "bin_end": 1132407594352618800,
            "bin_start": 1131426389780048300,
            "count": 0
           },
           {
            "bin_end": 1133388798925189100,
            "bin_start": 1132407594352618800,
            "count": 1
           }
          ],
          "max": 1133388798925189100,
          "min": 1123576753199484900,
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "text",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd"
           },
           {
            "count": 1,
            "name": "Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL"
           },
           {
            "count": 3,
            "name": "3 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "int64",
         "name": "task1",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1,
            "bin_start": 0,
            "count": 1
           },
           {
            "bin_end": 0.2,
            "bin_start": 0.1,
            "count": 0
           },
           {
            "bin_end": 0.30000000000000004,
            "bin_start": 0.2,
            "count": 0
           },
           {
            "bin_end": 0.4,
            "bin_start": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_end": 0.5,
            "bin_start": 0.4,
            "count": 0
           },
           {
            "bin_end": 0.6000000000000001,
            "bin_start": 0.5,
            "count": 0
           },
           {
            "bin_end": 0.7000000000000001,
            "bin_start": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_end": 0.8,
            "bin_start": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_end": 0.9,
            "bin_start": 0.8,
            "count": 0
           },
           {
            "bin_end": 1,
            "bin_start": 0.9,
            "count": 4
           }
          ],
          "max": 1,
          "min": 0,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "int64",
         "name": "task2",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.6,
            "bin_start": 0.5,
            "count": 0
           },
           {
            "bin_end": 0.7,
            "bin_start": 0.6,
            "count": 0
           },
           {
            "bin_end": 0.8,
            "bin_start": 0.7,
            "count": 0
           },
           {
            "bin_end": 0.9,
            "bin_start": 0.8,
            "count": 0
           },
           {
            "bin_end": 1,
            "bin_start": 0.9,
            "count": 0
           },
           {
            "bin_end": 1.1,
            "bin_start": 1,
            "count": 5
           },
           {
            "bin_end": 1.2000000000000002,
            "bin_start": 1.1,
            "count": 0
           },
           {
            "bin_end": 1.3,
            "bin_start": 1.2000000000000002,
            "count": 0
           },
           {
            "bin_end": 1.4,
            "bin_start": 1.3,
            "count": 0
           },
           {
            "bin_end": 1.5,
            "bin_start": 1.4,
            "count": 0
           }
          ],
          "max": 1,
          "min": 1,
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "object",
         "name": "ID",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "hasoc_2020_de_2684"
           },
           {
            "count": 1,
            "name": "hasoc_2020_de_2440"
           },
           {
            "count": 3,
            "name": "3 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 5,
       "rows_bottom": null,
       "rows_top": [
        {
         "ID": "hasoc_2020_de_2684",
         "_deepnote_index_column": 0,
         "task1": 1,
         "task2": 1,
         "text": "Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd",
         "tweet_id": 1133388798925189100
        },
        {
         "ID": "hasoc_2020_de_2440",
         "_deepnote_index_column": 1,
         "task1": 0,
         "task2": 1,
         "text": "Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL",
         "tweet_id": 1131117000279961600
        },
        {
         "ID": "hasoc_2020_de_1042",
         "_deepnote_index_column": 2,
         "task1": 1,
         "task2": 1,
         "text": "RT @NDRinfo: Die deutsche Klimaaktivistin Luisa Neubauer wirft Kanzlerin Merkel wegen ihrer fehlenden Unterstützung für den europäischen Kl…",
         "tweet_id": 1127134592517980200
        },
        {
         "ID": "hasoc_2020_de_774",
         "_deepnote_index_column": 3,
         "task1": 1,
         "task2": 1,
         "text": "@ruhrbahn jeden Morgen eine neue „Fahrzeugstörung“, ihr seid einfach nur zum Kotzen",
         "tweet_id": 1128897106171842600
        },
        {
         "ID": "hasoc_2020_de_559",
         "_deepnote_index_column": 4,
         "task1": 1,
         "task2": 1,
         "text": "@Junge_Freiheit Die Inkas hatten sich schon dämlich angestellt, bei den spanischen Eindringlingen, aber der Deutsche toppt dann doch alles",
         "tweet_id": 1123576753199484900
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1133388798925189122</td>\n",
       "      <td>Deutsche rothaarige porno reife deutsche fraue...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_2684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1131117000279961600</td>\n",
       "      <td>Lehrstück auch, wie in der linken Jammerfemini...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1127134592517980161</td>\n",
       "      <td>RT @NDRinfo: Die deutsche Klimaaktivistin Luis...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1128897106171842560</td>\n",
       "      <td>@ruhrbahn jeden Morgen eine neue „Fahrzeugstör...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1123576753199484928</td>\n",
       "      <td>@Junge_Freiheit Die Inkas hatten sich schon dä...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1133388798925189122  Deutsche rothaarige porno reife deutsche fraue...   \n",
       "1  1131117000279961600  Lehrstück auch, wie in der linken Jammerfemini...   \n",
       "2  1127134592517980161  RT @NDRinfo: Die deutsche Klimaaktivistin Luis...   \n",
       "3  1128897106171842560  @ruhrbahn jeden Morgen eine neue „Fahrzeugstör...   \n",
       "4  1123576753199484928  @Junge_Freiheit Die Inkas hatten sich schon dä...   \n",
       "\n",
       "   task1  task2                  ID  \n",
       "0      1      1  hasoc_2020_de_2684  \n",
       "1      0      1  hasoc_2020_de_2440  \n",
       "2      1      1  hasoc_2020_de_1042  \n",
       "3      1      1   hasoc_2020_de_774  \n",
       "4      1      1   hasoc_2020_de_559  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE = LabelEncoder()\n",
    "df['task1'] = LE.fit_transform(df['task1'])\n",
    "df['task2'] = LE.fit_transform(df['task2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "cell_id": "00008-1cd64ffd-062b-483d-ae4c-1c1e60b931d6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "column_count": 5,
       "columns": [
        {
         "dtype": "int64",
         "name": "tweet_id",
         "stats": {
          "histogram": [
           {
            "bin_end": 1124597880823833300,
            "bin_start": 1123475540441169900,
            "count": 234
           },
           {
            "bin_end": 1125720221206496900,
            "bin_start": 1124597880823833300,
            "count": 236
           },
           {
            "bin_end": 1126842561589160300,
            "bin_start": 1125720221206496900,
            "count": 194
           },
           {
            "bin_end": 1127964901971823900,
            "bin_start": 1126842561589160300,
            "count": 222
           },
           {
            "bin_end": 1129087242354487300,
            "bin_start": 1127964901971823900,
            "count": 234
           },
           {
            "bin_end": 1130209582737150700,
            "bin_start": 1129087242354487300,
            "count": 258
           },
           {
            "bin_end": 1131331923119814300,
            "bin_start": 1130209582737150700,
            "count": 191
           },
           {
            "bin_end": 1132454263502477700,
            "bin_start": 1131331923119814300,
            "count": 255
           },
           {
            "bin_end": 1133576603885141200,
            "bin_start": 1132454263502477700,
            "count": 349
           },
           {
            "bin_end": 1134698944267804700,
            "bin_start": 1133576603885141200,
            "count": 279
           }
          ],
          "max": 1134698944267804700,
          "min": 1123475540441169900,
          "nan_count": 0,
          "unique_count": 2452
         }
        },
        {
         "dtype": "object",
         "name": "text",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd"
           },
           {
            "count": 1,
            "name": "Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL"
           },
           {
            "count": 2450,
            "name": "2450 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 2452
         }
        },
        {
         "dtype": "int64",
         "name": "task1",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1,
            "bin_start": 0,
            "count": 601
           },
           {
            "bin_end": 0.2,
            "bin_start": 0.1,
            "count": 0
           },
           {
            "bin_end": 0.30000000000000004,
            "bin_start": 0.2,
            "count": 0
           },
           {
            "bin_end": 0.4,
            "bin_start": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_end": 0.5,
            "bin_start": 0.4,
            "count": 0
           },
           {
            "bin_end": 0.6000000000000001,
            "bin_start": 0.5,
            "count": 0
           },
           {
            "bin_end": 0.7000000000000001,
            "bin_start": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_end": 0.8,
            "bin_start": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_end": 0.9,
            "bin_start": 0.8,
            "count": 0
           },
           {
            "bin_end": 1,
            "bin_start": 0.9,
            "count": 1851
           }
          ],
          "max": 1,
          "min": 0,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "int64",
         "name": "task2",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.3,
            "bin_start": 0,
            "count": 102
           },
           {
            "bin_end": 0.6,
            "bin_start": 0.3,
            "count": 0
           },
           {
            "bin_end": 0.8999999999999999,
            "bin_start": 0.6,
            "count": 0
           },
           {
            "bin_end": 1.2,
            "bin_start": 0.8999999999999999,
            "count": 1860
           },
           {
            "bin_end": 1.5,
            "bin_start": 1.2,
            "count": 0
           },
           {
            "bin_end": 1.7999999999999998,
            "bin_start": 1.5,
            "count": 0
           },
           {
            "bin_end": 2.1,
            "bin_start": 1.7999999999999998,
            "count": 126
           },
           {
            "bin_end": 2.4,
            "bin_start": 2.1,
            "count": 0
           },
           {
            "bin_end": 2.6999999999999997,
            "bin_start": 2.4,
            "count": 0
           },
           {
            "bin_end": 3,
            "bin_start": 2.6999999999999997,
            "count": 364
           }
          ],
          "max": 3,
          "min": 0,
          "nan_count": 0,
          "unique_count": 4
         }
        },
        {
         "dtype": "object",
         "name": "ID",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "hasoc_2020_de_2684"
           },
           {
            "count": 1,
            "name": "hasoc_2020_de_2440"
           },
           {
            "count": 2450,
            "name": "2450 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 2452
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 2452,
       "rows_bottom": [
        {
         "ID": "hasoc_2020_de_361",
         "_deepnote_index_column": 2352,
         "task1": 1,
         "task2": 1,
         "text": "@MRSL0NELYY JEDES MAL EINFACH",
         "tweet_id": 1129829114053947400
        },
        {
         "ID": "hasoc_2020_de_38",
         "_deepnote_index_column": 2353,
         "task1": 1,
         "task2": 1,
         "text": "RT @PrimeVideoDE: Bäm, Ninja Style! Fleabag ist zurück.\n#Fleabag #Fleabag2 @fleabag #PhoebeWallerBridge #PrimeVideo https://t.co/0X2Mk5ikxa",
         "tweet_id": 1129651678226128900
        },
        {
         "ID": "hasoc_2020_de_1510",
         "_deepnote_index_column": 2354,
         "task1": 1,
         "task2": 1,
         "text": "@Magic_Cauldron @Kathari37057708 Früher hat man sich auch mit \"heil hitler\" begrüßt....",
         "tweet_id": 1132704041048387600
        },
        {
         "ID": "hasoc_2020_de_1405",
         "_deepnote_index_column": 2355,
         "task1": 1,
         "task2": 3,
         "text": "RT @32etr02: #Deutsche #Doppelmoral und deutsche #Hetze gegen #Muslime, vor allem gegen das #Kopftuch.\n\nWas fressen diese komischen #heuchl…",
         "tweet_id": 1133666210187546600
        },
        {
         "ID": "hasoc_2020_de_2400",
         "_deepnote_index_column": 2356,
         "task1": 0,
         "task2": 0,
         "text": "RT @FurzkissenN: Allah ist ein hurensohn",
         "tweet_id": 1130916013413937200
        },
        {
         "ID": "hasoc_2020_de_2392",
         "_deepnote_index_column": 2357,
         "task1": 1,
         "task2": 1,
         "text": "@VetovsVictory Löblich.\n\nIch kann mir aber nicht noch mehr Scheiße in den Kopf packen.",
         "tweet_id": 1125466853365645300
        },
        {
         "ID": "hasoc_2020_de_378",
         "_deepnote_index_column": 2358,
         "task1": 1,
         "task2": 1,
         "text": "Kleine, dicke Frauen in weißen Latzhosen haben sich auch nicht diskriminiert gefühlt.\n\nUnd die waren nicht nur sauber, sondern rein!",
         "tweet_id": 1125746495993544700
        },
        {
         "ID": "hasoc_2020_de_2923",
         "_deepnote_index_column": 2359,
         "task1": 1,
         "task2": 1,
         "text": "@lildidgeridoo1 @OlafWedekind Einfach nur noch cringe",
         "tweet_id": 1134400817371516900
        },
        {
         "ID": "hasoc_2020_de_3498",
         "_deepnote_index_column": 2360,
         "task1": 0,
         "task2": 0,
         "text": "RT @SteinbachErika: Hier geht es vor allem um die Pro-Pädophilenpolitik der Grünen. Bis zum heutigen Tage haben die Grünen die Opfer nicht…",
         "tweet_id": 1128252580046487600
        },
        {
         "ID": "hasoc_2020_de_3468",
         "_deepnote_index_column": 2361,
         "task1": 1,
         "task2": 1,
         "text": "RT @rbarris: @ReutersPolitics The Deutsche Abides",
         "tweet_id": 1131310244431310800
        },
        {
         "ID": "hasoc_2020_de_1656",
         "_deepnote_index_column": 2362,
         "task1": 0,
         "task2": 3,
         "text": "lsterne frauen free porn nutten chatarrero gratis dicke frauen masturbieren https://t.co/9gGfVrRYFx",
         "tweet_id": 1132389732468645900
        },
        {
         "ID": "hasoc_2020_de_1367",
         "_deepnote_index_column": 2363,
         "task1": 0,
         "task2": 3,
         "text": "RT @_Zwirn: Leute, die ihre IBAN ohne Leerzeichen angeben, sollen in der Hölle schmoren und mit dem nackten Arsch auf des Teufels Dreizack…",
         "tweet_id": 1133406440138448900
        },
        {
         "ID": "hasoc_2020_de_1395",
         "_deepnote_index_column": 2364,
         "task1": 1,
         "task2": 1,
         "text": "RT @MagnumPhotos: Susan Meiselas wins the 2019 Deutsche Börse prize, Sohrab Hura collaborates with fashion designer Kiko Kostadinov, plus m…",
         "tweet_id": 1129884160112382000
        },
        {
         "ID": "hasoc_2020_de_3390",
         "_deepnote_index_column": 2365,
         "task1": 1,
         "task2": 1,
         "text": "Das ist mir zu dumm",
         "tweet_id": 1123586014205943800
        },
        {
         "ID": "hasoc_2020_de_1841",
         "_deepnote_index_column": 2366,
         "task1": 1,
         "task2": 1,
         "text": "RT @Manuel_k_b: Zeigt mir einen doktortitelklauenden, beim Lügen erwischten, eidbrechenden Politiker in Deutschland, der sich hinstellt und…",
         "tweet_id": 1129737514665893900
        },
        {
         "ID": "hasoc_2020_de_1129",
         "_deepnote_index_column": 2367,
         "task1": 1,
         "task2": 1,
         "text": "RT @schmid_claudio: Der «beliebteste» Politiker Deutschlands vereidigt gerade ein uraltes Postulat der GRÜNEN: KINDER;  liegt derzeit in ga…",
         "tweet_id": 1126118258149593100
        },
        {
         "ID": "hasoc_2020_de_1303",
         "_deepnote_index_column": 2368,
         "task1": 1,
         "task2": 1,
         "text": "RT @haniawiatrek: Die Grünen und Linken sind gegen Tierquälerei unterstützen aber Halal! Moslems dürfen in Deutschland Tiere quälen!",
         "tweet_id": 1133665727809032200
        },
        {
         "ID": "hasoc_2020_de_3311",
         "_deepnote_index_column": 2369,
         "task1": 1,
         "task2": 1,
         "text": "@Hartmantelgesc1 @MiriamOzen @Anemalon19 @tagesschau Die merken schon lange nichts mehr! Die werden rot wenn sie au… https://t.co/Gxa5649VGW",
         "tweet_id": 1124599399982751700
        },
        {
         "ID": "hasoc_2020_de_2202",
         "_deepnote_index_column": 2370,
         "task1": 1,
         "task2": 1,
         "text": "einfach unhatebar https://t.co/ALHeePS8Xs",
         "tweet_id": 1132609212016734200
        },
        {
         "ID": "hasoc_2020_de_2808",
         "_deepnote_index_column": 2371,
         "task1": 1,
         "task2": 1,
         "text": "FPÖ-Funktionärin tritt nach Hetz-Postings zurück: Nachdem erst Ende April der Braunauer FPÖ-Vizebürgermeister Chris… https://t.co/jAG8uyxRmB",
         "tweet_id": 1125413187237634000
        },
        {
         "ID": "hasoc_2020_de_623",
         "_deepnote_index_column": 2372,
         "task1": 1,
         "task2": 1,
         "text": "@tagesschau Dorr Ami holt mal wieder seine ganze Gestörtheit mit Waffen raus, und die Welt, besonders Dschland badets wieder aus!",
         "tweet_id": 1128114411296759800
        },
        {
         "ID": "hasoc_2020_de_1089",
         "_deepnote_index_column": 2373,
         "task1": 0,
         "task2": 1,
         "text": "Freiheizstatue: nuevocanario13 _teutonia YouTube Der Buntepräsident erzählt wieder mal dummen Scheiss. Darauf kann… https://t.co/6TFKu2liRB",
         "tweet_id": 1130021255145545700
        },
        {
         "ID": "hasoc_2020_de_2787",
         "_deepnote_index_column": 2374,
         "task1": 1,
         "task2": 1,
         "text": "@PolizeiSachsen Nazis in der \"Versammlugsbehörde\". Mal aufräumen da.",
         "tweet_id": 1123642977027592200
        },
        {
         "ID": "hasoc_2020_de_3489",
         "_deepnote_index_column": 2375,
         "task1": 0,
         "task2": 3,
         "text": "@alex1990glubb Und du? Was willst du jetzt? Deinen Privilegierten Arsch tätscheln lassen? Zieh Leine!",
         "tweet_id": 1127162094590283800
        },
        {
         "ID": "hasoc_2020_de_1525",
         "_deepnote_index_column": 2376,
         "task1": 1,
         "task2": 1,
         "text": "@asltf Der Andere da haut eine Falschinfo nach der Anderen raus und widerspricht sich selbst. Ob man das schon Dunn… https://t.co/vNoGffC85V",
         "tweet_id": 1131674293249683500
        },
        {
         "ID": "hasoc_2020_de_1719",
         "_deepnote_index_column": 2377,
         "task1": 1,
         "task2": 1,
         "text": "RT @akita_rocky: #Ich denke mal das der Islam da hinter steckt, denkt an den,, Syrer,, der einen Juden in Berlin mit einem Gürtel verprügel…",
         "tweet_id": 1134463786432323600
        },
        {
         "ID": "hasoc_2020_de_3372",
         "_deepnote_index_column": 2378,
         "task1": 1,
         "task2": 2,
         "text": "RT @_donalphonso: \"Der deutsche Gossenkomiker bezeichnet uns Österreicher alle als debil, ich wähle ganz schnell die Grünen, damit er mich…",
         "tweet_id": 1126824016893767700
        },
        {
         "ID": "hasoc_2020_de_35",
         "_deepnote_index_column": 2379,
         "task1": 1,
         "task2": 1,
         "text": "RT @pinkcrazypony: Eine #Islamisierung findet NICHT statt ? Fakten im #ZDF ? \n\n#Islamisierung ist schon weitaus fortgeschrittener als WIR u…",
         "tweet_id": 1133777438914355200
        },
        {
         "ID": "hasoc_2020_de_1363",
         "_deepnote_index_column": 2380,
         "task1": 0,
         "task2": 3,
         "text": "porno deutsche arbeits kollegin mutter und sohn deutsch bester deutscher anal porno privatsex kostenlos https://t.co/bT2WXRClhM",
         "tweet_id": 1131224613529280500
        },
        {
         "ID": "hasoc_2020_de_3038",
         "_deepnote_index_column": 2381,
         "task1": 1,
         "task2": 1,
         "text": "RT @VonErlenbach: @NiemaMovassat @BolzAndrea NATÜRLICH! WIR DEUTSCHE sind doch ohnehin alle Nazis, nicht wahr? \"Ist der Ruf erst ruiniert,…",
         "tweet_id": 1123921973778763800
        },
        {
         "ID": "hasoc_2020_de_899",
         "_deepnote_index_column": 2382,
         "task1": 1,
         "task2": 1,
         "text": "RT @DianeKrass: Gegenwärtig befinden sich die Grünen auf einem regelrechten Höhenflug. \nWenn Sie also der Meinung sind, dass der Islam zu D…",
         "tweet_id": 1130808807028924400
        },
        {
         "ID": "hasoc_2020_de_590",
         "_deepnote_index_column": 2383,
         "task1": 1,
         "task2": 1,
         "text": "RT @DSarkast: Fassen wir mal grob zusammen: Du liebst dein Land?- Nazi.\n\nDu gehst nicht konform mit linken Gedankengut?- Nazi.\n\nDu übst Kri…",
         "tweet_id": 1130060136318459900
        },
        {
         "ID": "hasoc_2020_de_816",
         "_deepnote_index_column": 2384,
         "task1": 1,
         "task2": 1,
         "text": "RT @ichbinkoelnerin: Schööööön\nSeit einigen Tagen schon zeichnet sich der positive Trend für Marine #LePen ab, nun nimmt die 50-j Politiker…",
         "tweet_id": 1125672789485150200
        },
        {
         "ID": "hasoc_2020_de_1555",
         "_deepnote_index_column": 2385,
         "task1": 1,
         "task2": 1,
         "text": "RT @SuseSchmitt: Zum Wahl-Ergebnis folgendes:\n\nJault mir noch einmal einer über Merkel ... CDU ... SPD ... oder Grüne die Ohren voll, gibt…",
         "tweet_id": 1132717248873934800
        },
        {
         "ID": "hasoc_2020_de_1160",
         "_deepnote_index_column": 2386,
         "task1": 1,
         "task2": 1,
         "text": "@bas_ton Das ist jetzt einfach nur gemein.",
         "tweet_id": 1131655276279619600
        },
        {
         "ID": "hasoc_2020_de_3410",
         "_deepnote_index_column": 2387,
         "task1": 1,
         "task2": 1,
         "text": "Grüne klagen gegen Bayerns Grenzpolizei https://t.co/ORCsjS9MD0",
         "tweet_id": 1123993524410757100
        },
        {
         "ID": "hasoc_2020_de_918",
         "_deepnote_index_column": 2388,
         "task1": 1,
         "task2": 1,
         "text": "Schlagabtausch zwischen Schulz und Gauland ► ,,Das ist nicht mein Niveau https://t.co/LApAeP49v5",
         "tweet_id": 1124644954335264800
        },
        {
         "ID": "hasoc_2020_de_1417",
         "_deepnote_index_column": 2389,
         "task1": 1,
         "task2": 1,
         "text": "RT @wolfgs2: Islam: Das vergessene Massaker der Türken an den Aleviten - WELT https://t.co/aLFAnx4BMn",
         "tweet_id": 1125498482612158500
        },
        {
         "ID": "hasoc_2020_de_1198",
         "_deepnote_index_column": 2390,
         "task1": 0,
         "task2": 1,
         "text": "@Quaelgott @jan_wiebe @tonline_news @DSchreckenberg Die linken Medien können nicht anders als überall Nazis zu sehe… https://t.co/g4Qax2yxPR",
         "tweet_id": 1134530475865837600
        },
        {
         "ID": "hasoc_2020_de_1918",
         "_deepnote_index_column": 2391,
         "task1": 1,
         "task2": 1,
         "text": "RT @KayeMenner: Boogie Underground @alhanda &gt; https://t.co/kWDZHID7NS",
         "tweet_id": 1124990648883601400
        },
        {
         "ID": "hasoc_2020_de_1518",
         "_deepnote_index_column": 2392,
         "task1": 1,
         "task2": 1,
         "text": "Hey Kuzeng\n\n       😜\n   👊/||\\_ \n    _/¯    ¯\\_\n\nWollen wir zusammen Türken sein? \n\n👋 \n    \\ 😳\n        || \\_\n    _/¯  ¯\\_",
         "tweet_id": 1132247184844492800
        },
        {
         "ID": "hasoc_2020_de_231",
         "_deepnote_index_column": 2393,
         "task1": 0,
         "task2": 0,
         "text": "Hört bitte auf mir das Video in die Zeitlinie zu spülen in dem Felix von primitiven Moslems gesteinigt wird",
         "tweet_id": 1127494484772651000
        },
        {
         "ID": "hasoc_2020_de_2831",
         "_deepnote_index_column": 2394,
         "task1": 0,
         "task2": 3,
         "text": "Dein Yokuzuna-Sumo ficke ich mit 'nem Pushkick\nWas los, du Hurensohn? Komm wieder, wenn du Luft kriegst",
         "tweet_id": 1130750543922548700
        },
        {
         "ID": "hasoc_2020_de_1733",
         "_deepnote_index_column": 2395,
         "task1": 1,
         "task2": 1,
         "text": "@KialoHQ lieber nicht das Chaos ist doch schon groß genug dann macht diese EU was sie eh schon macht dank Merkel u.… https://t.co/v3GrtaHSMC",
         "tweet_id": 1127551594424348700
        },
        {
         "ID": "hasoc_2020_de_3346",
         "_deepnote_index_column": 2396,
         "task1": 1,
         "task2": 1,
         "text": "@anandahasbi Kangen gabriel",
         "tweet_id": 1132514328467320800
        },
        {
         "ID": "hasoc_2020_de_1842",
         "_deepnote_index_column": 2397,
         "task1": 1,
         "task2": 1,
         "text": "Seo ist einfach oder? #internetmarketing https://t.co/CZVtKoCceo",
         "tweet_id": 1127302482105913300
        },
        {
         "ID": "hasoc_2020_de_240",
         "_deepnote_index_column": 2398,
         "task1": 0,
         "task2": 2,
         "text": "@subversivaktiv @erkanallesffm Geh zurück ins Loch.  DUMMES Individuum.",
         "tweet_id": 1124583923013627900
        },
        {
         "ID": "hasoc_2020_de_2373",
         "_deepnote_index_column": 2399,
         "task1": 1,
         "task2": 0,
         "text": "@cem_oezdemir Grüne sind russophobe Faschisten",
         "tweet_id": 1130038246258483200
        },
        {
         "ID": "hasoc_2020_de_1409",
         "_deepnote_index_column": 2400,
         "task1": 0,
         "task2": 1,
         "text": "RT @alfunterwegs70: Sie belügen euch ganz öffentlich und der dumme Deutsche will es einfach nicht kapieren https://t.co/TeaHVQqZZ2",
         "tweet_id": 1134395448637173800
        },
        {
         "ID": "hasoc_2020_de_525",
         "_deepnote_index_column": 2401,
         "task1": 0,
         "task2": 1,
         "text": "Video: Martin Sellner: Auf einen Cafe mit der Lügenpresse (18:15) https://t.co/DktJPZxfXK",
         "tweet_id": 1128772627626111000
        },
        {
         "ID": "hasoc_2020_de_2046",
         "_deepnote_index_column": 2402,
         "task1": 1,
         "task2": 1,
         "text": "RT @deutsch365: ‼️‼️ Donald Trump Das deutsche Volk wird Angela Merkel stürzen‼️‼️ https://t.co/TY3f7wimF5",
         "tweet_id": 1130840578860748800
        },
        {
         "ID": "hasoc_2020_de_385",
         "_deepnote_index_column": 2403,
         "task1": 1,
         "task2": 1,
         "text": "Können diese “Panik-Attacken“ nicht einfach verschwinden ?!😕",
         "tweet_id": 1131234465974497300
        },
        {
         "ID": "hasoc_2020_de_489",
         "_deepnote_index_column": 2404,
         "task1": 1,
         "task2": 1,
         "text": "@edwin_IBOE @Martin_Sellner Rot grün sind nicht selten farben die clowns verwenden.",
         "tweet_id": 1128212927096459300
        },
        {
         "ID": "hasoc_2020_de_1327",
         "_deepnote_index_column": 2405,
         "task1": 1,
         "task2": 1,
         "text": "RT @suanahilt: Diese #Grünen sind echt anstrengend😁 https://t.co/sHyUqKqzqe",
         "tweet_id": 1130860015265427500
        },
        {
         "ID": "hasoc_2020_de_3096",
         "_deepnote_index_column": 2406,
         "task1": 0,
         "task2": 1,
         "text": "@SandraLustig13 Mannomann, dumme, verwöhnte, gepamperte Kinder im Furor der moralisch-ethischen Überhöhung. Sie wer… https://t.co/l7JcrRgEzt",
         "tweet_id": 1123614787143962600
        },
        {
         "ID": "hasoc_2020_de_1213",
         "_deepnote_index_column": 2407,
         "task1": 1,
         "task2": 1,
         "text": "@kernkraftzwerg Ich hätte auch eher was anderes vernascht, als den Kopf. 😅",
         "tweet_id": 1131454021003354100
        },
        {
         "ID": "hasoc_2020_de_2339",
         "_deepnote_index_column": 2408,
         "task1": 1,
         "task2": 1,
         "text": "RT @Hallo_Max: Sich in Herzenssachen einfach aus dem Staube machen, ist wie nicht gezahlte Zeche - und zeigt Charakterschwäche.",
         "tweet_id": 1130849340790992900
        },
        {
         "ID": "hasoc_2020_de_604",
         "_deepnote_index_column": 2409,
         "task1": 1,
         "task2": 1,
         "text": "RT @helllud123: Unsere alternativlose Gottkanzlerin Merkel sammelt gerade wieder überall illegale Einwanderer ein, die der deutsche Maloche…",
         "tweet_id": 1124122994173988900
        },
        {
         "ID": "hasoc_2020_de_1407",
         "_deepnote_index_column": 2410,
         "task1": 1,
         "task2": 1,
         "text": "Gesucht wird Michael und Tobias. Sorry steckt euch eure pc in den Arsch. Diese N@zis mal wieder.... Versuchter Tots… https://t.co/3pDfrheKrb",
         "tweet_id": 1127915819386789900
        },
        {
         "ID": "hasoc_2020_de_2534",
         "_deepnote_index_column": 2411,
         "task1": 1,
         "task2": 1,
         "text": "Mir fällt auf wie oft ich einfach shcin Konfermation anstatt Konfirmation geschrieben hab wie dumm bin ich eigentlich",
         "tweet_id": 1124730123876479000
        },
        {
         "ID": "hasoc_2020_de_2832",
         "_deepnote_index_column": 2412,
         "task1": 1,
         "task2": 1,
         "text": "@JoergSchindler Das haut der Linke einfach mal so raus.",
         "tweet_id": 1128887257962876900
        },
        {
         "ID": "hasoc_2020_de_1629",
         "_deepnote_index_column": 2413,
         "task1": 1,
         "task2": 1,
         "text": "RT @RfD_BRD: Frau #Bundeskanzlerin Dr. Angela #Merkel nehmen Sie bitte den Satz zurueck: \"Der #Islam gehoert zu #Deutschland.“ https://t.co…",
         "tweet_id": 1125059523532791800
        },
        {
         "ID": "hasoc_2020_de_2621",
         "_deepnote_index_column": 2414,
         "task1": 0,
         "task2": 1,
         "text": "@FCBayern Wo sind die KOVACOUT Leute ? Heuchler.",
         "tweet_id": 1129789985392144400
        },
        {
         "ID": "hasoc_2020_de_3063",
         "_deepnote_index_column": 2415,
         "task1": 1,
         "task2": 1,
         "text": "Nazis haben ihn getötet!",
         "tweet_id": 1128355369879838700
        },
        {
         "ID": "hasoc_2020_de_2238",
         "_deepnote_index_column": 2416,
         "task1": 1,
         "task2": 1,
         "text": "@tagesschau Dankeschön❤️🌙😽",
         "tweet_id": 1125436117518626800
        },
        {
         "ID": "hasoc_2020_de_3288",
         "_deepnote_index_column": 2417,
         "task1": 1,
         "task2": 1,
         "text": "@Tisoow So gefällst du mir schon viel besser :)",
         "tweet_id": 1128560047708155900
        },
        {
         "ID": "hasoc_2020_de_177",
         "_deepnote_index_column": 2418,
         "task1": 0,
         "task2": 0,
         "text": "RT @helllud123: @ZDFheute @sven_giegold @Joerg_Meuthen Mal sehen, ob die dumm-naiven Wessis noch immer den Grünen und ihrer Klimalüge nachr…",
         "tweet_id": 1131102202762805200
        },
        {
         "ID": "hasoc_2020_de_985",
         "_deepnote_index_column": 2419,
         "task1": 1,
         "task2": 1,
         "text": "@ZDFheute Warum wird eigentlich die Fresse von dem 🍯-Pot verpixelt?",
         "tweet_id": 1130092658972610600
        },
        {
         "ID": "hasoc_2020_de_2963",
         "_deepnote_index_column": 2420,
         "task1": 0,
         "task2": 2,
         "text": "@ZeitW @welt Weil auch jeder ein hoher Politiker ist?! Wie dumm bist du eigentlich?",
         "tweet_id": 1129718078282117100
        },
        {
         "ID": "hasoc_2020_de_3188",
         "_deepnote_index_column": 2421,
         "task1": 1,
         "task2": 1,
         "text": "@natascher_ Tönung = kurzhaltende Haarfarbe einfach?",
         "tweet_id": 1128607783094509600
        },
        {
         "ID": "hasoc_2020_de_1328",
         "_deepnote_index_column": 2422,
         "task1": 0,
         "task2": 2,
         "text": "@wodojiban @princhillmaru9 😍🥰😘😜Der Schwanz ist in deiner Pussy der Arsch ist weiter hinten und enger Babe😜😘🥰😍",
         "tweet_id": 1134068150318878700
        },
        {
         "ID": "hasoc_2020_de_1711",
         "_deepnote_index_column": 2423,
         "task1": 1,
         "task2": 1,
         "text": "RT @fraeulein_tessa: Das sind nur 50 von noch viel mehr Frauen, die gesellschaftliche Debatten mit voranbringen – wir zeichnen sie heute Ab…",
         "tweet_id": 1129279303722688500
        },
        {
         "ID": "hasoc_2020_de_2995",
         "_deepnote_index_column": 2424,
         "task1": 0,
         "task2": 2,
         "text": "ich breche meine fingernägel an deinem rücken ab",
         "tweet_id": 1124957329328357400
        },
        {
         "ID": "hasoc_2020_de_1371",
         "_deepnote_index_column": 2425,
         "task1": 0,
         "task2": 2,
         "text": "@WaisySZN @Tommy_MUFC11 @JuicyComan @clearmindset @BredAsARed wo bin ich du hurensohn",
         "tweet_id": 1123625692325871600
        },
        {
         "ID": "hasoc_2020_de_551",
         "_deepnote_index_column": 2426,
         "task1": 0,
         "task2": 0,
         "text": "RT @den_tyske: ... gibt es wirklich noch Idioten die denken, dass sich der Islam bzw. Muslime domestizieren lassen, geschweige denn integri…",
         "tweet_id": 1124930687084187600
        },
        {
         "ID": "hasoc_2020_de_1758",
         "_deepnote_index_column": 2427,
         "task1": 1,
         "task2": 1,
         "text": "RT @DDcyclist: Hört endlich auf mit diesen Scheiss Aufklebern! https://t.co/v8hsZpMWG4",
         "tweet_id": 1133329038464966700
        },
        {
         "ID": "hasoc_2020_de_1941",
         "_deepnote_index_column": 2428,
         "task1": 1,
         "task2": 1,
         "text": "RT @abc321xyblabla: Mal eine dumme Frage: Wodurch reduziert sich eigentlich das CO2 wenn ich eine Steuer oder ein Zertifikat erwerbe?\n\nZerp…",
         "tweet_id": 1125137055254765600
        },
        {
         "ID": "hasoc_2020_de_1440",
         "_deepnote_index_column": 2429,
         "task1": 1,
         "task2": 1,
         "text": "@MatthiasHoehn @dieLinke Wie schon so oft und auch jetzt wieder:\n#DieGrünen: Der schwarze Wolf im grünen Schafspelz… https://t.co/OVNI2MAFCI",
         "tweet_id": 1132583849039487000
        },
        {
         "ID": "hasoc_2020_de_52",
         "_deepnote_index_column": 2430,
         "task1": 0,
         "task2": 3,
         "text": "deutsche mollige schwanzlutscher pornos asmr deutsch porno deutsche gangbangpornos deutsch gespochen porno https://t.co/6cPb7YMSPY",
         "tweet_id": 1132361475438391300
        },
        {
         "ID": "hasoc_2020_de_2896",
         "_deepnote_index_column": 2431,
         "task1": 1,
         "task2": 1,
         "text": "@PaddyVomDorf Stimmt. Top! Habe mich auch schon ertappt wie ich auf einem Flug nach Capetown nicht ein Mal auf dem Klo war.",
         "tweet_id": 1132210790893871100
        },
        {
         "ID": "hasoc_2020_de_3417",
         "_deepnote_index_column": 2432,
         "task1": 1,
         "task2": 1,
         "text": "Kommentar zu Österreichs Steuersenkung: Merkels Steuerversagen https://t.co/0SDm7u8D2I",
         "tweet_id": 1124077259491631100
        },
        {
         "ID": "hasoc_2020_de_2130",
         "_deepnote_index_column": 2433,
         "task1": 1,
         "task2": 1,
         "text": "@ZDFheute Die GroKo flirtet mit der Antifa und subventioniert sie. #Lügenpresse",
         "tweet_id": 1133140445796732900
        },
        {
         "ID": "hasoc_2020_de_3262",
         "_deepnote_index_column": 2434,
         "task1": 1,
         "task2": 1,
         "text": "RT @julia_verlinden: #Schneckentempo BuReg bei #Klimaschutz:\nbis 2030 will sie nur 13%Punkte besser werden -&gt; erst 2086 (!) wäre Wärmeverso…",
         "tweet_id": 1125779517753131000
        },
        {
         "ID": "hasoc_2020_de_1475",
         "_deepnote_index_column": 2435,
         "task1": 1,
         "task2": 1,
         "text": "RT @annakatrein: #Merkel wie sie hätte sein können: eine Weltpolikerin, die aus ihrer Biographie eine mutige, progressive Agenda ableitet #…",
         "tweet_id": 1134453300693217300
        },
        {
         "ID": "hasoc_2020_de_2161",
         "_deepnote_index_column": 2436,
         "task1": 1,
         "task2": 3,
         "text": "@Psemy_FUT Krasser Scheiß 🙈🙈🙄",
         "tweet_id": 1130814821656670200
        },
        {
         "ID": "hasoc_2020_de_2729",
         "_deepnote_index_column": 2437,
         "task1": 0,
         "task2": 0,
         "text": "RT @sueszling: Ich hasse Deutsche https://t.co/mM6l8mFOWY",
         "tweet_id": 1132873633511166000
        },
        {
         "ID": "hasoc_2020_de_2201",
         "_deepnote_index_column": 2438,
         "task1": 1,
         "task2": 1,
         "text": "Das ärgert mich echt, naja dann wird halt wieder improvisiert 😂",
         "tweet_id": 1127122433234866200
        },
        {
         "ID": "hasoc_2020_de_333",
         "_deepnote_index_column": 2439,
         "task1": 1,
         "task2": 1,
         "text": "Und diese fucking nagelneuen weißen, glänzenden Kacheln. Dagegen sehen die scheiß lackierten gelblich aus. Ich wüns… https://t.co/MIwueRZZ2u",
         "tweet_id": 1124590256408420400
        },
        {
         "ID": "hasoc_2020_de_3398",
         "_deepnote_index_column": 2440,
         "task1": 1,
         "task2": 1,
         "text": "Islam heißt übersetzt, zurück ins Mittelalter",
         "tweet_id": 1124582043944398800
        },
        {
         "ID": "hasoc_2020_de_2642",
         "_deepnote_index_column": 2441,
         "task1": 1,
         "task2": 1,
         "text": "RT @schulle99a: Da ist Strache ein Engel 😇  gegen Merkel , von der Laien u. Schwesig! Aber diese Mafia von Altparteien manipulieren alles u…",
         "tweet_id": 1130853782546346000
        },
        {
         "ID": "hasoc_2020_de_1023",
         "_deepnote_index_column": 2442,
         "task1": 1,
         "task2": 1,
         "text": "Die Zerstörung der Grünen. https://t.co/bN5BgqdoRe",
         "tweet_id": 1132999185828110300
        },
        {
         "ID": "hasoc_2020_de_819",
         "_deepnote_index_column": 2443,
         "task1": 1,
         "task2": 3,
         "text": "@BeaurisZ Is ja auch fürn Arsch",
         "tweet_id": 1125801533675696100
        },
        {
         "ID": "hasoc_2020_de_246",
         "_deepnote_index_column": 2444,
         "task1": 0,
         "task2": 1,
         "text": "RT @WennVolk: Jeder deutsche Bürger muss sich über die Rede der Kanzlerin schämen.\nMerkel sprach über Gastarbeiter und Flüchtlinge die dass…",
         "tweet_id": 1128254433920454700
        },
        {
         "ID": "hasoc_2020_de_699",
         "_deepnote_index_column": 2445,
         "task1": 1,
         "task2": 1,
         "text": "RT @Seb_Baehr: #pl0105 Lärm gegen Nazis in der Kaiserstrasse. Die Kirche läutet ihre Glocken, aus den Fenstern dröhnt Grönemeyer, auf dem B…",
         "tweet_id": 1123604343326937100
        },
        {
         "ID": "hasoc_2020_de_2458",
         "_deepnote_index_column": 2446,
         "task1": 1,
         "task2": 1,
         "text": "@sachsenkrieger4 Das ist mal ein pack as ist dann natürlich schlecht wenn du so ne Sachen in einem untrade pack ziehst",
         "tweet_id": 1123878575285973000
        },
        {
         "ID": "hasoc_2020_de_1840",
         "_deepnote_index_column": 2447,
         "task1": 1,
         "task2": 1,
         "text": "in conclusion, ich bin dumm",
         "tweet_id": 1129340066604699600
        },
        {
         "ID": "hasoc_2020_de_345",
         "_deepnote_index_column": 2448,
         "task1": 1,
         "task2": 1,
         "text": "RT @chrisjuko: Mit viel Geld an die Macht \n#Soros #Grünen\n\nhttps://t.co/aSDPJx8rhG",
         "tweet_id": 1133032786397409300
        },
        {
         "ID": "hasoc_2020_de_700",
         "_deepnote_index_column": 2449,
         "task1": 1,
         "task2": 1,
         "text": "@FritzAlter1 @MiriamOzen Ich habe den Eindruck, daß sich eine Menge Leute dazu äußern, die nie selbst ein Abitur ab… https://t.co/O906ni7npY",
         "tweet_id": 1125397127251804200
        },
        {
         "ID": "hasoc_2020_de_2167",
         "_deepnote_index_column": 2450,
         "task1": 0,
         "task2": 3,
         "text": "RT @LucaBBM: Wieso hab ich immer Stress mit so hurnsöhnen die dann ihre Freunde holen weil sie allein den schanz nicht aus dem arsch bekomm…",
         "tweet_id": 1128318292228223000
        },
        {
         "ID": "hasoc_2020_de_225",
         "_deepnote_index_column": 2451,
         "task1": 1,
         "task2": 1,
         "text": "RT @tschemal: Deutsche Depressionen https://t.co/80XKn94V2z",
         "tweet_id": 1124962538632958000
        }
       ],
       "rows_top": [
        {
         "ID": "hasoc_2020_de_2684",
         "_deepnote_index_column": 0,
         "task1": 1,
         "task2": 1,
         "text": "Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd",
         "tweet_id": 1133388798925189100
        },
        {
         "ID": "hasoc_2020_de_2440",
         "_deepnote_index_column": 1,
         "task1": 0,
         "task2": 1,
         "text": "Lehrstück auch, wie in der linken Jammerfeministinnenbubble mit keinem Wort erwähnt wird und werden darf, was das d… https://t.co/B6PQKzl2tL",
         "tweet_id": 1131117000279961600
        },
        {
         "ID": "hasoc_2020_de_1042",
         "_deepnote_index_column": 2,
         "task1": 1,
         "task2": 1,
         "text": "RT @NDRinfo: Die deutsche Klimaaktivistin Luisa Neubauer wirft Kanzlerin Merkel wegen ihrer fehlenden Unterstützung für den europäischen Kl…",
         "tweet_id": 1127134592517980200
        },
        {
         "ID": "hasoc_2020_de_774",
         "_deepnote_index_column": 3,
         "task1": 1,
         "task2": 1,
         "text": "@ruhrbahn jeden Morgen eine neue „Fahrzeugstörung“, ihr seid einfach nur zum Kotzen",
         "tweet_id": 1128897106171842600
        },
        {
         "ID": "hasoc_2020_de_559",
         "_deepnote_index_column": 4,
         "task1": 1,
         "task2": 1,
         "text": "@Junge_Freiheit Die Inkas hatten sich schon dämlich angestellt, bei den spanischen Eindringlingen, aber der Deutsche toppt dann doch alles",
         "tweet_id": 1123576753199484900
        },
        {
         "ID": "hasoc_2020_de_1969",
         "_deepnote_index_column": 5,
         "task1": 0,
         "task2": 1,
         "text": "RT @technosteron: leute die 'boar' schreiben lassen sich bestimmt auch von ihren analphabetischen Vater in arsch ficken",
         "tweet_id": 1128743783393312800
        },
        {
         "ID": "hasoc_2020_de_2926",
         "_deepnote_index_column": 6,
         "task1": 1,
         "task2": 3,
         "text": "ich habe mir gerade diese dorne aus meinem arsch gezogen https://t.co/x40mpKgBZN",
         "tweet_id": 1134139256375525400
        },
        {
         "ID": "hasoc_2020_de_2994",
         "_deepnote_index_column": 7,
         "task1": 1,
         "task2": 1,
         "text": "RT @Schrammi44: Er ist einfach der GOAT @Bouncaay https://t.co/bb9b5SWaKi",
         "tweet_id": 1132759271618293800
        },
        {
         "ID": "hasoc_2020_de_1574",
         "_deepnote_index_column": 8,
         "task1": 1,
         "task2": 1,
         "text": "KFM Deutsche Mittelstand AG: Deutscher Mittelstandsanleihen FONDS zeichnet neue Hörmann-Anleihe mit Mindest-Kupon 4… https://t.co/5HeRedf9cx",
         "tweet_id": 1131107693123526700
        },
        {
         "ID": "hasoc_2020_de_1480",
         "_deepnote_index_column": 9,
         "task1": 1,
         "task2": 3,
         "text": "@mentalshards_ ich will jetzt nicht offenden oder so, ich will einfach nur, das die scheiße weniger wird",
         "tweet_id": 1133093570229997600
        },
        {
         "ID": "hasoc_2020_de_3224",
         "_deepnote_index_column": 10,
         "task1": 1,
         "task2": 1,
         "text": "RT @SteinbachErika: Einfach skandalös! https://t.co/dC5NXFbviQ",
         "tweet_id": 1126898700683501600
        },
        {
         "ID": "hasoc_2020_de_410",
         "_deepnote_index_column": 11,
         "task1": 1,
         "task2": 3,
         "text": "\"Das geht nicht.\"\n\"Doch. Zugreifen!\"\n\nAch halt doch die Fresse. Scheiß Werbung.",
         "tweet_id": 1128386596452085800
        },
        {
         "ID": "hasoc_2020_de_3441",
         "_deepnote_index_column": 12,
         "task1": 1,
         "task2": 1,
         "text": "@holzraspler @Apple @Botometer Ja Touchbar da kotzen die meisten, unnötig wie sau\nXcode: seit 2003 aber egal… https://t.co/BQnklEeDMw",
         "tweet_id": 1124322999551840300
        },
        {
         "ID": "hasoc_2020_de_1685",
         "_deepnote_index_column": 13,
         "task1": 1,
         "task2": 1,
         "text": "XXL weights pack v1.1.1.0 FS19 https://t.co/zTpqTSGujE",
         "tweet_id": 1123489897548001300
        },
        {
         "ID": "hasoc_2020_de_2215",
         "_deepnote_index_column": 14,
         "task1": 1,
         "task2": 1,
         "text": "@Kizer_Media @Zero_Aquila @tagesschau (1) https://t.co/oiwSZhwsPt\n(2) https://t.co/l12aHaxDIy\n(3) Agora ist etwas z… https://t.co/kHsKBubRsk",
         "tweet_id": 1128294539872165900
        },
        {
         "ID": "hasoc_2020_de_3404",
         "_deepnote_index_column": 15,
         "task1": 0,
         "task2": 3,
         "text": "deutsche lesben free deutsches ehepaar fickt porno deutscher porno ffentlichkeit free porno muschilecken deutsch https://t.co/GRHmv0F1J9",
         "tweet_id": 1130919100451106800
        },
        {
         "ID": "hasoc_2020_de_1996",
         "_deepnote_index_column": 16,
         "task1": 0,
         "task2": 2,
         "text": "RT @johannesgrunert: Armselig, peinlich und dumm 👎\n#ESC2019 #Iceland https://t.co/sWesovTO9q",
         "tweet_id": 1129884743137398800
        },
        {
         "ID": "hasoc_2020_de_3198",
         "_deepnote_index_column": 17,
         "task1": 1,
         "task2": 1,
         "text": "@FlorianFOLI 1. Waren es 2\n2. Musst du nicht, aber hat halt eh keinen Effekt\n3. Ist der Sieg immernoch eher lucky als verdient",
         "tweet_id": 1127236438624473100
        },
        {
         "ID": "hasoc_2020_de_1803",
         "_deepnote_index_column": 18,
         "task1": 1,
         "task2": 1,
         "text": "@joahlen Da ist nichts mehr zu retten. Hirn schon gar nicht. Wo nichts ist, kann nichts werden.",
         "tweet_id": 1132021174773461000
        },
        {
         "ID": "hasoc_2020_de_2391",
         "_deepnote_index_column": 19,
         "task1": 1,
         "task2": 1,
         "text": "süggest movies plz ty :&gt;",
         "tweet_id": 1124358286218649600
        },
        {
         "ID": "hasoc_2020_de_1228",
         "_deepnote_index_column": 20,
         "task1": 0,
         "task2": 3,
         "text": "@EinJanik @MHP2002 Ja ok du arsch",
         "tweet_id": 1132029215279394800
        },
        {
         "ID": "hasoc_2020_de_2683",
         "_deepnote_index_column": 21,
         "task1": 0,
         "task2": 1,
         "text": "@KONRAD1000 @LVZ Wenn man Ihnen eine Körperverletzung antut, wird man hoffentlich genauso handeln und Täter-Opfer R… https://t.co/cq2bdfZ3ly",
         "tweet_id": 1131180745324728300
        },
        {
         "ID": "hasoc_2020_de_1618",
         "_deepnote_index_column": 22,
         "task1": 1,
         "task2": 1,
         "text": "RT @maxotte_says: \"#Merkel muss weg!\" darf bei #Kundgebug in #Wuppertal nicht gerufen werden. Willkommen in der dunklen Zeit! #Repression #…",
         "tweet_id": 1128638657353650200
        },
        {
         "ID": "hasoc_2020_de_2427",
         "_deepnote_index_column": 23,
         "task1": 1,
         "task2": 1,
         "text": "@heuteshow Und linke Journalisten wollen die humanitäre Aufnahme von Flüchtlingen, halt nur nicht in ihrer Wohngege… https://t.co/qr9WNCNZtP",
         "tweet_id": 1125703697332351000
        },
        {
         "ID": "hasoc_2020_de_1804",
         "_deepnote_index_column": 24,
         "task1": 1,
         "task2": 1,
         "text": "@zeitonline was haben die beiden Kopalken nur im Kopf?",
         "tweet_id": 1131536384542629900
        },
        {
         "ID": "hasoc_2020_de_1970",
         "_deepnote_index_column": 25,
         "task1": 1,
         "task2": 1,
         "text": "@tagesschau Die Hasselfeld, Wahlkampfhelferin der Grünen. https://t.co/FLQKHv0jFb",
         "tweet_id": 1132713729852870700
        },
        {
         "ID": "hasoc_2020_de_776",
         "_deepnote_index_column": 26,
         "task1": 1,
         "task2": 1,
         "text": "Gescher -  Christdemokrat möchte, dass Gänse aus dem Naturschutzgebiet Berkeltal verschwinden. Dreck.… https://t.co/b8060gV3hK",
         "tweet_id": 1131482508711874600
        },
        {
         "ID": "hasoc_2020_de_2755",
         "_deepnote_index_column": 27,
         "task1": 0,
         "task2": 1,
         "text": "Desnudas xxx gratis, deutsche schwgerin porno! Italiani porno gratis, canal chat gratis. https://t.co/wJWFnVuhVe",
         "tweet_id": 1133037848913961000
        },
        {
         "ID": "hasoc_2020_de_3043",
         "_deepnote_index_column": 28,
         "task1": 0,
         "task2": 0,
         "text": "@SatireUNfreiesD So ist es, der Islam ist keine Religion des Friedens! https://t.co/NSyjYNJ477",
         "tweet_id": 1127483176929116200
        },
        {
         "ID": "hasoc_2020_de_2820",
         "_deepnote_index_column": 29,
         "task1": 1,
         "task2": 1,
         "text": "@hubertus_zulu @nilzee011 @LinoTobi @dieserRoman Wie dumm kann man eigentlich sein?",
         "tweet_id": 1127626345285136400
        },
        {
         "ID": "hasoc_2020_de_759",
         "_deepnote_index_column": 30,
         "task1": 0,
         "task2": 3,
         "text": "porno estar gratis unterwrfige deutsche frauen porno mutter fisten sexy mdchen in hotpants https://t.co/FvAKmqCJCu",
         "tweet_id": 1130853572814393300
        },
        {
         "ID": "hasoc_2020_de_2431",
         "_deepnote_index_column": 31,
         "task1": 1,
         "task2": 1,
         "text": "@LianePoost @welt Ich beziehe mich nicht nur auf die DDR. Sozialistische System waren schon immer unproduktiv und w… https://t.co/lrQeZvtXF6",
         "tweet_id": 1123649289463451600
        },
        {
         "ID": "hasoc_2020_de_3466",
         "_deepnote_index_column": 32,
         "task1": 1,
         "task2": 1,
         "text": "@brunipamder237 @Sonnenbebrillt3 @Altonese86 Wir halt einfach immer",
         "tweet_id": 1133836125641105400
        },
        {
         "ID": "hasoc_2020_de_1268",
         "_deepnote_index_column": 33,
         "task1": 1,
         "task2": 1,
         "text": "Ohne scheiss, das is doch einfach unfair",
         "tweet_id": 1125537229575807000
        },
        {
         "ID": "hasoc_2020_de_520",
         "_deepnote_index_column": 34,
         "task1": 1,
         "task2": 1,
         "text": "RT @yungatakan: niemand:\n\nvertrau mir niemand\n\nnichtmal merkel:\n\ndeutsche kinder von afd wählern: https://t.co/EHO9ip8u6z",
         "tweet_id": 1131886101415759900
        },
        {
         "ID": "hasoc_2020_de_2861",
         "_deepnote_index_column": 35,
         "task1": 1,
         "task2": 1,
         "text": "RT @Deacon__Blues: Da @TwitterDE anscheinend ein Haufen Idioten sind, die momentan auf Zuruf von afd-Bots Leute wie @RAStadler oder @Sawsan…",
         "tweet_id": 1125064237922046000
        },
        {
         "ID": "hasoc_2020_de_3475",
         "_deepnote_index_column": 36,
         "task1": 1,
         "task2": 1,
         "text": "Ist es nicht Extrem Dumm Zeit für Geld zu verkaufen ? Arbeitskraft/Stundenlohn/ Zeit. Ist es nicht erbärmlich Wisse… https://t.co/rymiIWWH7O",
         "tweet_id": 1123704385861824500
        },
        {
         "ID": "hasoc_2020_de_1920",
         "_deepnote_index_column": 37,
         "task1": 1,
         "task2": 1,
         "text": "#NWBRB58\n\n+++ Ausfall +++ Osnabrück Hbf (ab 13:26) -&gt; Delmenhorst (an 15:24)  am 18.05.2019  Grund: Fahrzeugstörung… https://t.co/0yqkl1fESN",
         "tweet_id": 1129724864649150500
        },
        {
         "ID": "hasoc_2020_de_474",
         "_deepnote_index_column": 38,
         "task1": 1,
         "task2": 1,
         "text": "@JoergRMayer @lichtmesz Wirst schon noch deinen Versorgungsposten bekommen. So wie jeder hartnäckige Karrierist. Ab… https://t.co/UsQERWQxXm",
         "tweet_id": 1131183823948013600
        },
        {
         "ID": "hasoc_2020_de_548",
         "_deepnote_index_column": 39,
         "task1": 0,
         "task2": 0,
         "text": "@Hartes_Geld #Nazis waren schon immer #Linke, halt miese #Sozialisten - wie die #Antifanten der #kriminellen #Antifa.",
         "tweet_id": 1127239156508311600
        },
        {
         "ID": "hasoc_2020_de_3163",
         "_deepnote_index_column": 40,
         "task1": 1,
         "task2": 1,
         "text": "RT @451_grad: #Benziner dreckiger als #Diesel?\nFür @Die_Gruenen schon, aber stimmt das auch? 🤷‍\n📍 Noch mehr dazu: https://t.co/NSfD2FOCqd 📍…",
         "tweet_id": 1128056961906413600
        },
        {
         "ID": "hasoc_2020_de_2813",
         "_deepnote_index_column": 41,
         "task1": 1,
         "task2": 1,
         "text": "@xRxss_ Ich hab halt überall Wireless Charging Pads, wär sau dumm. 😅",
         "tweet_id": 1128589927934042100
        },
        {
         "ID": "hasoc_2020_de_1035",
         "_deepnote_index_column": 42,
         "task1": 1,
         "task2": 1,
         "text": "RT @Katerfrieden: Ich bin dafür dass die Grünen das Geschoß ertragen. https://t.co/E8azul8WmQ",
         "tweet_id": 1133741065901609000
        },
        {
         "ID": "hasoc_2020_de_2368",
         "_deepnote_index_column": 43,
         "task1": 1,
         "task2": 0,
         "text": "RT @braindead_xy: @tagesschau Deutsche Regierung stürzt ausländische Regierung.\n\nUnd feiert das!\n\nIhr Deutschen (87%) kotzt mich so an! Ihr…",
         "tweet_id": 1130112447690530800
        },
        {
         "ID": "hasoc_2020_de_3140",
         "_deepnote_index_column": 44,
         "task1": 1,
         "task2": 1,
         "text": "@johannesw82 @bellefired @VollzeitJonas 🤦‍♂️wie kann man nur so dumm sein 😂🤣",
         "tweet_id": 1134362607249449000
        },
        {
         "ID": "hasoc_2020_de_2663",
         "_deepnote_index_column": 45,
         "task1": 1,
         "task2": 1,
         "text": "*hier nach belieben auch Politik(er) einfügen.",
         "tweet_id": 1128223819708162000
        },
        {
         "ID": "hasoc_2020_de_1266",
         "_deepnote_index_column": 46,
         "task1": 0,
         "task2": 2,
         "text": "Schlagabtausch  im ZDF..... regierungsgesteuerte SCHEISSE mit dementsprechendem Publikum vor Ort!!!!!!!!!!",
         "tweet_id": 1129130045224685600
        },
        {
         "ID": "hasoc_2020_de_874",
         "_deepnote_index_column": 47,
         "task1": 1,
         "task2": 3,
         "text": "ich verleih nie wieder mein scheiß crusher",
         "tweet_id": 1123557279037644800
        },
        {
         "ID": "hasoc_2020_de_1344",
         "_deepnote_index_column": 48,
         "task1": 1,
         "task2": 1,
         "text": "RT @neomagazin: Deutsche Carpool Karaoke #DoTheyKnowItsEurope https://t.co/JENTkGq11j",
         "tweet_id": 1131663094458003500
        },
        {
         "ID": "hasoc_2020_de_1282",
         "_deepnote_index_column": 49,
         "task1": 1,
         "task2": 1,
         "text": "WELT: Horst Seehofer kündigt kriminellen Clans den Kampf an WELT: Horst Seehofer kündigt kriminellen Clans den Kamp… https://t.co/RT1oMHEGvq",
         "tweet_id": 1131180690777751600
        },
        {
         "ID": "hasoc_2020_de_2590",
         "_deepnote_index_column": 50,
         "task1": 1,
         "task2": 1,
         "text": "RT @Das_KGB: Zum kotzen https://t.co/LGsgiE4Jjb",
         "tweet_id": 1125641143490895900
        },
        {
         "ID": "hasoc_2020_de_2658",
         "_deepnote_index_column": 51,
         "task1": 0,
         "task2": 3,
         "text": "RT @Hihi97948034: Mero ist ein Hurensohn",
         "tweet_id": 1131966011295584300
        },
        {
         "ID": "hasoc_2020_de_1062",
         "_deepnote_index_column": 52,
         "task1": 1,
         "task2": 1,
         "text": "Wenn du schon seit knapp 10 Monaten mit einem Kollegen zusammen arbeitest, nie ein Wort Deutsch mit ihm sprachst, w… https://t.co/94SZrnqotc",
         "tweet_id": 1133326932911808500
        },
        {
         "ID": "hasoc_2020_de_2822",
         "_deepnote_index_column": 53,
         "task1": 1,
         "task2": 1,
         "text": "@domi_twr Einfach kein assi sein der im kreisel nicht blinkt. Und auf der autobahn wäre blinken auch vorteilhaft",
         "tweet_id": 1128699319576612900
        },
        {
         "ID": "hasoc_2020_de_1305",
         "_deepnote_index_column": 54,
         "task1": 0,
         "task2": 2,
         "text": "hab glaube echt gerade den größten hurensohn busfahrer deutschlands",
         "tweet_id": 1125431663163531300
        },
        {
         "ID": "hasoc_2020_de_1254",
         "_deepnote_index_column": 55,
         "task1": 1,
         "task2": 1,
         "text": "RT @tagesschau: EVP-Spitzenkandidat Weber will Europa einen https://t.co/ooAUUBLZTj #Europawahl2019 #Weber #Wahlarena",
         "tweet_id": 1125778708260896800
        },
        {
         "ID": "hasoc_2020_de_94",
         "_deepnote_index_column": 56,
         "task1": 1,
         "task2": 1,
         "text": "Warum haben linke Politiker eigentlich kaum ein richtiges Berufsleben?",
         "tweet_id": 1124427395782643700
        },
        {
         "ID": "hasoc_2020_de_2946",
         "_deepnote_index_column": 57,
         "task1": 1,
         "task2": 1,
         "text": "Aaaain gabriel ❤️😍",
         "tweet_id": 1131875879913742300
        },
        {
         "ID": "hasoc_2020_de_1384",
         "_deepnote_index_column": 58,
         "task1": 1,
         "task2": 1,
         "text": "hört sich an wie Hintergrundmusik für  Mobilfunkwerbung #escorf",
         "tweet_id": 1129105403709743100
        },
        {
         "ID": "hasoc_2020_de_944",
         "_deepnote_index_column": 59,
         "task1": 1,
         "task2": 1,
         "text": "@Der_OG_Julius ist der echt mit der zusammen",
         "tweet_id": 1126030790100435000
        },
        {
         "ID": "hasoc_2020_de_336",
         "_deepnote_index_column": 60,
         "task1": 0,
         "task2": 3,
         "text": "RT @TopTomCH: Aufruf zum ficken!  Da Er @MichaelHaenel3 seit über 3 Monaten keinen echten Schwanz mehr im Arsch hatte wird eine geiler Top…",
         "tweet_id": 1132576718722617300
        },
        {
         "ID": "hasoc_2020_de_2289",
         "_deepnote_index_column": 61,
         "task1": 1,
         "task2": 3,
         "text": "Damit wir vor #Scheisse bewahrt werden: #FusionBleibt !! https://t.co/0mLIf78Gms",
         "tweet_id": 1130882173794496500
        },
        {
         "ID": "hasoc_2020_de_1916",
         "_deepnote_index_column": 62,
         "task1": 1,
         "task2": 1,
         "text": "@go73729092 @Ralf_Stegner Machen doch alle privaten Medien so!",
         "tweet_id": 1132919389169291300
        },
        {
         "ID": "hasoc_2020_de_923",
         "_deepnote_index_column": 63,
         "task1": 1,
         "task2": 1,
         "text": "@xmerve021 ja ist halt echt",
         "tweet_id": 1134044712552357900
        },
        {
         "ID": "hasoc_2020_de_260",
         "_deepnote_index_column": 64,
         "task1": 1,
         "task2": 1,
         "text": "RT @Jesus_folgen: @HaraldBecker80 Als Gegenbeispiel gibt es ein Video, in dem gezeigt wird, wie die DEUTSCHE BUNDESKANZLERIN NAMENS MERKEL…",
         "tweet_id": 1125980915635773400
        },
        {
         "ID": "hasoc_2020_de_1943",
         "_deepnote_index_column": 65,
         "task1": 1,
         "task2": 1,
         "text": "900.000 deutsche „Flüchtlinge“ im EU-Ausland https://t.co/8RsEOvFFY0",
         "tweet_id": 1126848356435865600
        },
        {
         "ID": "hasoc_2020_de_154",
         "_deepnote_index_column": 66,
         "task1": 1,
         "task2": 1,
         "text": "ALERTE Ruche4 Poids=10.2kg -&gt;https://t.co/WTpfLC1Afv  … Infos-&gt;https://t.co/nzwsT5HGPr (14:47:56) https://t.co/gjZnuXZoSy",
         "tweet_id": 1125744075880136700
        },
        {
         "ID": "hasoc_2020_de_3088",
         "_deepnote_index_column": 67,
         "task1": 1,
         "task2": 1,
         "text": "@GG_SSBM was soll dieser scheiß man",
         "tweet_id": 1132456476415541200
        },
        {
         "ID": "hasoc_2020_de_217",
         "_deepnote_index_column": 68,
         "task1": 1,
         "task2": 1,
         "text": "RT @BB12_DE: @krippmarie @KunkelMuhme Die linken Schlägertrupps sind wieder in Deutschland unterwegs. 🙄\n\n#unserwir https://t.co/mhkvN554er",
         "tweet_id": 1131482424809054200
        },
        {
         "ID": "hasoc_2020_de_2137",
         "_deepnote_index_column": 69,
         "task1": 1,
         "task2": 1,
         "text": "RT @Atomreisfleisch: @jacky_lope Und man hört diese ganzen Elektro-Dinger nicht. Ich stelle mich schon heute auf das Geheule ein, wenn Leut…",
         "tweet_id": 1124999989590147100
        },
        {
         "ID": "hasoc_2020_de_1965",
         "_deepnote_index_column": 70,
         "task1": 1,
         "task2": 1,
         "text": "* rachsüchtige Frauen. Es gibt hasserfüllte Frauen,  Frauen die betrügen und lügen. \n\nHört auf eure Mysogynie hinte… https://t.co/k2mvmkkVsC",
         "tweet_id": 1133839673988718600
        },
        {
         "ID": "hasoc_2020_de_167",
         "_deepnote_index_column": 71,
         "task1": 0,
         "task2": 1,
         "text": "RT @AchimSpiegel: \"Frau Merkel, es bewegt mich, dass Sie noch immer nicht vor Gericht stehen und tatsächlich noch Kanzlerin sind.\" https://…",
         "tweet_id": 1123511443687727100
        },
        {
         "ID": "hasoc_2020_de_1886",
         "_deepnote_index_column": 72,
         "task1": 0,
         "task2": 1,
         "text": "die arroganz der leute aus den westlichen bundesländern ist so ekelhaft und abstoßend, ich könnt kotzen \nwie kann m… https://t.co/uXdFuejOZB",
         "tweet_id": 1132931582032076800
        },
        {
         "ID": "hasoc_2020_de_1962",
         "_deepnote_index_column": 73,
         "task1": 1,
         "task2": 1,
         "text": "@Pokehita97 Hey sorry das ich so overreacted habe!! (kenn das deutsche wort für overreact nicht sry😅) CAN U PLS FOR… https://t.co/nLPmQIhVCW",
         "tweet_id": 1124444323997716500
        },
        {
         "ID": "hasoc_2020_de_2217",
         "_deepnote_index_column": 74,
         "task1": 0,
         "task2": 0,
         "text": "RT @burger_ein: Radikale Moslems und nationalistische Türken Die verschwiegenenen Wähler der Grünen https://t.co/u7Jj38Ldc1",
         "tweet_id": 1125147385791942700
        },
        {
         "ID": "hasoc_2020_de_762",
         "_deepnote_index_column": 75,
         "task1": 1,
         "task2": 1,
         "text": "@Obi_Jan_Kenobii @walkerjole Die haben sich mit der Zeit schon selbst entsorgt.",
         "tweet_id": 1134550809855844400
        },
        {
         "ID": "hasoc_2020_de_227",
         "_deepnote_index_column": 76,
         "task1": 1,
         "task2": 1,
         "text": "Siege &gt; CoD",
         "tweet_id": 1124388900464676900
        },
        {
         "ID": "hasoc_2020_de_59",
         "_deepnote_index_column": 77,
         "task1": 1,
         "task2": 1,
         "text": "RT @RolandTichy: Keiner redet vom Wetter, nur wir: Die Grünen sitzen jetzt ihrerseits in der Klimafalle: Radiale Maßnahmen sind hübsche For…",
         "tweet_id": 1133480201197948900
        },
        {
         "ID": "hasoc_2020_de_2573",
         "_deepnote_index_column": 78,
         "task1": 1,
         "task2": 1,
         "text": "@MartinaHolst @rudolfuz Politiker kriegen bestimmt wieder Extrarechte",
         "tweet_id": 1130111608838139900
        },
        {
         "ID": "hasoc_2020_de_980",
         "_deepnote_index_column": 79,
         "task1": 1,
         "task2": 1,
         "text": "Die Bigotterie von Grünen ist kaum aushaltbar. FCK NAZIS auf den Plakaten aber neben ihnen Stehen und Flyer verteil… https://t.co/D2SoJv7zU0",
         "tweet_id": 1124744715868549100
        },
        {
         "ID": "hasoc_2020_de_792",
         "_deepnote_index_column": 80,
         "task1": 1,
         "task2": 1,
         "text": "@STARGAMESTV69 @verankernLUL @Papaplatte @tim0cy arsch https://t.co/ezeiGmpJAb",
         "tweet_id": 1130096303801872400
        },
        {
         "ID": "hasoc_2020_de_1069",
         "_deepnote_index_column": 81,
         "task1": 1,
         "task2": 1,
         "text": "EINFACH NUR AUS ZEITVERTREIB",
         "tweet_id": 1131161308911398900
        },
        {
         "ID": "hasoc_2020_de_2052",
         "_deepnote_index_column": 82,
         "task1": 0,
         "task2": 2,
         "text": "@Tlaytweet hilfe diese frau was befindet sich in ihrem kopf an der stelle, wo normale menschen ein hirn haben?",
         "tweet_id": 1133436311979868200
        },
        {
         "ID": "hasoc_2020_de_3433",
         "_deepnote_index_column": 83,
         "task1": 1,
         "task2": 1,
         "text": "RT @PeterZobel4: @ZDFheute \"Keine Zusammenarbeit mit rechten Parteien\". Also gehört Merkel einer linken Partei an? Oder was soll das heißen?",
         "tweet_id": 1124546006505357300
        },
        {
         "ID": "hasoc_2020_de_1435",
         "_deepnote_index_column": 84,
         "task1": 1,
         "task2": 1,
         "text": "Gutschein \"-50% REBELITA Fleecepulover für Frauen!\" https://t.co/EC0VX6LWYj",
         "tweet_id": 1129294617134784500
        },
        {
         "ID": "hasoc_2020_de_2513",
         "_deepnote_index_column": 85,
         "task1": 0,
         "task2": 3,
         "text": "#Hofheim Hobbyhure-sucht-Taschengeld: Anna zarte St*te - Zarte, deutsche St*te  will d... https://t.co/z5s5n4K9fl https://t.co/FJqJxBuyIS",
         "tweet_id": 1129510569256587300
        },
        {
         "ID": "hasoc_2020_de_862",
         "_deepnote_index_column": 86,
         "task1": 0,
         "task2": 0,
         "text": "„Die Wähler sind halt zu dumm, sodass wir ihnen unsere Positionen nicht vermitteln konnten. Diese Verachtung für si… https://t.co/rMolA6HBKi",
         "tweet_id": 1132621144824139800
        },
        {
         "ID": "hasoc_2020_de_682",
         "_deepnote_index_column": 87,
         "task1": 1,
         "task2": 1,
         "text": "benutze dein Gehirn! \nNiemand hat mich bezahlt :(\n#EducaAunFifi",
         "tweet_id": 1125732050818994200
        },
        {
         "ID": "hasoc_2020_de_2005",
         "_deepnote_index_column": 88,
         "task1": 1,
         "task2": 1,
         "text": "RT @Ulrik_von: @spdberlin @gabischoff @RaedSalehBerlin Das neue Logo ist auch schon fertig. https://t.co/BZeIk9yBzf",
         "tweet_id": 1124636523788406800
        },
        {
         "ID": "hasoc_2020_de_1763",
         "_deepnote_index_column": 89,
         "task1": 1,
         "task2": 1,
         "text": "@RainerWinklerDE Lieben Heteros, jetz habter richtig Scheiße am Arsch, etzala fliegt ihr raus.",
         "tweet_id": 1132616921155743700
        },
        {
         "ID": "hasoc_2020_de_3104",
         "_deepnote_index_column": 90,
         "task1": 1,
         "task2": 1,
         "text": "@Piratenpartei Was soll dieses Gequatsche von Nazis? Wer gegen ILLEGALE Migration und linken Gesinnungsterror ist,… https://t.co/FRLCTDCZoE",
         "tweet_id": 1134560620349706200
        },
        {
         "ID": "hasoc_2020_de_2162",
         "_deepnote_index_column": 91,
         "task1": 0,
         "task2": 3,
         "text": "@nicerlucas Scheiß alki",
         "tweet_id": 1134399169010065400
        },
        {
         "ID": "hasoc_2020_de_2690",
         "_deepnote_index_column": 92,
         "task1": 1,
         "task2": 1,
         "text": "@DanyVause Deutsche Kultur https://t.co/nUdQfuCLsc",
         "tweet_id": 1128376953747247100
        },
        {
         "ID": "hasoc_2020_de_380",
         "_deepnote_index_column": 93,
         "task1": 1,
         "task2": 1,
         "text": "@maynstream Bin halt aber schon tip-top motivator https://t.co/AvRaWLUEMI",
         "tweet_id": 1128709427824136200
        },
        {
         "ID": "hasoc_2020_de_2475",
         "_deepnote_index_column": 94,
         "task1": 0,
         "task2": 1,
         "text": "wo sitzt dieser schlaumeier, der für alle idioten dieser welt die reden schreibt.",
         "tweet_id": 1134189608986710000
        },
        {
         "ID": "hasoc_2020_de_1320",
         "_deepnote_index_column": 95,
         "task1": 0,
         "task2": 3,
         "text": "ich hasse meine eltern ohne spass diese dummen hurensohne ich bin fucking 14 und die heulen rum wenn ich EINE fuxki… https://t.co/ARddcXxbDn",
         "tweet_id": 1128390300026789900
        },
        {
         "ID": "hasoc_2020_de_2707",
         "_deepnote_index_column": 96,
         "task1": 1,
         "task2": 1,
         "text": "scheiß bauchschmerzen",
         "tweet_id": 1127530950055874600
        },
        {
         "ID": "hasoc_2020_de_2890",
         "_deepnote_index_column": 97,
         "task1": 1,
         "task2": 1,
         "text": "@DrThomasBruns @VonSchwer Wer tritt denn heute von den dt. Politikern noch zurück?\nDemnach hätte Merkel schon vor J… https://t.co/rRzVlfdkMK",
         "tweet_id": 1132663154943701000
        },
        {
         "ID": "hasoc_2020_de_2065",
         "_deepnote_index_column": 98,
         "task1": 1,
         "task2": 1,
         "text": "RT @ZDFheute: Wer wählte die Grünen bei der #Europawahl2019? https://t.co/bDmYsTVuTf",
         "tweet_id": 1132690636027641900
        },
        {
         "ID": "hasoc_2020_de_870",
         "_deepnote_index_column": 99,
         "task1": 1,
         "task2": 1,
         "text": "@tukyabitch lass linken",
         "tweet_id": 1125638278768681000
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1133388798925189122</td>\n",
       "      <td>Deutsche rothaarige porno reife deutsche fraue...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_2684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1131117000279961600</td>\n",
       "      <td>Lehrstück auch, wie in der linken Jammerfemini...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1127134592517980161</td>\n",
       "      <td>RT @NDRinfo: Die deutsche Klimaaktivistin Luis...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1128897106171842560</td>\n",
       "      <td>@ruhrbahn jeden Morgen eine neue „Fahrzeugstör...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1123576753199484928</td>\n",
       "      <td>@Junge_Freiheit Die Inkas hatten sich schon dä...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>1129340066604699649</td>\n",
       "      <td>in conclusion, ich bin dumm</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>1133032786397409280</td>\n",
       "      <td>RT @chrisjuko: Mit viel Geld an die Macht \\n#S...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>1125397127251804162</td>\n",
       "      <td>@FritzAlter1 @MiriamOzen Ich habe den Eindruck...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>1128318292228222982</td>\n",
       "      <td>RT @LucaBBM: Wieso hab ich immer Stress mit so...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>hasoc_2020_de_2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>1124962538632957952</td>\n",
       "      <td>RT @tschemal: Deutsche Depressionen https://t....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_2020_de_225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2452 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0     1133388798925189122  Deutsche rothaarige porno reife deutsche fraue...   \n",
       "1     1131117000279961600  Lehrstück auch, wie in der linken Jammerfemini...   \n",
       "2     1127134592517980161  RT @NDRinfo: Die deutsche Klimaaktivistin Luis...   \n",
       "3     1128897106171842560  @ruhrbahn jeden Morgen eine neue „Fahrzeugstör...   \n",
       "4     1123576753199484928  @Junge_Freiheit Die Inkas hatten sich schon dä...   \n",
       "...                   ...                                                ...   \n",
       "2447  1129340066604699649                        in conclusion, ich bin dumm   \n",
       "2448  1133032786397409280  RT @chrisjuko: Mit viel Geld an die Macht \\n#S...   \n",
       "2449  1125397127251804162  @FritzAlter1 @MiriamOzen Ich habe den Eindruck...   \n",
       "2450  1128318292228222982  RT @LucaBBM: Wieso hab ich immer Stress mit so...   \n",
       "2451  1124962538632957952  RT @tschemal: Deutsche Depressionen https://t....   \n",
       "\n",
       "      task1  task2                  ID  \n",
       "0         1      1  hasoc_2020_de_2684  \n",
       "1         0      1  hasoc_2020_de_2440  \n",
       "2         1      1  hasoc_2020_de_1042  \n",
       "3         1      1   hasoc_2020_de_774  \n",
       "4         1      1   hasoc_2020_de_559  \n",
       "...     ...    ...                 ...  \n",
       "2447      1      1  hasoc_2020_de_1840  \n",
       "2448      1      1   hasoc_2020_de_345  \n",
       "2449      1      1   hasoc_2020_de_700  \n",
       "2450      0      3  hasoc_2020_de_2167  \n",
       "2451      1      1   hasoc_2020_de_225  \n",
       "\n",
       "[2452 rows x 5 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "cell_id": "00009-316001c7-147b-4324-b306-1fb4554c0c25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "cell_id": "00010-5896edee-3448-491b-92d3-c8f9e249d786",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['text'], df['task1'], test_size=0.2, stratify=df['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "cell_id": "00011-2400c9ff-2747-4df5-a672-d519928ef0f3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.apply(count_words).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "cell_id": "00012-bff158fb-59b1-414d-a57c-72b559198e50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 74\n",
    "posts = train_x.values\n",
    "categories = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "cell_id": "00013-7cdb24ac-0c1f-4b44-b0da-70bc6499f2bb",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "cell_id": "00014-660813de-c66b-4a5f-b451-d2a2bed12c11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @PillePa47127736 @rezomusik Ich wollte nur zeigen, dass meine Tweets einen Scheiß zur Umweltbelastung beiträgt. Und… https://t.co/NXyPAdCIBE\n",
      "Token IDs: tensor([  102, 16249,  4322,   231,  1142,  6437,  3423,  3955,  4047, 16249,\n",
      "          248,  5267,  4110,   260,  2091,   435,  3274,   806,   347,  1388,\n",
      "        21042,  3488,   366, 11294,   327,  2192, 15544,  4718, 30942,   552,\n",
      "          143,   101,  6222,   847,  1061,  1061,   160,   552,   928,  1061,\n",
      "          132, 12508,  1142, 10177,  1502, 30937,   103,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', posts[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "cell_id": "00015-fdf17462-fa47-42a5-a326-d5fe2eb0fa22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maxn = 0\n",
    "# for i in input_ids:\n",
    "#     if maxn < len(i[0]):\n",
    "#         maxn = len(i[0])\n",
    "# print(maxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "cell_id": "00015-4ffeb47f-115b-47de-ae86-60e24bbb9f73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,715 training samples\n",
      "  246 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.875 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "cell_id": "00016-35bc439d-d9d7-475f-a81a-4cbf21235a04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "cell_id": "00017-930b9086-8194-4ae0-b1f8-9acea0c32938",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-german-dbmdz-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-dbmdz-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31102, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-german-dbmdz-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 4, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "cell_id": "00018-7977d83b-29e8-43f5-bbd6-0852f9d1343f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (31102, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (4, 768)\n",
      "classifier.bias                                                 (4,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "cell_id": "00019-c7d00cc1-4be1-4afc-8838-768a2e99e91e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "cell_id": "00020-7a071a93-c375-4743-9e61-2a669cad31ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "cell_id": "00021-ec9bf438-ff81-4099-a8d1-1f009425becd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "cell_id": "00022-f935f9e6-c356-4bc4-8890-88825a6856fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cell_id": "00023-4dd4b295-39f1-49ae-8020-77abe2230103",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    108.    Elapsed: 0:00:10.\n",
      "  Batch    80  of    108.    Elapsed: 0:00:20.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epcoh took: 0:00:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation Loss: 0.38\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    108.    Elapsed: 0:00:10.\n",
      "  Batch    80  of    108.    Elapsed: 0:00:19.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epcoh took: 0:00:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:54 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "cell_id": "00024-4b845de9-8564-48cc-8e11-5386c6985626",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "column_count": 5,
       "columns": [
        {
         "dtype": "float64",
         "name": "Training Loss",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.3871199141773913,
            "bin_start": 0.3676942778682267,
            "count": 1
           },
           {
            "bin_end": 0.40654555048655583,
            "bin_start": 0.3871199141773913,
            "count": 0
           },
           {
            "bin_end": 0.4259711867957203,
            "bin_start": 0.40654555048655583,
            "count": 0
           },
           {
            "bin_end": 0.4453968231048849,
            "bin_start": 0.4259711867957203,
            "count": 0
           },
           {
            "bin_end": 0.4648224594140494,
            "bin_start": 0.4453968231048849,
            "count": 0
           },
           {
            "bin_end": 0.484248095723214,
            "bin_start": 0.4648224594140494,
            "count": 0
           },
           {
            "bin_end": 0.5036737320323785,
            "bin_start": 0.484248095723214,
            "count": 0
           },
           {
            "bin_end": 0.523099368341543,
            "bin_start": 0.5036737320323785,
            "count": 0
           },
           {
            "bin_end": 0.5425250046507076,
            "bin_start": 0.523099368341543,
            "count": 0
           },
           {
            "bin_end": 0.5619506409598721,
            "bin_start": 0.5425250046507076,
            "count": 1
           }
          ],
          "max": 0.5619506409598721,
          "min": 0.3676942778682267,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "float64",
         "name": "Valid. Loss",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.3523804827593267,
            "bin_start": 0.3498148387297988,
            "count": 1
           },
           {
            "bin_end": 0.3549461267888546,
            "bin_start": 0.3523804827593267,
            "count": 0
           },
           {
            "bin_end": 0.3575117708183825,
            "bin_start": 0.3549461267888546,
            "count": 0
           },
           {
            "bin_end": 0.3600774148479104,
            "bin_start": 0.3575117708183825,
            "count": 0
           },
           {
            "bin_end": 0.3626430588774383,
            "bin_start": 0.3600774148479104,
            "count": 0
           },
           {
            "bin_end": 0.3652087029069662,
            "bin_start": 0.3626430588774383,
            "count": 0
           },
           {
            "bin_end": 0.36777434693649413,
            "bin_start": 0.3652087029069662,
            "count": 0
           },
           {
            "bin_end": 0.370339990966022,
            "bin_start": 0.36777434693649413,
            "count": 0
           },
           {
            "bin_end": 0.3729056349955499,
            "bin_start": 0.370339990966022,
            "count": 0
           },
           {
            "bin_end": 0.3754712790250778,
            "bin_start": 0.3729056349955499,
            "count": 1
           }
          ],
          "max": 0.3754712790250778,
          "min": 0.3498148387297988,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "float64",
         "name": "Valid. Accur.",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.8539062500000001,
            "bin_start": 0.8528645833333334,
            "count": 1
           },
           {
            "bin_end": 0.8549479166666667,
            "bin_start": 0.8539062500000001,
            "count": 0
           },
           {
            "bin_end": 0.8559895833333333,
            "bin_start": 0.8549479166666667,
            "count": 0
           },
           {
            "bin_end": 0.85703125,
            "bin_start": 0.8559895833333333,
            "count": 0
           },
           {
            "bin_end": 0.8580729166666667,
            "bin_start": 0.85703125,
            "count": 0
           },
           {
            "bin_end": 0.8591145833333333,
            "bin_start": 0.8580729166666667,
            "count": 0
           },
           {
            "bin_end": 0.86015625,
            "bin_start": 0.8591145833333333,
            "count": 0
           },
           {
            "bin_end": 0.8611979166666667,
            "bin_start": 0.86015625,
            "count": 0
           },
           {
            "bin_end": 0.8622395833333334,
            "bin_start": 0.8611979166666667,
            "count": 0
           },
           {
            "bin_end": 0.86328125,
            "bin_start": 0.8622395833333334,
            "count": 1
           }
          ],
          "max": 0.86328125,
          "min": 0.8528645833333334,
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "object",
         "name": "Training Time",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "0:00:26"
           }
          ],
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "object",
         "name": "Validation Time",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "0:00:01"
           }
          ],
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 2,
       "rows_bottom": null,
       "rows_top": [
        {
         "Training Loss": 0.5619506409598721,
         "Training Time": "0:00:26",
         "Valid. Accur.": 0.86328125,
         "Valid. Loss": 0.3754712790250778,
         "Validation Time": "0:00:01",
         "_deepnote_index_column": 1
        },
        {
         "Training Loss": 0.3676942778682267,
         "Training Time": "0:00:26",
         "Valid. Accur.": 0.8528645833333334,
         "Valid. Loss": 0.3498148387297988,
         "Validation Time": "0:00:01",
         "_deepnote_index_column": 2
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0:00:26</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0:00:26</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.56         0.38           0.86       0:00:26         0:00:01\n",
       "2               0.37         0.35           0.85       0:00:26         0:00:01"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cell_id": "00025-7900fc16-8cd9-4342-853c-22ead051492b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cell_id": "00026-944e6d3b-b8a7-4272-a2b2-4d95a0a336eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts = valid_x.values\n",
    "categories = valid_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cell_id": "00027-7f6a5e98-208e-4b83-a225-ec22610a8e4a",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cell_id": "00028-5b7a2ad2-a7c2-4cee-a943-04bed8dcd314",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 491 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "cell_id": "00029-94e88b41-8557-4532-8232-26bcd3e186ba",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68428683  3.8481889  -2.1858387  -2.5279148 ]\n",
      " [-0.02049743  3.8279877  -1.8866813  -2.2648325 ]\n",
      " [ 0.8243229   3.568537   -2.3134735  -2.5458517 ]\n",
      " [ 1.0863403   3.6213236  -2.5824502  -2.6082053 ]\n",
      " [ 0.38756293  3.6930938  -2.2260582  -2.460192  ]\n",
      " [ 0.41200435  3.9861095  -2.0716238  -2.5371358 ]\n",
      " [ 1.1985929   3.6286051  -2.5712252  -2.686405  ]\n",
      " [ 0.00659738  3.3986619  -1.7234045  -1.8784758 ]\n",
      " [ 0.91401494  3.7148178  -2.5729551  -2.6785269 ]\n",
      " [ 1.8955438   2.863001   -2.6840312  -2.831638  ]\n",
      " [ 1.2554092   3.492081   -2.6119995  -2.8383734 ]\n",
      " [ 0.5254765   3.437034   -2.2476583  -2.5845327 ]\n",
      " [ 2.8742137   0.14882568 -2.1940696  -2.1060605 ]\n",
      " [ 1.1510572   3.450995   -2.6009371  -2.6221318 ]\n",
      " [ 0.65465593  4.0587125  -2.4744384  -2.5904906 ]\n",
      " [ 1.5799985   2.9082248  -2.680606   -2.7477477 ]] [1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0],true_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "cell_id": "00030-b8da3ca0-957f-4cde-8168-b16b980fe581",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n",
      "Accuracy: 0.8635437881873728\n",
      "/opt/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "predicts = []\n",
    "accurate = 0\n",
    "total_len = 0\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    predicts.append(pred_labels_i)\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "\n",
    "    matthews_set.append(matthews)\n",
    "    for j in range(len(true_labels[i])):\n",
    "        if true_labels[i][j] == pred_labels_i[j]:\n",
    "            accurate+=1\n",
    "        total_len+=1\n",
    "print(\"Accuracy:\",accurate/total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "cell_id": "00031-c62c4660-e5a0-495a-8013-f6c2c434d123",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cell_id": "00032-125872d9-8943-42c7-9271-98714294aa87",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.605\n"
     ]
    }
   ],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "cell_id": "00033-b3d8fb5e-eec8-4bcf-9f93-b9f621f0a81e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8635437881873728"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurate = 0\n",
    "for (i,j) in zip(flat_predictions, flat_true_labels):\n",
    "    if i==j:\n",
    "        accurate += 1\n",
    "accurate/len(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "cell_id": "00034-d3f29f2b-13c3-44b0-bad3-f7d0274cdc71",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7949384447561165"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(flat_true_labels, flat_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "cell_id": "00035-5868d782-1680-4050-88f1-b3e9aec0aed4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = '/home/jovyan/work/Hate-Speech-Detection/model_epoch_'+str(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "cell_id": "00036-264dc950-4723-40d7-8386-436312bd464d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "cell_id": "00037-f835665e-3af1-4102-ae39-edc715143132",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = torch.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "cell_id": "00038-1ad95704-c388-42e4-bd9f-79a89fc79492",
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5595bdb8-5007-4b26-a196-adb914f8a1d3",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
