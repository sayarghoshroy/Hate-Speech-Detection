{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HASOC_alltests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f4844a265fb46b2ac7bc3495203ed4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f7dfb4629c240b69132e4d5aa7c332d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09abbce550634a45ae3b891e030a12f8",
              "IPY_MODEL_d9bcd80d126d4ac2a031324d79b8a1fb"
            ]
          }
        },
        "6f7dfb4629c240b69132e4d5aa7c332d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09abbce550634a45ae3b891e030a12f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_27edd6a1538c46f0aef7c69899acbfbe",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 247333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 247333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af9f8a23eeee4f6cb9e9cbee8081a627"
          }
        },
        "d9bcd80d126d4ac2a031324d79b8a1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f50cda79879470a834193c725c928e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 247k/247k [00:00&lt;00:00, 1.59MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c22825422fd4fa8acf4a8f3ddae1e0a"
          }
        },
        "27edd6a1538c46f0aef7c69899acbfbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af9f8a23eeee4f6cb9e9cbee8081a627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f50cda79879470a834193c725c928e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c22825422fd4fa8acf4a8f3ddae1e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36b8eeea93fa4d74ace74bf9eb501cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f39e5acab674eb191a1ad690312c6bb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_110e6e1655e941b28c92de002ea7a947",
              "IPY_MODEL_41c181fc244543f29affc0e1611b9f15"
            ]
          }
        },
        "9f39e5acab674eb191a1ad690312c6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "110e6e1655e941b28c92de002ea7a947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_80bc1b33433f4472b34e917c1cf00c47",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76d0acb3ba3647a081a6915c51f1204b"
          }
        },
        "41c181fc244543f29affc0e1611b9f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5cc684216fcc4265ae0d62b4a9e94b51",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 785B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea55906d911f473d9b27f1dac6f46ac3"
          }
        },
        "80bc1b33433f4472b34e917c1cf00c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76d0acb3ba3647a081a6915c51f1204b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5cc684216fcc4265ae0d62b4a9e94b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea55906d911f473d9b27f1dac6f46ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b6eece69dfc44d78b830223fd4f1fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_94b44bdb8dd342b0bc2b15d112c368f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3bd978faf6604530b16f52ac1386cc96",
              "IPY_MODEL_ad755932caec4926b6d7623eca8ba814"
            ]
          }
        },
        "94b44bdb8dd342b0bc2b15d112c368f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3bd978faf6604530b16f52ac1386cc96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_29a19cbabb0c4e8e86ee941e327104cc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442256365,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442256365,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6efc8f6cbd084b7b8267c33131b14818"
          }
        },
        "ad755932caec4926b6d7623eca8ba814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0eb60a3ff694e46910544ed880512f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442M/442M [00:28&lt;00:00, 15.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_adcf12e5ffc149bea974f804368f8d7d"
          }
        },
        "29a19cbabb0c4e8e86ee941e327104cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6efc8f6cbd084b7b8267c33131b14818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0eb60a3ff694e46910544ed880512f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "adcf12e5ffc149bea974f804368f8d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/HASOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymx75g3TKBPu",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AusydJSf1JWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5c0008a-17e0-4b61-ba07-f2065572d272"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkgSSbTKFnl",
        "colab_type": "text"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5k6sJxMId20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "165489e9-22fe-4e05-87d7-1ba6b13c60b6"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install bert-tensorflow\n",
        "!pip install transformers\n",
        "!pip install seaborn\n",
        "!pip install sklearn-crfsuite\n",
        "!pip install -U sentence-transformers\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/16/0f9376af49c6adcfbaf2470a8f500105a74dd803aa54ac0110af445837b5/bert_tensorflow-1.0.4-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 6.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=62d2471a5c0172327a8267b9eb1fbe1e8f6269810eab2bf6159dbded4b0660a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.15.0)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 8.3MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/74/49848e9bb64482a7e5f475cc66da5de759077817ede36f8812060ebcaba6/sentence-transformers-0.3.6.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers<3.2.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.8.1rc2)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.2.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.2.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.6-cp36-none-any.whl size=101182 sha256=e13373899f28dd4bcdaf4a6fc6a85da62bf045b2b535c7148f7c1d28ab10caf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/3f/75/c0c4b3ef5dfbf8806d37b8dc661861772aba2f7aa419c85a9b\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-0.3.6\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8mJWYRNKH7l",
        "colab_type": "text"
      },
      "source": [
        "# Get imports\n",
        "\n",
        "- **General:** random, pickle, re, time, datetime\n",
        "- **General DS:** pandas, numpy, sklearn, matplotlib, seaborn, nltk\n",
        "- **Deep Learning:** torch, transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N0yrIDfJZaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5aee9b1a-7eca-4c4c-f914-8731d4da6535"
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk36oE5Lrg9W",
        "colab_type": "text"
      },
      "source": [
        "# GPU device Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2gcXgxjBfAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "598b2484-67a9-4f35-86dd-acd93a721eb9"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKl5AjVg2GGu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset load setup and exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtF92-5O1kHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_ROOT = '/content/drive/My Drive/HASOC/Data/2020_processed_data/'\n",
        "OUTPUT_ROOT = '/content/drive/My Drive/HASOC/'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34eBicH3p-Mc",
        "colab_type": "text"
      },
      "source": [
        "## Exploration with the German dataset\n",
        "\n",
        "This is not be used in the actual pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUnNe1bM1vaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(DATASET_ROOT+'ge.pickle', 'rb') as f:\n",
        "  ged = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MXmbq82tfXr",
        "colab_type": "text"
      },
      "source": [
        "Checking it once for content description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLKtbO3yBD-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fb3d2623-f0ca-4886-b5fc-5152c5557b1a"
      },
      "source": [
        "ged.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tweet_id', 'task_1', 'task_2', 'hasoc_id', 'full_tweet', 'tweet_raw_text', 'hashtags', 'smiley', 'emoji', 'url', 'mentions', 'numerals', 'reserved_word', 'emotext', 'segmented_hash'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJlOlcu0QWM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "592ffa71-c876-4c95-c163-be899294a3d1"
      },
      "source": [
        "ged['task_1'][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSohluLctjzO",
        "colab_type": "text"
      },
      "source": [
        "## Exploration: Split data into train-test-val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXL1bHONOQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train1_hash = ged['segmented_hash'][:2000]\n",
        "# test1_hash = ged['segmented_hash'][2000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYE1Uvx2O35x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_hash = []\n",
        "# for lis in train1_hash:\n",
        "#   train_hash.append(' '.join(lis))\n",
        "# test_hash = []\n",
        "# for lis in test1_hash:\n",
        "#   test_hash.append(' '.join(lis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyLubZmQuGzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_text, test_text, train_t1s, test_t1s = model_selection.train_test_split(\n",
        "#     ged['tweet_raw_text'],\n",
        "#     ged['task_1'],\n",
        "#     test_size = 0.2,\n",
        "#     # random_state = 42\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoKUgiqgtag5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_text = ged['tweet_raw_text'][:2000]\n",
        "# train_t1s = ged['task_1'][:2000]\n",
        "\n",
        "# test_text = ged['tweet_raw_text'][2000:]\n",
        "# test_t1s = ged['task_1'][2000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwhRamhzzZRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.DataFrame.from_dict(ged)\n",
        "# df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub6jEswMzlcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_df, test_df = model_selection.train_test_split(df, random_state = 42, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNTWBGA0tnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train1_hash = list(train_df['segmented_hash'])\n",
        "# test1_hash = list(test_df['segmented_hash'])\n",
        "# train_hash = []\n",
        "# for lis in train1_hash:\n",
        "#   train_hash.append(' '.join(lis))\n",
        "# test_hash = []\n",
        "# for lis in test1_hash:\n",
        "#   test_hash.append(' '.join(lis))\n",
        "# train_text = list(train_df['tweet_raw_text'])\n",
        "# train_t1s = list(train_df['task_1'])\n",
        "\n",
        "# test_text = list(test_df['tweet_raw_text'])\n",
        "# test_t1s = list(test_df['task_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_XcLvgGqafu",
        "colab_type": "text"
      },
      "source": [
        "# get Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU8BMAterqu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(embeddings):\n",
        "  # emb = []\n",
        "  # for e in embeddings:\n",
        "  #   emb.append({'feat': e})\n",
        "  emb = embeddings\n",
        "  return emb"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Jmwjber44S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_task(tasklist, taskType = 1):\n",
        "  newtasks = []\n",
        "  if taskType == 2:\n",
        "    for x in tasklist:\n",
        "      if x == \"NONE\":\n",
        "        newtasks.append(0)\n",
        "      elif x == \"HATE\":\n",
        "        newtasks.append(1)\n",
        "      elif x == \"PRFN\":\n",
        "        newtasks.append(2)\n",
        "      elif x == \"OFFN\":\n",
        "        newtasks.append(3)\n",
        "      else:\n",
        "        raise NameError(\"Class not defined\")\n",
        "  else:\n",
        "    for x in tasklist:\n",
        "      if x == 'NOT':\n",
        "        # newtasks.append(['0'])\n",
        "        newtasks.append(0)\n",
        "      elif x ==\"HOF\":\n",
        "        # newtasks.append(['1'])\n",
        "        newtasks.append(1)\n",
        "      else:\n",
        "        raise NameError(\"Class not defined\")\n",
        "\n",
        "  return newtasks"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP8JO5dt8FVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tasks_reverse(tasklist, taskType = 1):\n",
        "  newtasks = []\n",
        "  if taskType == 2:\n",
        "    for x in tasklist:\n",
        "      # print(x)\n",
        "      if x == 0:\n",
        "        newtasks.append(\"NONE\")\n",
        "      elif x == 1:\n",
        "        newtasks.append(\"HATE\")\n",
        "      elif x == 2:\n",
        "        newtasks.append(\"PRFN\")\n",
        "      elif x == 3:\n",
        "        newtasks.append(\"OFFN\")\n",
        "      else:\n",
        "        raise NameError(\"Class not defined\")\n",
        "  else:\n",
        "    for x in tasklist:\n",
        "      # print(x)\n",
        "      if x == 0:\n",
        "        # newtasks.append(['0'])\n",
        "        newtasks.append(\"NOT\")\n",
        "      elif x == 1:\n",
        "        # newtasks.append(['1'])\n",
        "        newtasks.append(\"HOF\")\n",
        "      else:\n",
        "        raise NameError(\"Class not defined\")\n",
        "\n",
        "  return newtasks\n",
        "  # return tasklist"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlXhZ-jlRdzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cab50cdd-f1ac-4dee-f911-a637b0eb8fab"
      },
      "source": [
        "import gensim.models as gsm\n",
        "e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/My Drive/HASOC/emoji2vec.bin', binary=True)\n",
        "# happy_vector = e2v['😂']    # Produces an embedding vector of length 300\n",
        "\n",
        "# Download the bin file from here https://github.com/uclnlp/emoji2vec/blob/master/pre-trained/emoji2vec.bin\n",
        "\n",
        "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
        "  \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
        "  if dim < 300:\n",
        "    raise IndexError(\"Dim has to be greater than 300\")\n",
        "  result = np.zeros(dim)\n",
        "  if (len(emojiList) == 0):\n",
        "    return result\n",
        "  else:\n",
        "    embs = None\n",
        "    for i in emojiList:\n",
        "      if verbose:\n",
        "        if i not in e2v.vocab:\n",
        "          print(i)\n",
        "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
        "  if np.any(np.isnan(embs)):\n",
        "    return result\n",
        "  result[:300] = embs\n",
        "  return result \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K9SOt3i1ihL",
        "colab_type": "text"
      },
      "source": [
        "# Ablation Studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcKXXDc8qii5",
        "colab_type": "text"
      },
      "source": [
        "## Encoder setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrjmeWh9q7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29ef8f61-0783-40ab-c8ca-036335598b6d"
      },
      "source": [
        "sent_encoder = SentenceTransformer('xlm-r-100langs-bert-base-nli-mean-tokens')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.01G/1.01G [00:18<00:00, 54.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8PBCX-ql4x",
        "colab_type": "text"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqkF55RX303E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData(lang, seed=42):\n",
        "  if lang not in ['hi','en','ge']:\n",
        "      raise NameError(\"Language not found\")\n",
        "  fileName = lang + '.pickle'\n",
        "  with open(DATASET_ROOT+fileName, 'rb') as f:\n",
        "    ged = pickle.load(f)\n",
        "  df = pd.DataFrame.from_dict(ged)\n",
        "  train_df, test_df = model_selection.train_test_split(df, random_state = seed, test_size = 0.25)\n",
        "  return train_df, test_df, df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tshEk6NF1aIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadAllLangs(seed=42):\n",
        "  train_en, test_en, en = loadData('en', seed)\n",
        "  train_ge, test_ge, ge = loadData('ge', seed)\n",
        "  train_hi, test_hi, hi = loadData('hi', seed)\n",
        "\n",
        "  # Combine the languages\n",
        "  train_all = pd.concat([train_en, train_ge, train_hi], ignore_index=True)\n",
        "  test_all = pd.concat([test_en, test_ge, test_hi], ignore_index=True)\n",
        "  all = pd.concat([en, ge, hi], ignore_index=True)\n",
        "\n",
        "  # Randomize\n",
        "  train_all = train_all.sample(frac=1, random_state=seed)\n",
        "  test_all = test_all.sample(frac=1, random_state=seed)\n",
        "  all = all.sample(frac=1, random_state=seed)\n",
        "\n",
        "  return train_all, test_all, all"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir8sgHd9uWmo",
        "colab_type": "text"
      },
      "source": [
        "## Ablation runner functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31CYez461hZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "b = []\n",
        "def trainModelWithFeatures(train_df,test_df, lang, hashtags=True, emojis=True, verbose = False):\n",
        "  \"\"\" Function to train a model on a specific configuration of features \"\"\"\n",
        "  global a\n",
        "  global b\n",
        "\n",
        "  # Get Segmented Hashtags from dataset\n",
        "  train1_hash = list(train_df['segmented_hash'])\n",
        "  test1_hash = list(test_df['segmented_hash'])\n",
        "  \n",
        "  # Segmented hashtags: List of strings --> Space-separated string\n",
        "  train_hash = []\n",
        "  for lis in train1_hash:\n",
        "    train_hash.append(' '.join(lis))\n",
        "  test_hash = []\n",
        "  for lis in test1_hash:\n",
        "    test_hash.append(' '.join(lis))\n",
        "\n",
        "  # Get raw train data from dataset\n",
        "  train_text = list(train_df['tweet_raw_text'])\n",
        "  train_t1s = list(train_df['task_1'])\n",
        "  train_t2s = list(train_df['task_2'])\n",
        "\n",
        "  # Get raw test data from the dataset\n",
        "  test_text = list(test_df['tweet_raw_text'])\n",
        "  test_t1s = list(test_df['task_1'])\n",
        "  test_t2s = list(test_df['task_2'])\n",
        "\n",
        "  # Get embeddings for the text\n",
        "  if verbose:\n",
        "    print(\"Started getting text embeddings\")\n",
        "  train_embeddings = sent_encoder.encode(train_text)\n",
        "  test_embeddings = sent_encoder.encode(test_text)\n",
        "  \n",
        "  # Get usable feature representations\n",
        "  if verbose:\n",
        "    print(\"Finished loading up the text embeddings\")\n",
        "  train_t1 = get_task(train_t1s)\n",
        "  train_t2 = get_task(train_t2s, 2)\n",
        "  \n",
        "  test_t1 = get_task(test_t1s)\n",
        "  test_t2 = get_task(test_t2s, 2)\n",
        "\n",
        "  train_emb = get_features(train_embeddings)\n",
        "  test_emb = get_features(test_embeddings)\n",
        "\n",
        "  # Ablation: If hashtag\n",
        "  if hashtags:\n",
        "    if verbose:\n",
        "      print(\"Started getting hash embeddings\")\n",
        "    train_hashembeddings = sent_encoder.encode(train_hash)\n",
        "    test_hashembeddings = sent_encoder.encode(test_hash)\n",
        "    train_emb = np.concatenate((train_emb , train_hashembeddings), axis = 1)\n",
        "    test_emb =  np.concatenate((test_emb , test_hashembeddings), axis = 1)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Finished loading up the hash embeddings\")\n",
        "  \n",
        "  # Ablation: if emoji\n",
        "  if emojis:\n",
        "    if verbose:\n",
        "      print(\"Started getting emoji embeddings\")\n",
        "    train_emojiEmbs = np.asarray([getEmojiEmbeddings(i,verbose=verbose) for i in (list(train_df['emoji']))])\n",
        "    test_emojiEmbs = np.asarray([getEmojiEmbeddings(i,verbose=verbose) for i in (list(test_df['emoji']))])\n",
        "    train_emb = np.concatenate((train_emb , train_emojiEmbs), axis = 1)\n",
        "    test_emb = np.concatenate((test_emb , test_emojiEmbs), axis = 1)\n",
        "    if verbose:\n",
        "      print(\"Finished loading up the emoji embeddings\")\n",
        "\n",
        "  # Fit and run classifier\n",
        "  a = train_emb\n",
        "  b = test_emb\n",
        "  \n",
        "  assert (train_t2 != test_t2)\n",
        "  clf1 = MLPClassifier(random_state=1, max_iter=300).fit(train_emb, train_t1)\n",
        "  clf2 =  MLPClassifier(random_state=1, max_iter=300).fit(train_emb, train_t2)\n",
        "  if verbose:\n",
        "    print(\"Finished training classifier\")\n",
        "  pred_test_t1 = clf1.predict(test_emb)\n",
        "  pred_test_t2 = clf2.predict(test_emb)\n",
        "\n",
        "  # print(pred_test_t1[:10])\n",
        "\n",
        "  writeOutput(test_df, lang, pred_test_t1, pred_test_t2)\n",
        "\n",
        "  print(\"-\"*20, \"Task 1\", \"-\"*20)\n",
        "  print(classification_report(test_t1, pred_test_t1))\n",
        "  print(\"-\"*20, \"Task 2\", \"-\"*20)\n",
        "  print(classification_report(test_t2, pred_test_t2))\n",
        "\n",
        "  # print(pred_test_t1[:10])\n",
        "  return clf1, clf2, pred_test_t1, pred_test_t2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AEBfX0BHk7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performAblations(train_df, test_df, lang, features = [\"hashtags\", \"emojis\"], verbose = False):\n",
        "  '''\n",
        "  Given the train and test DF and the featureset, print results for the \n",
        "  classifier on all combinations\n",
        "  '''\n",
        "  for i in features:\n",
        "    if i not in [\"hashtags\", \"emojis\"]:\n",
        "      raise NameError(\"Wrong set of features.\")\n",
        " # TODO Make it more extensible \n",
        "  results = {}\n",
        "  print(\"*\"*20, \"ALL:\", \"*\"*20)\n",
        "  results[\"hash+emoji\"] = trainModelWithFeatures(train_df,test_df, lang, verbose = verbose)\n",
        "  print(\"*\"*20, \"HASH:\", \"*\"*20)\n",
        "  results[\"hash\"] = trainModelWithFeatures(train_df,test_df,lang,emojis=False, verbose = verbose)\n",
        "  print(\"*\"*20, \"EMOJI:\", \"*\"*20)\n",
        "  results[\"emoji\"] = trainModelWithFeatures(train_df,test_df,lang,hashtags=False, verbose = verbose)\n",
        "  print(\"*\"*20, \"VANILLA:\", \"*\"*20)\n",
        "  results[\"vanilla\"] = trainModelWithFeatures(train_df,test_df,lang,hashtags=False,emojis=False, verbose = verbose)\n",
        "  return results"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHLT2XuZ57PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def writeOutput(test_df, language, test_pred_1, test_pred_2 = None):\n",
        "\n",
        "  lang = language.upper()\n",
        "  fprefix = 'SUBMISSION_' + lang + '_'\n",
        "\n",
        "  # Get tweet ID and system (HASOC) id for tests:\n",
        "  test_tweet_ids = list(test_df['tweet_id'])\n",
        "  test_hasoc_ids = list(test_df['hasoc_id'])\n",
        "\n",
        "  print('Writing Task A')\n",
        "  fname = fprefix + 'A.csv'\n",
        "  fpath = OUTPUT_ROOT + fname\n",
        "\n",
        "  test_pred_1 = get_tasks_reverse(test_pred_1)\n",
        "  pred = []\n",
        "  for i in range(len(test_tweet_ids)):\n",
        "    pred.append([test_tweet_ids[i], test_pred_1[i], test_hasoc_ids[i]])\n",
        "  # pred = [[test_tweet_ids, i, test_hasoc_ids] for i in test_pred_1]\n",
        "\n",
        "  with open(fpath, 'w+', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
        "    csvwriter.writerows(pred)\n",
        "\n",
        "  print('Finished writing Task A')\n",
        "\n",
        "  if not test_pred_2 is None:\n",
        "    print('Writing Task B')\n",
        "    fname = fprefix + 'B.csv'\n",
        "    fpath = OUTPUT_ROOT + fname\n",
        "\n",
        "    test_pred_2 = get_tasks_reverse(test_pred_2, taskType=2)\n",
        "    pred = []\n",
        "    for i in range(len(test_tweet_ids)):\n",
        "      pred.append([test_tweet_ids[i], test_pred_2[i], test_hasoc_ids[i]])\n",
        "\n",
        "    with open(fpath, 'w+', newline='') as csvfile:\n",
        "      csvwriter = csv.writer(csvfile, delimiter=',')\n",
        "      csvwriter.writerows(pred)\n",
        "  \n",
        "    print('Finished writing Task B')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fxqArg1xXJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def selectTokenizer(lang='en'):\n",
        "#   if lang not in ['en', 'ge', 'hi']:\n",
        "#     raise NameError(\"Wrong language\")\n",
        "#   tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
        "#   if lang == 'en':\n",
        "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "#   elif lang == 'hi':\n",
        "#     tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
        "#   return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_swX7AF24I1",
        "colab_type": "text"
      },
      "source": [
        "## Experiment with combined languages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szL9VEuO3ASf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runCombinedLangs(seed=42):\n",
        "  train_df, test_df, df = loadAllLangs(seed=seed)\n",
        "  performAblations(train_df, test_df, features=['hashtags', 'emojis'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rczVDdN93NsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cbb24d1-00ad-40bf-9020-d115241e1605"
      },
      "source": [
        "runCombinedLangs(seed=69)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************** ALL: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81      1423\n",
            "           1       0.68      0.59      0.63       839\n",
            "\n",
            "    accuracy                           0.75      2262\n",
            "   macro avg       0.73      0.71      0.72      2262\n",
            "weighted avg       0.74      0.75      0.74      2262\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82      1423\n",
            "           1       0.14      0.07      0.10       134\n",
            "           2       0.63      0.61      0.62       468\n",
            "           3       0.32      0.10      0.15       237\n",
            "\n",
            "    accuracy                           0.70      2262\n",
            "   macro avg       0.46      0.42      0.42      2262\n",
            "weighted avg       0.65      0.70      0.67      2262\n",
            "\n",
            "******************** HASH: ********************\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80      1423\n",
            "           1       0.66      0.60      0.63       839\n",
            "\n",
            "    accuracy                           0.74      2262\n",
            "   macro avg       0.72      0.71      0.71      2262\n",
            "weighted avg       0.73      0.74      0.73      2262\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.87      0.81      1423\n",
            "           1       0.18      0.10      0.13       134\n",
            "           2       0.63      0.56      0.59       468\n",
            "           3       0.32      0.19      0.23       237\n",
            "\n",
            "    accuracy                           0.69      2262\n",
            "   macro avg       0.47      0.43      0.44      2262\n",
            "weighted avg       0.65      0.69      0.66      2262\n",
            "\n",
            "******************** EMOJI: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81      1423\n",
            "           1       0.68      0.59      0.64       839\n",
            "\n",
            "    accuracy                           0.75      2262\n",
            "   macro avg       0.73      0.72      0.72      2262\n",
            "weighted avg       0.74      0.75      0.74      2262\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82      1423\n",
            "           1       0.23      0.10      0.14       134\n",
            "           2       0.62      0.58      0.60       468\n",
            "           3       0.26      0.16      0.20       237\n",
            "\n",
            "    accuracy                           0.69      2262\n",
            "   macro avg       0.47      0.43      0.44      2262\n",
            "weighted avg       0.65      0.69      0.67      2262\n",
            "\n",
            "******************** VANILLA: ********************\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82      1423\n",
            "           1       0.72      0.56      0.63       839\n",
            "\n",
            "    accuracy                           0.76      2262\n",
            "   macro avg       0.75      0.72      0.73      2262\n",
            "weighted avg       0.75      0.76      0.75      2262\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81      1423\n",
            "           1       0.17      0.13      0.15       134\n",
            "           2       0.57      0.66      0.61       468\n",
            "           3       0.24      0.12      0.16       237\n",
            "\n",
            "    accuracy                           0.68      2262\n",
            "   macro avg       0.44      0.44      0.43      2262\n",
            "weighted avg       0.65      0.68      0.66      2262\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nocImXKTvS-r",
        "colab_type": "text"
      },
      "source": [
        "## Experiment with all languages and all ablations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FetxHw78cXdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpwQ2uB0vbs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
        "def runAllLangs(langs=['en', 'ge', 'hi'], seed=42):\n",
        "  for i in langs:\n",
        "    if i not in ['en', 'ge', 'hi']:\n",
        "      raise NameError(\"Wrong set of languages\")\n",
        "  \n",
        "  for lang in langs:\n",
        "    print(\"\\n\\nFor the language:\", lang, '\\n', '#'*69)\n",
        "    # tokenizer = selectTokenizer(lang=lang)\n",
        "    train_df, test_df, df = loadData(lang, seed=seed)\n",
        "    results = performAblations(train_df, test_df, lang, features=['hashtags', 'emojis'])\n",
        "    models[lang] = results\n",
        "    # print(test_pred_1[:10])\n",
        "    # writeOutput(test_df, lang, test_pred_1, test_pred_2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvghASGcwOaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e86a5df-44cd-444b-f761-8cf3fba289ec"
      },
      "source": [
        "runAllLangs(seed=69)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "For the language: en \n",
            " #####################################################################\n",
            "******************** ALL: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 1 0 0 1 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.83       460\n",
            "           1       0.85      0.78      0.81       467\n",
            "\n",
            "    accuracy                           0.82       927\n",
            "   macro avg       0.82      0.82      0.82       927\n",
            "weighted avg       0.82      0.82      0.82       927\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82       460\n",
            "           1       0.32      0.14      0.19        50\n",
            "           2       0.72      0.73      0.72       332\n",
            "           3       0.31      0.22      0.26        85\n",
            "\n",
            "    accuracy                           0.72       927\n",
            "   macro avg       0.53      0.49      0.50       927\n",
            "weighted avg       0.69      0.72      0.70       927\n",
            "\n",
            "******************** HASH: ********************\n",
            "[0 0 0 0 0 1 0 0 1 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80       460\n",
            "           1       0.80      0.83      0.81       467\n",
            "\n",
            "    accuracy                           0.81       927\n",
            "   macro avg       0.81      0.81      0.81       927\n",
            "weighted avg       0.81      0.81      0.81       927\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.82       460\n",
            "           1       0.14      0.08      0.10        50\n",
            "           2       0.71      0.75      0.73       332\n",
            "           3       0.31      0.22      0.26        85\n",
            "\n",
            "    accuracy                           0.71       927\n",
            "   macro avg       0.49      0.47      0.48       927\n",
            "weighted avg       0.68      0.71      0.69       927\n",
            "\n",
            "******************** EMOJI: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 1 0 0 1 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.81       460\n",
            "           1       0.83      0.78      0.80       467\n",
            "\n",
            "    accuracy                           0.81       927\n",
            "   macro avg       0.81      0.81      0.81       927\n",
            "weighted avg       0.81      0.81      0.81       927\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       460\n",
            "           1       0.27      0.14      0.18        50\n",
            "           2       0.72      0.72      0.72       332\n",
            "           3       0.29      0.27      0.28        85\n",
            "\n",
            "    accuracy                           0.70       927\n",
            "   macro avg       0.52      0.49      0.50       927\n",
            "weighted avg       0.69      0.70      0.69       927\n",
            "\n",
            "******************** VANILLA: ********************\n",
            "[0 0 0 0 0 1 0 0 1 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.83       460\n",
            "           1       0.83      0.82      0.83       467\n",
            "\n",
            "    accuracy                           0.83       927\n",
            "   macro avg       0.83      0.83      0.83       927\n",
            "weighted avg       0.83      0.83      0.83       927\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.87      0.81       460\n",
            "           1       0.10      0.04      0.06        50\n",
            "           2       0.74      0.68      0.71       332\n",
            "           3       0.32      0.28      0.30        85\n",
            "\n",
            "    accuracy                           0.70       927\n",
            "   macro avg       0.48      0.47      0.47       927\n",
            "weighted avg       0.68      0.70      0.69       927\n",
            "\n",
            "\n",
            "\n",
            "For the language: ge \n",
            " #####################################################################\n",
            "******************** ALL: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 0 0 0 0 0 1]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       415\n",
            "           1       0.53      0.47      0.50       179\n",
            "\n",
            "    accuracy                           0.72       594\n",
            "   macro avg       0.66      0.65      0.65       594\n",
            "weighted avg       0.71      0.72      0.71       594\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       415\n",
            "           1       0.30      0.24      0.27        33\n",
            "           2       0.57      0.46      0.51       102\n",
            "           3       0.20      0.09      0.13        44\n",
            "\n",
            "    accuracy                           0.72       594\n",
            "   macro avg       0.46      0.42      0.43       594\n",
            "weighted avg       0.68      0.72      0.70       594\n",
            "\n",
            "******************** HASH: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 1 0 0 0 0 0 1]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81       415\n",
            "           1       0.55      0.50      0.52       179\n",
            "\n",
            "    accuracy                           0.73       594\n",
            "   macro avg       0.67      0.66      0.67       594\n",
            "weighted avg       0.72      0.73      0.72       594\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       415\n",
            "           1       0.24      0.21      0.23        33\n",
            "           2       0.57      0.47      0.52       102\n",
            "           3       0.25      0.11      0.16        44\n",
            "\n",
            "    accuracy                           0.72       594\n",
            "   macro avg       0.46      0.42      0.43       594\n",
            "weighted avg       0.68      0.72      0.69       594\n",
            "\n",
            "******************** EMOJI: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 0 0 0 0 0 1]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.82       415\n",
            "           1       0.57      0.49      0.53       179\n",
            "\n",
            "    accuracy                           0.74       594\n",
            "   macro avg       0.68      0.67      0.67       594\n",
            "weighted avg       0.73      0.74      0.73       594\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.87      0.83       415\n",
            "           1       0.27      0.21      0.24        33\n",
            "           2       0.55      0.49      0.52       102\n",
            "           3       0.22      0.11      0.15        44\n",
            "\n",
            "    accuracy                           0.71       594\n",
            "   macro avg       0.46      0.42      0.43       594\n",
            "weighted avg       0.68      0.71      0.69       594\n",
            "\n",
            "******************** VANILLA: ********************\n",
            "[0 0 1 1 0 0 0 0 0 1]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       415\n",
            "           1       0.60      0.53      0.56       179\n",
            "\n",
            "    accuracy                           0.75       594\n",
            "   macro avg       0.70      0.69      0.69       594\n",
            "weighted avg       0.74      0.75      0.75       594\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.85      0.81       415\n",
            "           1       0.18      0.12      0.15        33\n",
            "           2       0.50      0.50      0.50       102\n",
            "           3       0.16      0.07      0.10        44\n",
            "\n",
            "    accuracy                           0.69       594\n",
            "   macro avg       0.41      0.38      0.39       594\n",
            "weighted avg       0.65      0.69      0.67       594\n",
            "\n",
            "\n",
            "\n",
            "For the language: hi \n",
            " #####################################################################\n",
            "******************** ALL: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.95      0.84       548\n",
            "           1       0.41      0.11      0.17       193\n",
            "\n",
            "    accuracy                           0.73       741\n",
            "   macro avg       0.58      0.53      0.50       741\n",
            "weighted avg       0.66      0.73      0.66       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.97      0.84       548\n",
            "           1       0.00      0.00      0.00        51\n",
            "           2       0.00      0.00      0.00        34\n",
            "           3       0.20      0.02      0.03       108\n",
            "\n",
            "    accuracy                           0.72       741\n",
            "   macro avg       0.24      0.25      0.22       741\n",
            "weighted avg       0.58      0.72      0.63       741\n",
            "\n",
            "******************** HASH: ********************\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.90      0.82       548\n",
            "           1       0.31      0.12      0.18       193\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.53      0.51      0.50       741\n",
            "weighted avg       0.63      0.70      0.65       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.98      0.85       548\n",
            "           1       0.00      0.00      0.00        51\n",
            "           2       0.17      0.03      0.05        34\n",
            "           3       0.50      0.04      0.07       108\n",
            "\n",
            "    accuracy                           0.73       741\n",
            "   macro avg       0.35      0.26      0.24       741\n",
            "weighted avg       0.63      0.73      0.64       741\n",
            "\n",
            "******************** EMOJI: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.94      0.84       548\n",
            "           1       0.42      0.11      0.18       193\n",
            "\n",
            "    accuracy                           0.73       741\n",
            "   macro avg       0.58      0.53      0.51       741\n",
            "weighted avg       0.66      0.73      0.67       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.92      0.82       548\n",
            "           1       0.06      0.02      0.03        51\n",
            "           2       0.11      0.03      0.05        34\n",
            "           3       0.26      0.08      0.13       108\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.29      0.26      0.26       741\n",
            "weighted avg       0.60      0.70      0.63       741\n",
            "\n",
            "******************** VANILLA: ********************\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "Writing Task A\n",
            "Finished writing Task A\n",
            "Writing Task B\n",
            "Finished writing Task B\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.82       548\n",
            "           1       0.31      0.11      0.16       193\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.53      0.51      0.49       741\n",
            "weighted avg       0.63      0.70      0.65       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.97      0.84       548\n",
            "           1       0.00      0.00      0.00        51\n",
            "           2       0.00      0.00      0.00        34\n",
            "           3       0.20      0.02      0.03       108\n",
            "\n",
            "    accuracy                           0.72       741\n",
            "   macro avg       0.24      0.25      0.22       741\n",
            "weighted avg       0.58      0.72      0.63       741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CeYS2r3IYgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(OUTPUT_ROOT+'models.pkl', 'wb') as f:\n",
        "  pickle.dump(models, f)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uppPo3JfqZC",
        "colab_type": "text"
      },
      "source": [
        "# Generating test outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mN33uCYfsuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/HASOC/Data/2020_processed_test/en_test.pickle', 'rb') as f:\n",
        "  eed = pickle.load(f)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5loE07f4IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4184723f-258c-4751-dfe8-68c514fbc430"
      },
      "source": [
        "eed.keys()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tweet_id', 'task_1', 'task_2', 'hasoc_id', 'full_tweet', 'tweet_raw_text', 'hashtags', 'smiley', 'emoji', 'url', 'mentions', 'numerals', 'reserved_word', 'emotext', 'segmented_hash'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xbHuSEGf4XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(clf, df, hashtags=True, emojis=True, verbose=False):\n",
        "  pass"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLu7ir_f4lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6O8SAw0Itj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# runAllLangs(seed=42)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNLZGUrl0r8f",
        "colab_type": "text"
      },
      "source": [
        "## (Old experiment) Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdtrgDk18sNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_df, test_df, df = loadData('hi')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjQ1qlfM9CdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clf = trainModelWithFeatures(train_df,test_df,  verbose = False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47VtJ99RI2l9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d28f7f71-f57d-458f-df45-b268a7e3e5e3"
      },
      "source": [
        "r = performAblations(train_df, test_df,features = [\"hashtags\", \"emojis\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************** ALL: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.94      0.82       523\n",
            "           1       0.46      0.12      0.19       218\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.59      0.53      0.50       741\n",
            "weighted avg       0.64      0.70      0.63       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.98      0.83       523\n",
            "           1       0.00      0.00      0.00        54\n",
            "           2       0.50      0.02      0.04        45\n",
            "           3       0.32      0.06      0.10       119\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.38      0.26      0.24       741\n",
            "weighted avg       0.59      0.70      0.60       741\n",
            "\n",
            "******************** HASH: ********************\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.96      0.82       523\n",
            "           1       0.49      0.08      0.14       218\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.60      0.52      0.48       741\n",
            "weighted avg       0.65      0.70      0.62       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.96      0.82       523\n",
            "           1       0.00      0.00      0.00        54\n",
            "           2       0.00      0.00      0.00        45\n",
            "           3       0.35      0.07      0.11       119\n",
            "\n",
            "    accuracy                           0.69       741\n",
            "   macro avg       0.27      0.26      0.23       741\n",
            "weighted avg       0.56      0.69      0.60       741\n",
            "\n",
            "******************** EMOJI: ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.96      0.82       523\n",
            "           1       0.52      0.11      0.18       218\n",
            "\n",
            "    accuracy                           0.71       741\n",
            "   macro avg       0.62      0.53      0.50       741\n",
            "weighted avg       0.66      0.71      0.63       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.97      0.83       523\n",
            "           1       1.00      0.02      0.04        54\n",
            "           2       0.00      0.00      0.00        45\n",
            "           3       0.42      0.11      0.17       119\n",
            "\n",
            "    accuracy                           0.71       741\n",
            "   macro avg       0.54      0.28      0.26       741\n",
            "weighted avg       0.65      0.71      0.62       741\n",
            "\n",
            "******************** VANILLA: ********************\n",
            "-------------------- Task 1 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.94      0.81       523\n",
            "           1       0.42      0.11      0.17       218\n",
            "\n",
            "    accuracy                           0.69       741\n",
            "   macro avg       0.57      0.52      0.49       741\n",
            "weighted avg       0.63      0.69      0.62       741\n",
            "\n",
            "-------------------- Task 2 --------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.96      0.82       523\n",
            "           1       0.00      0.00      0.00        54\n",
            "           2       0.29      0.04      0.08        45\n",
            "           3       0.27      0.07      0.11       119\n",
            "\n",
            "    accuracy                           0.69       741\n",
            "   macro avg       0.32      0.27      0.25       741\n",
            "weighted avg       0.57      0.69      0.60       741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mDtiufnI7Gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8IJkLAPNHCV",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "Using grid-search with the MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvazzcrvQNmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmtcJlrXNO3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_param = {\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'hidden_layer_sizes': [(100,), (50,), (200,)],# (50, 50), (100, 100), (200, 200)],\n",
        "    'solver': ['adam'],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'random_state': [1,]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge6FU_CfQM3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search = GridSearchCV(\n",
        "    estimator = MLPClassifier(),\n",
        "    scoring = 'f1_macro',\n",
        "    param_grid = grid_param,\n",
        "    cv = 4,\n",
        "    n_jobs = 1,\n",
        "    verbose = 0\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf33rXLxRbJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
        "def runAllLangsGrid(grid_search, langs=['en', 'ge', 'hi'], seed=42):\n",
        "  for i in langs:\n",
        "    if i not in ['en', 'ge', 'hi']:\n",
        "      raise NameError(\"Wrong set of languages\")\n",
        "  \n",
        "  for lang in langs:\n",
        "    print(\"\\n\\nFor the language:\", lang, '\\n', '#'*69)\n",
        "    # tokenizer = selectTokenizer(lang=lang)\n",
        "    train_df, test_df, df = loadData(lang, seed=seed)\n",
        "    performGridSearch(df, grid_search, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-afKiUo0T6zg",
        "colab_type": "text"
      },
      "source": [
        "## Running experiment\n",
        "\n",
        "The function prepares the data, then runs the search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSK-ZMaGI8lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performGridSearch(df, grid_search, verbose = False):\n",
        "  \n",
        "  ### Preparing data ###\n",
        "  \n",
        "  # Get Segmented Hashtags from dataset\n",
        "  hash1 = list(df['segmented_hash'])\n",
        "  \n",
        "  # Segmented hashtags: List of strings --> Space-separated string\n",
        "  hash = []\n",
        "  for lis in hash1:\n",
        "    hash.append(' '.join(lis))\n",
        "  \n",
        "  # Get raw data from dataset\n",
        "  text = list(df['tweet_raw_text'])\n",
        "  t1s = list(df['task_1'])\n",
        "  t2s = list(df['task_2'])\n",
        "\n",
        "  # Get embeddings for the text\n",
        "  if verbose:\n",
        "    print(\"Started getting text embeddings\")\n",
        "  embeddings = sent_encoder.encode(text)\n",
        "\n",
        "  # Get usable feature representations\n",
        "  if verbose:\n",
        "    print(\"Finished loading up the text embeddings\")\n",
        "  t1 = get_task(t1s)\n",
        "  t2 = get_task(t2s, 2)\n",
        "\n",
        "  emb = get_features(embeddings)\n",
        "  \n",
        "  # Ablation: If hashtag\n",
        "  if verbose:\n",
        "    print(\"Started getting hash embeddings\")\n",
        "  hashembeddings = sent_encoder.encode(hash)\n",
        "  emb = np.concatenate((emb , hashembeddings), axis = 1)\n",
        "  \n",
        "  if verbose:\n",
        "    print(\"Finished loading up the hash embeddings\")\n",
        "\n",
        "  # Ablation: if emoji\n",
        "  if verbose:\n",
        "    print(\"Started getting emoji embeddings\")\n",
        "  emojiEmbs = np.asarray([getEmojiEmbeddings(i,verbose=verbose) for i in (list(df['emoji']))])\n",
        "  emb = np.concatenate((emb , emojiEmbs), axis = 1)\n",
        "  if verbose:\n",
        "    print(\"Finished loading up the emoji embeddings\")\n",
        "\n",
        "  print(\"-\"*20, \"Task 1\", \"-\"*20)\n",
        "  \n",
        "  grid_search.fit(emb, t1)\n",
        "  print(\"Best parameters: \", grid_search.best_params_)\n",
        "  print(\"Score: \", grid_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXvBW4-cJYjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72e2b9cd-f320-4bea-ad58-2a9a5b0698aa"
      },
      "source": [
        "runAllLangsGrid(grid_search)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "For the language: en \n",
            " #####################################################################\n",
            "Started getting text embeddings\n",
            "Finished loading up the text embeddings\n",
            "Started getting hash embeddings\n",
            "Finished loading up the hash embeddings\n",
            "Started getting emoji embeddings\n",
            "🏽\n",
            "🏽\n",
            "❤\n",
            "⚠\n",
            "⚠\n",
            "⚠\n",
            "⚠\n",
            "🏾\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "✖\n",
            "⚠\n",
            "🏽\n",
            "♀\n",
            "❤\n",
            "🏽\n",
            "🏽\n",
            "♀\n",
            "🏾\n",
            "🏻\n",
            "♂\n",
            "❤\n",
            "🏼\n",
            "♀\n",
            "🏻\n",
            "♀\n",
            "🏻\n",
            "♀\n",
            "🏼\n",
            "🏼\n",
            "🏼\n",
            "🏼\n",
            "☺\n",
            "🏽\n",
            "🏽\n",
            "♂\n",
            "⚠\n",
            "⚠\n",
            "❝\n",
            "❞\n",
            "♡\n",
            "♡\n",
            "♡\n",
            "➡\n",
            "❤\n",
            "🏽\n",
            "♀\n",
            "🏽\n",
            "♀\n",
            "🏽\n",
            "🏽\n",
            "♀\n",
            "🏽\n",
            "🏽\n",
            "🏽\n",
            "🏽\n",
            "🏽\n",
            "🏽\n",
            "🏻\n",
            "♂\n",
            "🏽\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "❤\n",
            "🏼\n",
            "🏻\n",
            "♂\n",
            "🏼\n",
            "❤\n",
            "❤\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏾\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "❤\n",
            "✌\n",
            "🏼\n",
            "☀\n",
            "🏾\n",
            "♂\n",
            "✌\n",
            "🏽\n",
            "🏽\n",
            "✔\n",
            "♀\n",
            "🏻\n",
            "♀\n",
            "♛\n",
            "🏾\n",
            "🏽\n",
            "♀\n",
            "☆\n",
            "☽\n",
            "☆\n",
            "❖\n",
            "❖\n",
            "❖\n",
            "🏽\n",
            "♂\n",
            "❤\n",
            "🏽\n",
            "🏽\n",
            "♂\n",
            "❤\n",
            "❤\n",
            "♥\n",
            "❤\n",
            "♥\n",
            "❤\n",
            "♥\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "🏻\n",
            "🏽\n",
            "♀\n",
            "🏽\n",
            "🏼\n",
            "🏼\n",
            "🏼\n",
            "🏻\n",
            "🏼\n",
            "🏼\n",
            "🏼\n",
            "🏾\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "🏻\n",
            "🏽\n",
            "❝\n",
            "❞\n",
            "🏻\n",
            "❤\n",
            "🏽\n",
            "♂\n",
            "🏾\n",
            "♀\n",
            "🏾\n",
            "🏻\n",
            "♂\n",
            "🏻\n",
            "♂\n",
            "🏻\n",
            "♂\n",
            "🏻\n",
            "♂\n",
            "🏻\n",
            "♂\n",
            "🏾\n",
            "♂\n",
            "♀\n",
            "✌\n",
            "🏽\n",
            "🏼\n",
            "🏾\n",
            "🏾\n",
            "⚘\n",
            "⚠\n",
            "☺\n",
            "❤\n",
            "♥\n",
            "🏾\n",
            "♀\n",
            "❄\n",
            "⚠\n",
            "⚠\n",
            "☺\n",
            "✾\n",
            "❤\n",
            "🏼\n",
            "✌\n",
            "🏽\n",
            "✌\n",
            "🏽\n",
            "✌\n",
            "🏽\n",
            "♡\n",
            "🏽\n",
            "♀\n",
            "✌\n",
            "🏻\n",
            "⚠\n",
            "⚠\n",
            "❤\n",
            "🏽\n",
            "🏾\n",
            "♂\n",
            "☝\n",
            "☝\n",
            "✔\n",
            "✔\n",
            "✔\n",
            "🏽\n",
            "♂\n",
            "Finished loading up the emoji embeddings\n",
            "-------------------- Task 1 --------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters:  {'activation': 'tanh', 'hidden_layer_sizes': (200,), 'learning_rate': 'constant', 'random_state': 1, 'solver': 'adam'}\n",
            "Score:  0.8235936763758844\n",
            "\n",
            "\n",
            "For the language: ge \n",
            " #####################################################################\n",
            "Started getting text embeddings\n",
            "Finished loading up the text embeddings\n",
            "Started getting hash embeddings\n",
            "Finished loading up the hash embeddings\n",
            "Started getting emoji embeddings\n",
            "♂\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "🏼\n",
            "♀\n",
            "♀\n",
            "➜\n",
            "♂\n",
            "❤\n",
            "☝\n",
            "☝\n",
            "🏻\n",
            "♂\n",
            "❤\n",
            "☝\n",
            "✌\n",
            "🏼\n",
            "❤\n",
            "🏻\n",
            "🛸\n",
            "🏽\n",
            "🏼\n",
            "🏼\n",
            "♀\n",
            "♀\n",
            "❤\n",
            "♀\n",
            "♂\n",
            "☝\n",
            "♂\n",
            "❤\n",
            "🏼\n",
            "🏼\n",
            "🏼\n",
            "❤\n",
            "✜\n",
            "☝\n",
            "🏼\n",
            "❤\n",
            "♂\n",
            "🏻\n",
            "🏿\n",
            "♀\n",
            "♀\n",
            "🏼\n",
            "♂\n",
            "♂\n",
            "❤\n",
            "Finished loading up the emoji embeddings\n",
            "-------------------- Task 1 --------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters:  {'activation': 'tanh', 'hidden_layer_sizes': (200,), 'learning_rate': 'constant', 'random_state': 1, 'solver': 'adam'}\n",
            "Score:  0.7138892079758792\n",
            "\n",
            "\n",
            "For the language: hi \n",
            " #####################################################################\n",
            "Started getting text embeddings\n",
            "Finished loading up the text embeddings\n",
            "Started getting hash embeddings\n",
            "Finished loading up the hash embeddings\n",
            "Started getting emoji embeddings\n",
            "♥\n",
            "✌\n",
            "❤\n",
            "♥\n",
            "♥\n",
            "🏼\n",
            "⛵\n",
            "✌\n",
            "✌\n",
            "❤\n",
            "❤\n",
            "♥\n",
            "✌\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "♂\n",
            "☺\n",
            "☝\n",
            "🏻\n",
            "☝\n",
            "🏻\n",
            "☝\n",
            "🏻\n",
            "🏻\n",
            "✌\n",
            "🏻\n",
            "✌\n",
            "🏻\n",
            "✌\n",
            "🏻\n",
            "✌\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "❤\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "✌\n",
            "➡\n",
            "➡\n",
            "✴\n",
            "✴\n",
            "✔\n",
            "✔\n",
            "✔\n",
            "✔\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "❤\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "☺\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "🏻\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "❤\n",
            "♥\n",
            "✔\n",
            "✔\n",
            "✔\n",
            "♥\n",
            "♦\n",
            "♦\n",
            "❤\n",
            "🏻\n",
            "🏻\n",
            "🏻\n",
            "❤\n",
            "❤\n",
            "🏼\n",
            "🏼\n",
            "✌\n",
            "✌\n",
            "✌\n",
            "✌\n",
            "✌\n",
            "✌\n",
            "❤\n",
            "♂\n",
            "♂\n",
            "♂\n",
            "✒\n",
            "Finished loading up the emoji embeddings\n",
            "-------------------- Task 1 --------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters:  {'activation': 'relu', 'hidden_layer_sizes': (200,), 'learning_rate': 'constant', 'random_state': 1, 'solver': 'adam'}\n",
            "Score:  0.5150191289087307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbVs6Hx7Jcc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr78Lk7-08Hc",
        "colab_type": "text"
      },
      "source": [
        "# Pure BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxzI760g1lQd",
        "colab_type": "text"
      },
      "source": [
        "## Loading Dataset\n",
        "\n",
        "Loading the dataset into a dataframe, then transforming the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAcmEjoxrx4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_PATH = '/content/drive/My Drive/HASOC/Data/2020_train_sets/hasoc_2020_de_train.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loxX_2glrxqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3a51569e-347d-4e65-99c4-8e85dccc84be"
      },
      "source": [
        "df = pd.read_csv(DATASET_PATH)\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,452\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task1</th>\n",
              "      <th>task2</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>1128229574318407680</td>\n",
              "      <td>@BoserEr @SebastianHampel @DrDavidBerger Was i...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_2811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>1133029984573054976</td>\n",
              "      <td>RT @PlatonsTochter: Baden-Württemberg: Ehemali...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>1133427357140836352</td>\n",
              "      <td>Kostenlose dicke deutsche fisten. Geile nachba...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_de_3091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>1133715547777052672</td>\n",
              "      <td>@panemetc62 @ZDF @ZDFneo bei Dokus ist da ZDF ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>1131616638342782976</td>\n",
              "      <td>Merkel Doktorarbeit: Ein Fake? | MMnews https:...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_2579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2133</th>\n",
              "      <td>1132000001926848512</td>\n",
              "      <td>@Ruebenhorst Diese Islamisten erscheinen irgen...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>hasoc_2020_de_884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>1131515152958918656</td>\n",
              "      <td>RT @D4ncingMonkey: #Amthor Leben und  Tod des ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_1896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>1132701889357914112</td>\n",
              "      <td>@EhrenmannErich der deutsche kocht sein chilli...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_1894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095</th>\n",
              "      <td>1130044562884444160</td>\n",
              "      <td>@vickersHV10 @den_tyske Was ist los mit uns #E...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_1596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>1131518185474285572</td>\n",
              "      <td>@milchmitzucker sorry aber sie ist auch einfac...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_de_1847</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 tweet_id  ...                  ID\n",
              "200   1128229574318407680  ...  hasoc_2020_de_2811\n",
              "1033  1133029984573054976  ...   hasoc_2020_de_710\n",
              "1889  1133427357140836352  ...  hasoc_2020_de_3091\n",
              "1307  1133715547777052672  ...   hasoc_2020_de_531\n",
              "143   1131616638342782976  ...  hasoc_2020_de_2579\n",
              "2133  1132000001926848512  ...   hasoc_2020_de_884\n",
              "305   1131515152958918656  ...  hasoc_2020_de_1896\n",
              "1478  1132701889357914112  ...  hasoc_2020_de_1894\n",
              "1095  1130044562884444160  ...  hasoc_2020_de_1596\n",
              "1111  1131518185474285572  ...  hasoc_2020_de_1847\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo60AHM91IeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4b9ec8a5-1e38-425f-8f7c-5f3c64dc693e"
      },
      "source": [
        "LE = LabelEncoder()\n",
        "df['task1'] = LE.fit_transform(df['task1'])\n",
        "df['task2'] = LE.fit_transform(df['task2'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task1</th>\n",
              "      <th>task2</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1133388798925189122</td>\n",
              "      <td>Deutsche rothaarige porno reife deutsche fraue...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>hasoc_2020_de_2684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1131117000279961600</td>\n",
              "      <td>Lehrstück auch, wie in der linken Jammerfemini...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>hasoc_2020_de_2440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1127134592517980161</td>\n",
              "      <td>RT @NDRinfo: Die deutsche Klimaaktivistin Luis...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>hasoc_2020_de_1042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1128897106171842560</td>\n",
              "      <td>@ruhrbahn jeden Morgen eine neue „Fahrzeugstör...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>hasoc_2020_de_774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1123576753199484928</td>\n",
              "      <td>@Junge_Freiheit Die Inkas hatten sich schon dä...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>hasoc_2020_de_559</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ...                  ID\n",
              "0  1133388798925189122  ...  hasoc_2020_de_2684\n",
              "1  1131117000279961600  ...  hasoc_2020_de_2440\n",
              "2  1127134592517980161  ...  hasoc_2020_de_1042\n",
              "3  1128897106171842560  ...   hasoc_2020_de_774\n",
              "4  1123576753199484928  ...   hasoc_2020_de_559\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHrWazNo1LoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a0e804b-4b8f-40ee-9a76-6b695c3ace84"
      },
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "df.text.apply(count_words).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ULzJBy1Paa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 74"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nCg277B18qw",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the Dataset\n",
        "\n",
        "And then extracting the posts and tasks from that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60tEuNZX1NuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(ged['tweet_raw_text'], get_task(ged['task_1']), test_size=0.2)\n",
        "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['text'], df['task1'], test_size=0.2, stratify=df['task1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPE59dv1QvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# posts = train_x.values\n",
        "# categories = train_y.values\n",
        "posts = train_x\n",
        "categories = train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu-_UGz81Szr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH2VP_v-3D9r",
        "colab_type": "text"
      },
      "source": [
        "## Encoding the Data\n",
        "\n",
        "Into BERT-type preprocessed things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g6GFCCa1Tse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "3f4844a265fb46b2ac7bc3495203ed4b",
            "6f7dfb4629c240b69132e4d5aa7c332d",
            "09abbce550634a45ae3b891e030a12f8",
            "d9bcd80d126d4ac2a031324d79b8a1fb",
            "27edd6a1538c46f0aef7c69899acbfbe",
            "af9f8a23eeee4f6cb9e9cbee8081a627",
            "0f50cda79879470a834193c725c928e2",
            "8c22825422fd4fa8acf4a8f3ddae1e0a"
          ]
        },
        "outputId": "d30a9a4f-135d-4f56-f7d7-0166c028f9ff"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased', do_lower_case=True)\n",
        "\n",
        "# For every sentence...\n",
        "for sent in posts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
        "                        truncation=True,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f4844a265fb46b2ac7bc3495203ed4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=247333.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJNv-M4V3NhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b1e93a03-796e-4fa9-bb3f-43e908a8c63b"
      },
      "source": [
        "print('Original: ', posts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Lass uns einfach mal Dopamin vergeuden\n",
            "Token IDs: tensor([  102,  4734,   704,  1533,   774, 17825,  6233,  5272,   204,   166,\n",
            "          103,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Wc-M043Nrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6c97b576-f028-4085-be1d-6efe437c46e5"
      },
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.875 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,660 training samples\n",
            "  238 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYrb2Ypi3NpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcBb8t383Nnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "36b8eeea93fa4d74ace74bf9eb501cae",
            "9f39e5acab674eb191a1ad690312c6bb",
            "110e6e1655e941b28c92de002ea7a947",
            "41c181fc244543f29affc0e1611b9f15",
            "80bc1b33433f4472b34e917c1cf00c47",
            "76d0acb3ba3647a081a6915c51f1204b",
            "5cc684216fcc4265ae0d62b4a9e94b51",
            "ea55906d911f473d9b27f1dac6f46ac3",
            "8b6eece69dfc44d78b830223fd4f1fe3",
            "94b44bdb8dd342b0bc2b15d112c368f2",
            "3bd978faf6604530b16f52ac1386cc96",
            "ad755932caec4926b6d7623eca8ba814",
            "29a19cbabb0c4e8e86ee941e327104cc",
            "6efc8f6cbd084b7b8267c33131b14818",
            "f0eb60a3ff694e46910544ed880512f9",
            "adcf12e5ffc149bea974f804368f8d7d"
          ]
        },
        "outputId": "000c00b3-26e2-427b-9c4c-6cbbc44d3fa3"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-german-dbmdz-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36b8eeea93fa4d74ace74bf9eb501cae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b6eece69dfc44d78b830223fd4f1fe3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442256365.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-german-dbmdz-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-dbmdz-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31102, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DC_gcsJ3NlR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "dca3793c-4248-4feb-a8d6-b3d4efc889a8"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (31102, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (4, 768)\n",
            "classifier.bias                                                 (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiLVh19n9JFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DD-3nVZ9JDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIVGWrc19JBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HovaG5-9I-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdSKj6Jo9I5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "4d4e34b8-e530-45af-f0e9-9d0ea0126f4c"
      },
      "source": [
        "seed_val = 42\n",
        "torch.cuda.empty_cache()\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    104.    Elapsed: 0:00:24.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    104.    Elapsed: 0:00:24.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    104.    Elapsed: 0:00:24.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epoch took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.38\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:36 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BmNsEOq9Unz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "5e9a6b40-5aeb-43b1-8954-c82edda8a256"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:00:31</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:00:31</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:00:31</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.55         0.39           0.86       0:00:31         0:00:01\n",
              "2               0.33         0.35           0.86       0:00:31         0:00:01\n",
              "3               0.23         0.38           0.86       0:00:31         0:00:01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XzoDddQ9Uj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "9e49f1d4-3c8d-433b-97bc-5c0c16aef24b"
      },
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUQPpBsf9Ug_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# posts = valid_x.values\n",
        "# categories = valid_y.values\n",
        "posts = valid_x\n",
        "categories = valid_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UmpeSGX9Udu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cd8b1e53-a63a-4526-9cc0-32c5f2ec8d71"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in posts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(categories)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJbqr33a9UZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "57d1f078-1372-4ea5-9291-1612c07d9f56"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 475 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H6pRi5K9fag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a350f1cf-00a7-4159-8668-25a1feaf7c3f"
      },
      "source": [
        "print(predictions[0],true_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.6323337   3.12274    -2.5567174  -2.9750652 ]\n",
            " [ 4.073063    0.55542827 -2.7315142  -2.840895  ]\n",
            " [-0.58148086  3.386612   -1.9880607  -2.290066  ]\n",
            " [-0.0607793   3.573624   -2.3927078  -2.639462  ]\n",
            " [ 4.159212   -0.33992413 -2.0751953  -2.034514  ]\n",
            " [-0.27654496  3.2491677  -2.220152   -2.4454396 ]\n",
            " [ 3.7280843   0.2307884  -2.5400493  -2.4072416 ]\n",
            " [-0.62693834  3.4375453  -2.0344937  -2.5686939 ]\n",
            " [ 2.6987808   1.9011959  -2.739314   -3.0364413 ]\n",
            " [ 3.247344    1.3262775  -2.8440442  -2.8480377 ]\n",
            " [ 3.5067365  -0.5013843  -1.812414   -1.8513571 ]\n",
            " [ 4.1196136   0.8659061  -2.718529   -2.7948933 ]\n",
            " [ 2.2716424   1.6258057  -2.880927   -2.9757748 ]\n",
            " [ 3.7171805  -0.1532326  -2.4594858  -2.371483  ]\n",
            " [ 2.258548    1.7691505  -2.7650666  -2.7793112 ]\n",
            " [ 4.1161647   0.20726839 -2.4591339  -2.6009326 ]] [1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph-g-1py9g2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3cdc0f69-3893-4501-8c01-9a6047be70cc"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "predicts = []\n",
        "accurate = 0\n",
        "total_len = 0\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "    predicts.append(pred_labels_i)\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "\n",
        "    matthews_set.append(matthews)\n",
        "    for j in range(len(true_labels[i])):\n",
        "        if true_labels[i][j] == pred_labels_i[j]:\n",
        "            accurate+=1\n",
        "        total_len+=1\n",
        "print(\"Accuracy:\",accurate/total_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n",
            "Accuracy: 0.8610526315789474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW6oFj1N9h_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "1b1e1fdc-3997-452f-d4a5-6d5eff8baec4"
      },
      "source": [
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fh7Aa1n9jUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cde5609d-8545-49a6-fc3b-5b2e99b50856"
      },
      "source": [
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atBWPnpv9kgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "953645e9-95d8-42af-b6a3-9cc680b189e0"
      },
      "source": [
        "accurate = 0\n",
        "for (i,j) in zip(flat_predictions, flat_true_labels):\n",
        "    if i==j:\n",
        "        accurate += 1\n",
        "accurate/len(flat_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8610526315789474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKHaUWhL9lpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21656757-ac6b-42e2-ecfa-14d27d806aa2"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(flat_true_labels, flat_predictions, average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8284785748675976"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7yuX5QviNxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}