{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN56TeSpn1nZ9OhsBKdO9oL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/dataset_creation/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SzZYXq3ER6b"
      },
      "source": [
        "import pandas as pd\n",
        "import xlrd\n",
        "import re\n",
        "import pickle\n",
        "import csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOqUHRECEX9o",
        "outputId": "4ec4770f-ea54-4ee0-e3a4-2661629c52ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "# to leverage word statistics from Twitter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.18.5)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (5.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvfnKs-dEZwN",
        "outputId": "06a34c8d-e6e1-4842-8f9f-dca10138b77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk2BUZTuEdN1",
        "outputId": "86fc9ba1-de39-4992-a083-9a760455c35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "!pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emot in /usr/local/lib/python3.6/dist-packages (2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqibDmdYEjIK"
      },
      "source": [
        "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRsC-wmEhdx",
        "outputId": "dc25dd60-4bb2-40a9-a8a4-6d367cf96c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD2Ksnv6EmaS"
      },
      "source": [
        "def make_list(proc_obj):\n",
        "  if proc_obj == None:\n",
        "    return []\n",
        "  \n",
        "  store = []\n",
        "  for unit in proc_obj:\n",
        "    store.append(unit.match)\n",
        "  \n",
        "  return store\n",
        "\n",
        "def emotext(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI524oqVE026"
      },
      "source": [
        "# For 2020 Datasets\n",
        "\n",
        "is_hindi = 0\n",
        "\n",
        "# For Train Data\n",
        "# datatype = \"train\"\n",
        "# For English\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
        "\n",
        "# For Hindi\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
        "# is_hindi = 1\n",
        "\n",
        "# For German\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\"\n",
        "\n",
        "# For Test Data\n",
        "datatype = \"test\"\n",
        "# For English\n",
        "file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/english_test_1509.csv\"\n",
        "\n",
        "# For Hindi\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/hindi_test_1509.csv\"\n",
        "# is_hindi = 1\n",
        "\n",
        "# For German\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/german_test_1509.csv\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNoGyv8E5th"
      },
      "source": [
        "# Initializing Lists\n",
        "datapoints_count = 0\n",
        "see_index = True\n",
        "\n",
        "tweets = []\n",
        "raw_tweet_texts = []\n",
        "tokenized_tweets = []\n",
        "hashtags = []\n",
        "smileys = []\n",
        "emojis = []\n",
        "urls = []\n",
        "mentions = []\n",
        "numbers = []\n",
        "reserveds = []\n",
        "\n",
        "task_1_labels = []\n",
        "task_2_labels = []\n",
        "tweet_ids = []\n",
        "hasoc_ID = []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsKZHtQ-FvOw"
      },
      "source": [
        "def strip_list(listie):\n",
        "  stripped = []\n",
        "  for item in listie:\n",
        "    stripped.append(item.strip())\n",
        "  return stripped\n",
        "\n",
        "def hindi_clean(line, parse_obj):\n",
        "  # beta\n",
        "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
        "  valid_stri = \"\"\n",
        "\n",
        "  for raw_token in tokens:\n",
        "    token = raw_token.strip()\n",
        "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.smileys)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.emojis)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.urls)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.mentions)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.numbers)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.reserved)):\n",
        "      continue\n",
        "    valid_stri = valid_stri + \" \" + token\n",
        "  return valid_stri.strip()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TuJvskdE95H"
      },
      "source": [
        "if datatype == 'train':\n",
        "    workbook = xlrd.open_workbook(file_name)\n",
        "    sheet = workbook.sheet_by_index(0)\n",
        "\n",
        "    for row in range(sheet.nrows):\n",
        "        line = sheet.row_values(row)\n",
        "\n",
        "    file = open(file_name, 'r')\n",
        "    file_reader = csv.reader(file, delimiter = \",\")\n",
        "    for line in file_reader:\n",
        "        if see_index == True:\n",
        "            see_index = False\n",
        "            continue\n",
        "\n",
        "        datapoints_count += 1\n",
        "        tweet_ids.append(line[0])\n",
        "        task_1_labels.append(line[2])\n",
        "        task_2_labels.append(line[3])\n",
        "        hasoc_ID.append(line[4])\n",
        "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
        "\n",
        "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
        "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
        "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
        "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
        "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
        "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
        "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
        "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
        "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
        "\n",
        "        if is_hindi == 0:\n",
        "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
        "        else:\n",
        "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
        "\n",
        "    print(\"Number of Datapoints: \" + str(datapoints_count))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gtRGkIkNmIv",
        "outputId": "8a0715aa-55a2-459c-ceed-94666fe0c387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if datatype == 'test':\n",
        "    file = open(file_name, 'r')\n",
        "    file_reader = csv.reader(file, delimiter = \",\")\n",
        "    for line in file_reader:\n",
        "        if see_index == True:\n",
        "            see_index = False\n",
        "            continue\n",
        "\n",
        "        datapoints_count += 1\n",
        "        tweet_ids.append(line[0])\n",
        "        task_1_labels.append(line[2])\n",
        "        task_2_labels.append(line[3])\n",
        "        hasoc_ID.append(line[4])\n",
        "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
        "\n",
        "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
        "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
        "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
        "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
        "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
        "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
        "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
        "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
        "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
        "\n",
        "        if is_hindi == 0:\n",
        "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
        "        else:\n",
        "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
        "\n",
        "    print(\"Number of Datapoints: \" + str(datapoints_count))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Datapoints: 814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd_fy5REFxUX",
        "outputId": "f70d9fb9-2246-4c8d-f59e-5574161390b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "# Viewing Created Dataset\n",
        "display_size = 10\n",
        "start = 100\n",
        "\n",
        "print(\"Tweets:\")\n",
        "print(tweets[start: start + display_size])\n",
        "\n",
        "print(\"Raw Texts:\")\n",
        "print(raw_tweet_texts[start: start + display_size])\n",
        "\n",
        "print(\"Hashtags:\")\n",
        "print(hashtags[start: start + display_size])\n",
        "\n",
        "print(\"Smileys:\")\n",
        "print(smileys[start: start + display_size])\n",
        "\n",
        "print(\"Emojis:\")\n",
        "print(emojis[start: start + display_size])\n",
        "\n",
        "print(\"Urls:\")\n",
        "print(urls[start: start + display_size])\n",
        "\n",
        "print(\"Mentions:\")\n",
        "print(mentions[start: start + display_size])\n",
        "\n",
        "print(\"Numbers:\")\n",
        "print(numbers[start: start + display_size])\n",
        "\n",
        "print(\"Reserved Words:\")\n",
        "print(reserveds[start: start + display_size])\n",
        "\n",
        "print(\"Task Labels:\")\n",
        "print(task_1_labels[start: start + display_size])\n",
        "print(task_2_labels[start: start + display_size])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweets:\n",
            "['RT @Samuel_Verson1: @CCSantini fucking her girlfriend @TrinityStClair @ShemaleSurfer2 @adultparody @jockosrocket @TGirlsAddict @MeninaaSafa…', 'I think I summoned Mr. Usinger today because I yelled “retard!” in the tunnel and abracadabra bada bing bada boom t… https://t.co/VNv1mHT1T4', '12) bigbang: tried to stan a few times, glad I didn’t lol! top is hot, gd is an icon, and some of their songs are g… https://t.co/P4dmG1jjsY', '@fairiecult the oomf rting racist ggs', 'You be playin= I’m tryna fuck', 'RT @ghostlyminded: im tired of white people saying you can be racist towards them pls', 'RT @targarcyn: this is so fucking corny how did anyone think this was good #gameofthrones', 'RT @dankruptdev: PSA for all my ladies that like wandering at festivals if you happen to run into my group at edc this year and you’re on y…', 'RT @jeonggukpics: Don’t disturb please, he is enjoying his snacks while making those little dance 😭😂😂😭💜  #BBMAsTopSocial BTS #JUNGKOOK #정국…', 'RT @polarbearyoongi: me : [was mad at bbmas]   yoongi : [giggles]  me : u know what ? everything is fucking perfect and amazing. Love that…']\n",
            "Raw Texts:\n",
            "[': fucking her girlfriend', 'I think I summoned Mr. Usinger today because I yelled retard! in the tunnel and abracadabra bada bing bada boom t', ') bigbang: tried to stan a few times, glad I didnt lol! top is hot, gd is an icon, and some of their songs are g', 'the oomf rting racist ggs', 'You be playin= Im tryna fuck', ': im tired of white people saying you can be racist towards them pls', ': this is so fucking corny how did anyone think this was good', ': PSA for all my ladies that like wandering at festivals if you happen to run into my group at edc this year and youre on y', ': Dont disturb please, he is enjoying his snacks while making those little dance BTS', ': me : [was mad at bbmas] yoongi : [giggles] me : u know what ? everything is fucking perfect and amazing. Love that']\n",
            "Hashtags:\n",
            "[[], [], [], [], [], [], ['#gameofthrones'], [], ['#BBMAsTopSocial', '#JUNGKOOK', '#정국'], []]\n",
            "Smileys:\n",
            "[[], [], [], [], [], [], [], [], [], []]\n",
            "Emojis:\n",
            "[[], [], [], [], [], [], [], [], ['😭', '😂', '😂', '😭', '💜'], []]\n",
            "Urls:\n",
            "[[], ['https://t.co/VNv1mHT1T4'], ['https://t.co/P4dmG1jjsY'], [], [], [], [], [], [], []]\n",
            "Mentions:\n",
            "[['@Samuel_Verson1', '@CCSantini', '@TrinityStClair', '@ShemaleSurfer2', '@adultparody', '@jockosrocket', '@TGirlsAddict', '@MeninaaSafa'], [], [], ['@fairiecult'], [], ['@ghostlyminded'], ['@targarcyn'], ['@dankruptdev'], ['@jeonggukpics'], ['@polarbearyoongi']]\n",
            "Numbers:\n",
            "[[], [], ['12'], [], [], [], [], [], [], []]\n",
            "Reserved Words:\n",
            "[['RT'], [], [], [], [], ['RT'], ['RT'], ['RT'], ['RT'], ['RT']]\n",
            "Task Labels:\n",
            "['NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF']\n",
            "['PRFN', 'NONE', 'NONE', 'OFFN', 'PRFN', 'HATE', 'PRFN', 'NONE', 'NONE', 'PRFN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6sOeB9UWE7p"
      },
      "source": [
        "### Example\n",
        "#### Raw Tweet Text\n",
        "'RT @jeonggukpics: Don’t disturb please, he is enjoying his snacks while making those little dance 😭😂😂😭💜  #BBMAsTopSocial BTS #JUNGKOOK #정국…'\n",
        "#### Clean Text\n",
        "': Dont disturb please, he is enjoying his snacks while making those little dance BTS'\n",
        "\n",
        "#### Emojis\n",
        "'['😭', '😂', '😂', '😭', '💜']'\n",
        "\n",
        "#### Hashtags\n",
        "''#BBMAsTopSocial', '#JUNGKOOK', '#정국''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUrX_FPcIVQR",
        "outputId": "74d61f00-c87e-48d7-98a2-2ee5d8dc3633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Generating Emoji Texts\n",
        "emoji_texts = []\n",
        "\n",
        "for emo_list in emojis:\n",
        "  texts = []\n",
        "  for emoji in emo_list:\n",
        "    text = emotext(emoji)\n",
        "    texts.append(text.replace(\"_\", \" \"))\n",
        "  emoji_texts.append(texts)\n",
        "\n",
        "print(\"Emoji Descriptions:\")\n",
        "print(emoji_texts[0: 5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emoji Descriptions:\n",
            "[[], [], [], [], ['smiling face with halo']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWvCf65IXQT",
        "outputId": "93f0c3df-ccb5-415e-c26b-a743d2157c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Segmenting Hashtags\n",
        "segmented_hashtags = []\n",
        "\n",
        "for hashset in hashtags:\n",
        "  segmented_set = []\n",
        "  for tag in hashset:\n",
        "    word = tag[1: ]\n",
        "    # removing the hash symbol\n",
        "    segmented_set.append(seg_tw.segment(word))\n",
        "  segmented_hashtags.append(segmented_set)\n",
        "\n",
        "print(\"Segmented Hashtags:\")\n",
        "print(segmented_hashtags[0: 5])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segmented Hashtags:\n",
            "[[], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGPepLXoJsqR"
      },
      "source": [
        "name = 'ge_test.pickle'\n",
        "dickie = {}\n",
        "dickie['tweet_id'] = tweet_ids\n",
        "dickie['task_1'] = task_1_labels\n",
        "dickie['task_2'] = task_2_labels\n",
        "dickie['hasoc_id'] = hasoc_ID\n",
        "dickie['full_tweet'] = tweets\n",
        "dickie['tweet_raw_text'] = raw_tweet_texts\n",
        "dickie['hashtags'] = hashtags\n",
        "dickie['smiley'] = smileys\n",
        "dickie['emoji'] = emojis\n",
        "dickie['url'] = urls\n",
        "dickie['mentions'] = mentions\n",
        "dickie['numerals'] = numbers\n",
        "dickie['reserved_word'] = reserveds\n",
        "dickie['emotext'] = emoji_texts\n",
        "dickie['segmented_hash'] = segmented_hashtags\n",
        "\n",
        "with open(name, 'wb') as f:\n",
        "  pickle.dump(dickie, f)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK4uO1J1LeIC",
        "outputId": "ec2a5973-5576-43de-93ec-db36098869df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open(name, 'rb') as f:\n",
        "  try_dict = pickle.load(f)\n",
        "\n",
        "sizes = []\n",
        "for key in try_dict.keys():\n",
        "  sizes.append(len(try_dict[key]))\n",
        "\n",
        "# Verifying if all sizes are equal\n",
        "print(sizes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[814, 814, 814, 814, 814, 814, 814, 814, 814, 814, 814, 814, 814, 814, 814]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUMYDP9IaXe"
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}