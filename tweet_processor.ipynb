{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpDT2LHM+ExLNxHjy6NJ+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SzZYXq3ER6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import xlrd\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOqUHRECEX9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f5c80521-b231-4f02-8543-bbac88a0526a"
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "# to leverage word statistics from Twitter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvfnKs-dEZwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk2BUZTuEdN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqibDmdYEjIK",
        "colab_type": "text"
      },
      "source": [
        "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRsC-wmEhdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2398f3b4-0f78-4835-f24c-3f2c72b697cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD2Ksnv6EmaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_list(proc_obj):\n",
        "  if proc_obj == None:\n",
        "    return []\n",
        "  \n",
        "  store = []\n",
        "  for unit in proc_obj:\n",
        "    store.append(unit.match)\n",
        "  \n",
        "  return store\n",
        "\n",
        "def emotext(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI524oqVE026",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For 2020 Datasets\n",
        "\n",
        "is_hindi = 0\n",
        "\n",
        "# For English\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
        "\n",
        "# For Hindi\n",
        "file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
        "is_hindi = 1\n",
        "\n",
        "# For German\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNoGyv8E5th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing Lists\n",
        "datapoints_count = 0\n",
        "see_index = True\n",
        "\n",
        "tweets = []\n",
        "raw_tweet_texts = []\n",
        "tokenized_tweets = []\n",
        "hashtags = []\n",
        "smileys = []\n",
        "emojis = []\n",
        "urls = []\n",
        "mentions = []\n",
        "numbers = []\n",
        "reserveds = []\n",
        "\n",
        "task_1_labels = []\n",
        "task_2_labels = []\n",
        "tweet_ids = []\n",
        "hasoc_ID = []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsKZHtQ-FvOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_list(listie):\n",
        "  stripped = []\n",
        "  for item in listie:\n",
        "    stripped.append(item.strip())\n",
        "  return stripped\n",
        "\n",
        "def hindi_clean(line, parse_obj):\n",
        "  # beta\n",
        "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
        "  valid_stri = \"\"\n",
        "\n",
        "  for raw_token in tokens:\n",
        "    token = raw_token.strip()\n",
        "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.smileys)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.emojis)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.urls)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.mentions)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.numbers)):\n",
        "      continue\n",
        "    if token in strip_list(make_list(parse_obj.reserved)):\n",
        "      continue\n",
        "    valid_stri = valid_stri + \" \" + token\n",
        "  return valid_stri.strip()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TuJvskdE95H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d5a88a9-66ea-47f5-c79e-f70453466bed"
      },
      "source": [
        "workbook = xlrd.open_workbook(file_name)\n",
        "sheet = workbook.sheet_by_index(0)\n",
        "\n",
        "count = 0\n",
        "for row in range(sheet.nrows):\n",
        "    line = sheet.row_values(row)\n",
        "\n",
        "    if see_index == True:\n",
        "        see_index = False\n",
        "        continue\n",
        "\n",
        "    datapoints_count += 1\n",
        "    tweet_ids.append(line[0])\n",
        "    task_1_labels.append(line[2])\n",
        "    task_2_labels.append(line[3])\n",
        "    hasoc_ID.append(line[4])\n",
        "    tweets.append(line[1].replace(\"\\n\", \" \"))\n",
        "\n",
        "    parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
        "    tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
        "    hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
        "    smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
        "    emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
        "    urls.append(strip_list(make_list(parse_obj.urls)))\n",
        "    mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
        "    numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
        "    reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
        "\n",
        "    if is_hindi == 0:\n",
        "      raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
        "    else:\n",
        "      raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
        "\n",
        "print(\"Number of Datapoints: \" + str(datapoints_count))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Datapoints: 2963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd_fy5REFxUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "77d21b6a-cb51-4988-dbcb-6475083dabc3"
      },
      "source": [
        "# Viewing Created Dataset\n",
        "\n",
        "print(\"Tweets:\")\n",
        "print(tweets[0: 5])\n",
        "\n",
        "print(\"Raw Texts:\")\n",
        "print(raw_tweet_texts[0: 5])\n",
        "\n",
        "print(\"Hashtags:\")\n",
        "print(hashtags[0: 5])\n",
        "\n",
        "print(\"Smileys:\")\n",
        "print(smileys[0: 5])\n",
        "\n",
        "print(\"Emojis:\")\n",
        "print(emojis[0: 5])\n",
        "\n",
        "print(\"Urls:\")\n",
        "print(urls[0: 5])\n",
        "\n",
        "print(\"Mentions:\")\n",
        "print(mentions[0: 5])\n",
        "\n",
        "print(\"Numbers:\")\n",
        "print(numbers[0: 5])\n",
        "\n",
        "print(\"Reserved Words:\")\n",
        "print(reserveds[0: 5])\n",
        "\n",
        "print(\"Task Labels:\")\n",
        "print(task_1_labels[0: 5])\n",
        "print(task_2_labels[0: 5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweets:\n",
            "['1 आदमीं को मारने पर गोडसे आतंकी हो सके है तो 17000 सिखो, 5000 भोपाली, 3000 तमिलों का कत्लेआम करवाने वाला भारत रत्न कैसे हो सके है?', 'RT @Vishesh4: @jawaharyadavbjp जवाहर यादव, अगर हिम्मत है तो पूरा वीडियो डालो फिर तुम्हारे झूठ सामने आएंगे। दीपेंदर ने साफ कहा, कि भाजपा और…', 'RT @FunKeyBaat: #भगवा वस्त्र पहन कर मतदान नही कर सकते लेकिन बुरखा पहन के कर सकते हैं @yadavakhilesh ?  ये आज़मगढ़ है और यहाँ से \"टोटी चोर\"…', 'Yey nina khothani labafazi benu phambili Finish and klaar 🍑🌊', \"RT @Rajeshbhanjan2: जब भी कोई सिकुलर कोंग्रेसी गुलाम चाटुकार चरणचाट चमचा 'गाय' और 'बछड़े' को लेकर भाजपा पर तंज कसता हुआ दिखे तो इतिहास के झ…\"]\n",
            "Raw Texts:\n",
            "['आदमीं को मारने पर गोडसे आतंकी हो सके है तो सिखो ,  भोपाली ,  तमिलों का कत्लेआम करवाने वाला भारत रत्न कैसे हो सके है?', ':  जवाहर यादव ,  अगर हिम्मत है तो पूरा वीडियो डालो फिर तुम्हारे झूठ सामने आएंगे। दीपेंदर ने साफ कहा ,  कि भाजपा और…', ':  #भगवा वस्त्र पहन कर मतदान नही कर सकते लेकिन बुरखा पहन के कर सकते हैं ?  ये आज़मगढ़ है और यहाँ से \"टोटी चोर\"…', 'Yey nina khothani labafazi benu phambili Finish and klaar 🍑🌊', \":  जब भी कोई सिकुलर कोंग्रेसी गुलाम चाटुकार चरणचाट चमचा 'गाय' और 'बछड़े' को लेकर भाजपा पर तंज कसता हुआ दिखे तो इतिहास के झ…\"]\n",
            "Hashtags:\n",
            "[[], [], ['#भगव'], [], []]\n",
            "Smileys:\n",
            "[[], [], [], [], []]\n",
            "Emojis:\n",
            "[[], [], [], ['🍑', '🌊'], []]\n",
            "Urls:\n",
            "[[], [], [], [], []]\n",
            "Mentions:\n",
            "[[], ['@Vishesh4', '@jawaharyadavbjp'], ['@FunKeyBaat', '@yadavakhilesh'], [], ['@Rajeshbhanjan2']]\n",
            "Numbers:\n",
            "[['1', '17000', '5000', '3000'], [], [], [], []]\n",
            "Reserved Words:\n",
            "[[], ['RT'], ['RT'], [], ['RT']]\n",
            "Task Labels:\n",
            "['HOF', 'NOT', 'HOF', 'HOF', 'HOF']\n",
            "['HATE', 'NONE', 'HATE', 'PRFN', 'HATE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUrX_FPcIVQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "711f607d-c6ac-4381-dbe0-433e886fc183"
      },
      "source": [
        "# Generating Emoji Texts\n",
        "emoji_texts = []\n",
        "\n",
        "for emo_list in emojis:\n",
        "  texts = []\n",
        "  for emoji in emo_list:\n",
        "    text = emotext(emoji)\n",
        "    texts.append(text.replace(\"_\", \" \"))\n",
        "  emoji_texts.append(texts)\n",
        "\n",
        "print(\"Emoji Descriptions:\")\n",
        "print(emoji_texts[0: 5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emoji Descriptions:\n",
            "[[], [], [], ['peach', 'water wave'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWvCf65IXQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4a0c7222-3f8d-43ae-99ad-a5ee52e75627"
      },
      "source": [
        "# Segmenting Hashtags\n",
        "segmented_hashtags = []\n",
        "\n",
        "for hashset in hashtags:\n",
        "  segmented_set = []\n",
        "  for tag in hashset:\n",
        "    word = tag[1: ]\n",
        "    # removing the hash symbol\n",
        "    segmented_set.append(seg_tw.segment(word))\n",
        "  segmented_hashtags.append(segmented_set)\n",
        "\n",
        "print(\"Segmented Hashtags:\")\n",
        "print(segmented_hashtags[0: 5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segmented Hashtags:\n",
            "[[], [], ['भगव'], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGPepLXoJsqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = 'hi.pickle'\n",
        "dickie = {}\n",
        "dickie['tweet_id'] = tweet_ids\n",
        "dickie['task_1'] = task_1_labels\n",
        "dickie['task_2'] = task_2_labels\n",
        "dickie['hasoc_id'] = hasoc_ID\n",
        "dickie['full_tweet'] = tweets\n",
        "dickie['tweet_raw_text'] = raw_tweet_texts\n",
        "dickie['hashtags'] = hashtags\n",
        "dickie['smiley'] = smileys\n",
        "dickie['emoji'] = emojis\n",
        "dickie['url'] = urls\n",
        "dickie['mentions'] = mentions\n",
        "dickie['numerals'] = numbers\n",
        "dickie['reserved_word'] = reserveds\n",
        "dickie['emotext'] = emoji_texts\n",
        "dickie['segmented_hash'] = segmented_hashtags\n",
        "\n",
        "with open(name, 'wb') as f:\n",
        "  pickle.dump(dickie, f)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK4uO1J1LeIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bab4902-2da3-491d-aad7-8dd8a28784ca"
      },
      "source": [
        "with open(name, 'rb') as f:\n",
        "  try_dict = pickle.load(f)\n",
        "\n",
        "sizes = []\n",
        "for key in try_dict.keys():\n",
        "  sizes.append(len(try_dict[key]))\n",
        "\n",
        "# Verifying if all sizes are equal\n",
        "print(sizes)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963, 2963]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUMYDP9IaXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}