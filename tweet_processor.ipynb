{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTurPmcpM1FCssJcwUOy2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SzZYXq3ER6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import xlrd\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOqUHRECEX9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "db19d4a5-38f8-4061-e1c0-5415a5d233fe"
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "# to leverage word statistics from Twitter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvfnKs-dEZwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk2BUZTuEdN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment if you're running it for the first time\n",
        "# !pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqibDmdYEjIK",
        "colab_type": "text"
      },
      "source": [
        "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRsC-wmEhdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9890338f-d388-4d05-a664-ccd18c316f76"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD2Ksnv6EmaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_list(proc_obj):\n",
        "  if proc_obj == None:\n",
        "    return []\n",
        "  \n",
        "  store = []\n",
        "  for unit in proc_obj:\n",
        "    store.append(unit.match)\n",
        "  \n",
        "  return store\n",
        "\n",
        "def emotext(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI524oqVE026",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For 2020 Datasets\n",
        "# For English\n",
        "file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
        "\n",
        "# For Hindi\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
        "\n",
        "# For German\n",
        "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNoGyv8E5th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing Lists\n",
        "datapoints_count = 0\n",
        "see_index = True\n",
        "\n",
        "tweets = []\n",
        "raw_tweet_texts = []\n",
        "\n",
        "hashtags = []\n",
        "smileys = []\n",
        "emojis = []\n",
        "urls = []\n",
        "mentions = []\n",
        "numbers = []\n",
        "reserveds = []\n",
        "\n",
        "task_1_labels = []\n",
        "task_2_labels = []\n",
        "tweet_ids = []\n",
        "hasoc_ID = []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TuJvskdE95H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5361f66c-ab00-4f37-e39a-3122a86c1414"
      },
      "source": [
        "workbook = xlrd.open_workbook(file_name)\n",
        "sheet = workbook.sheet_by_index(0)\n",
        "\n",
        "count = 0\n",
        "for row in range(sheet.nrows):\n",
        "    line = sheet.row_values(row)\n",
        "\n",
        "    if see_index == True:\n",
        "        see_index = False\n",
        "        continue\n",
        "\n",
        "    datapoints_count += 1\n",
        "    tweet_ids.append(line[0])\n",
        "    task_1_labels.append(line[2])\n",
        "    task_2_labels.append(line[3])\n",
        "    hasoc_ID.append(line[4])\n",
        "    tweets.append(line[1])\n",
        "    raw_tweet_texts.append(tweet_proc.clean(line[1]))\n",
        "\n",
        "    parse_obj = tweet_proc.parse(line[1])\n",
        "\n",
        "    hashtags.append(make_list(parse_obj.hashtags))\n",
        "    smileys.append(make_list(parse_obj.smileys))\n",
        "    emojis.append(make_list(parse_obj.emojis))\n",
        "    urls.append(make_list(parse_obj.urls))\n",
        "    mentions.append(make_list(parse_obj.mentions))\n",
        "    numbers.append(make_list(parse_obj.numbers))\n",
        "    reserveds.append(make_list(parse_obj.reserved))\n",
        "\n",
        "print(\"Number of Datapoints: \" + str(datapoints_count))\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Datapoints: 3708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd_fy5REFxUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "ba73efad-270f-4a24-c3ba-cad72cf31e27"
      },
      "source": [
        "# Viewing Created Dataset\n",
        "\n",
        "print(\"Tweets:\")\n",
        "print(tweets[0: 5])\n",
        "\n",
        "print(\"Raw Texts:\")\n",
        "print(raw_tweet_texts[0: 5])\n",
        "\n",
        "print(\"Hashtags:\")\n",
        "print(hashtags[0: 5])\n",
        "\n",
        "print(\"Smileys:\")\n",
        "print(smileys[0: 5])\n",
        "\n",
        "print(\"Emojis:\")\n",
        "print(emojis[0: 5])\n",
        "\n",
        "print(\"Urls:\")\n",
        "print(urls[0: 5])\n",
        "\n",
        "print(\"Mentions:\")\n",
        "print(mentions[0: 5])\n",
        "\n",
        "print(\"Numbers:\")\n",
        "print(numbers[0: 5])\n",
        "\n",
        "print(\"Reserved Words:\")\n",
        "print(reserveds[0: 5])\n",
        "\n",
        "print(\"Task Labels:\")\n",
        "print(task_1_labels[0: 5])\n",
        "print(task_2_labels[0: 5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweets:\n",
            "['hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚, Iâ€™m tryna make u my la sweety , fuck ah bro', \"RT @airjunebug: When you're from the Bay but you're really a NY nigga at heart. W/ @supportcaleon https://t.co/mZ8BAYlnlf\", 'RT @DonaldJTrumpJr: Dear Democrats: The American people arenâ€™t stupid, they know what spying is and no amount of gaslighting will change thâ€¦', 'RT @SheLoveTimothy: He ainâ€™t on drugs he just bored. I be doing the same shit when Iâ€™m bored ðŸ˜‚ https://t.co/tkdjSbddET', 'RT @TavianJordan: Summer â€˜19 Iâ€™m coming for you ! No boring shit ! Beach days, road trips, kickbacks and HOT DAYS ! Iâ€™m ready Iâ€™m ready Iâ€™mâ€¦']\n",
            "Raw Texts:\n",
            "['hate wen females hit ah nigga with tht bro , Im tryna make u my la sweety , fuck ah bro', \": When you're from the Bay but you're really a NY nigga at heart. W/\", ': Dear Democrats: The American people arent stupid, they know what spying is and no amount of gaslighting will change th', ': He aint on drugs he just bored. I be doing the same shit when Im bored', ': Summer Im coming for you ! No boring shit ! Beach days, road trips, kickbacks and HOT DAYS ! Im ready Im ready Im']\n",
            "Hashtags:\n",
            "[[], [], [], [], []]\n",
            "Smileys:\n",
            "[[], [], [], [], []]\n",
            "Emojis:\n",
            "[['ðŸ˜‚', 'ðŸ˜‚'], [], [], ['ðŸ˜‚'], []]\n",
            "Urls:\n",
            "[[], ['https://t.co/mZ8BAYlnlf'], [], ['https://t.co/tkdjSbddET'], []]\n",
            "Mentions:\n",
            "[[], ['@airjunebug', '@supportcaleon'], ['@DonaldJTrumpJr'], ['@SheLoveTimothy'], ['@TavianJordan']]\n",
            "Numbers:\n",
            "[[], [], [], [], []]\n",
            "Reserved Words:\n",
            "[[], ['RT'], ['RT'], ['RT'], ['RT']]\n",
            "Task Labels:\n",
            "['HOF', 'HOF', 'NOT', 'HOF', 'NOT']\n",
            "['PRFN', 'PRFN', 'NONE', 'PRFN', 'NONE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUrX_FPcIVQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9c065692-26b2-4fcb-a983-4a5489183093"
      },
      "source": [
        "# Generating Emoji Texts\n",
        "emoji_texts = []\n",
        "\n",
        "for emo_list in emojis:\n",
        "  texts = []\n",
        "  for emoji in emo_list:\n",
        "    text = emotext(emoji)\n",
        "    texts.append(text.replace(\"_\", \" \"))\n",
        "  emoji_texts.append(texts)\n",
        "\n",
        "print(\"Emoji Descriptions:\")\n",
        "print(emoji_texts[0: 5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emoji Descriptions:\n",
            "[['face with tears of joy', 'face with tears of joy'], [], [], ['face with tears of joy'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWvCf65IXQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "69941aa3-1ced-4228-c623-48530d92a04f"
      },
      "source": [
        "# Segmenting Hashtags\n",
        "segmented_hashtags = []\n",
        "\n",
        "for hashset in hashtags:\n",
        "  segmented_set = []\n",
        "  for tag in hashset:\n",
        "    word = tag[1: ]\n",
        "    # removing the hash symbol\n",
        "    segmented_set.append(seg_tw.segment(word))\n",
        "  segmented_hashtags.append(segmented_set)\n",
        "\n",
        "print(\"Segmented Hashtags:\")\n",
        "print(segmented_hashtags[0: 5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segmented Hashtags:\n",
            "[[], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGPepLXoJsqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = 'ge.pickle'\n",
        "dickie = {}\n",
        "dickie['tweet_id'] = tweet_ids\n",
        "dickie['task_1'] = task_1_labels\n",
        "dickie['task_2'] = task_2_labels\n",
        "dickie['hasoc_id'] = hasoc_ID\n",
        "dickie['full_tweet'] = tweets\n",
        "dickie['tweet_raw_text'] = raw_tweet_texts\n",
        "dickie['hashtags'] = hashtags\n",
        "dickie['smiley'] = smileys\n",
        "dickie['emoji'] = emojis\n",
        "dickie['url'] = urls\n",
        "dickie['mentions'] = mentions\n",
        "dickie['numerals'] = numbers\n",
        "dickie['reserved_word'] = reserveds\n",
        "dickie['emotext'] = emoji_texts\n",
        "dickie['segmented_hash'] = segmented_hashtags\n",
        "\n",
        "with open(name, 'wb') as f:\n",
        "  pickle.dump(dickie, f)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK4uO1J1LeIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cda29d8b-8bc7-457a-d82c-bd7feb5c2c5c"
      },
      "source": [
        "with open(name, 'rb') as f:\n",
        "  try_dict = pickle.load(f)\n",
        "\n",
        "sizes = []\n",
        "for key in try_dict.keys():\n",
        "  sizes.append(len(try_dict[key]))\n",
        "\n",
        "# Verifying if all sizes are equal\n",
        "print(sizes)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708, 3708]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUMYDP9IaXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}